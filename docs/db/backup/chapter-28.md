---
title: ç¬¬29ç« ï¼šAI åº”ç”¨æ•°æ®åº“æ¶æ„
---

# ï¼šAI åº”ç”¨æ•°æ®åº“æ¶æ„

> **éš¾åº¦ç­‰çº§**ï¼šâ­â­â­â­â­ ä¸“å®¶çº§ | **å­¦ä¹ æ—¶é•¿**ï¼š15å°æ—¶ | **å®æˆ˜é¡¹ç›®**ï¼šAI æ™ºèƒ½æœç´¢ç³»ç»Ÿ

## ğŸ“š æœ¬ç« ç›®å½•

- [28.1 RAG æ£€ç´¢å¢å¼º](#281-rag-æ£€ç´¢å¢å¼º)
- [28.2 å‘é‡æœç´¢ä¼˜åŒ–](#282-å‘é‡æœç´¢ä¼˜åŒ–)
- [28.3 æ··åˆæ£€ç´¢](#283-æ··åˆæ£€ç´¢)
- [28.4 æ€§èƒ½è°ƒä¼˜](#284-æ€§èƒ½è°ƒä¼˜)
- [28.5 å®æˆ˜é¡¹ç›®ï¼šAI æœç´¢ç³»ç»Ÿ](#285-å®æˆ˜é¡¹ç›®ai-æœç´¢ç³»ç»Ÿ)

---

## RAG æ£€ç´¢å¢å¼º

### ä»€ä¹ˆæ˜¯ RAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            RAG (Retrieval-Augmented Generation)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  ä¼ ç»Ÿ LLM ç”Ÿæˆ:                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚  â”‚ ç”¨æˆ·    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â†’â”‚   LLM   â”‚                    â”‚
â”‚  â”‚ é—®é¢˜    â”‚         â”‚ ç”Ÿæˆå›ç­” â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                           â”‚                          â”‚
â”‚                           â–¼                          â”‚
â”‚                    å¯èƒ½äº§ç”Ÿå¹»è§‰                        â”‚
â”‚                    çŸ¥è¯†æˆªæ­¢æ—¥æœŸé™åˆ¶                    â”‚
â”‚                                                      â”‚
â”‚  RAG å¢å¼º:                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                         â”‚
â”‚  â”‚ ç”¨æˆ·    â”‚                                         â”‚
â”‚  â”‚ é—®é¢˜    â”‚                                         â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                                         â”‚
â”‚       â”‚                                               â”‚
â”‚       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚       â”‚                  â”‚                          â”‚
â”‚       â–¼                  â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚çŸ¥è¯†åº“   â”‚       â”‚  æœç´¢   â”‚                     â”‚
â”‚  â”‚(æ–‡æ¡£)   â”‚       â”‚ å¼•æ“    â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜                     â”‚
â”‚                          â”‚                          â”‚
â”‚                    æ£€ç´¢ç›¸å…³æ–‡æ¡£                       â”‚
â”‚                          â”‚                          â”‚
â”‚                          â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚            Prompt + ä¸Šä¸‹æ–‡              â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                    â”‚                                 â”‚
â”‚                    â–¼                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ä¸Šä¸‹æ–‡  â”‚â”€â”€â”€â”€â”€â”€â†’â”‚   LLM   â”‚â”€â”€â”€â”€â”€â”€â†’â”‚ æ›´å‡†ç¡®  â”‚  â”‚
â”‚  â”‚         â”‚       â”‚ ç”Ÿæˆå›ç­” â”‚       â”‚ çš„å›ç­”  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                      â”‚
â”‚  ä¼˜ç‚¹:                                              â”‚
â”‚  âœ… å‡å°‘å¹»è§‰                                          â”‚
â”‚  âœ… çŸ¥è¯†å®æ—¶æ›´æ–°                                      â”‚
â”‚  âœ… å¯è§£é‡Šæ€§å¼º                                        â”‚
â”‚  âœ… æˆæœ¬ä½ï¼ˆä¸éœ€è¦å¾®è°ƒæ¨¡å‹ï¼‰                           â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG æ¶æ„è®¾è®¡

**æ•°æ®åº“å±‚**ï¼š

```python
from pymilvus import Collection, FieldSchema, CollectionSchema, DataType
from pymilvus import connections
from sentence_transformers import SentenceTransformer
import chromadb

# è¿æ¥å‘é‡æ•°æ®åº“
connections.connect(host='localhost', port='19530')

# åˆ›å»ºæ–‡æ¡£ Collection
fields = [
    FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
    FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=100),
    FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, max_length=100),
    FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384),
    FieldSchema(name="metadata", dtype=DataType.JSON),
    FieldSchema(name="created_at", dtype=DataType.INT64)
]
schema = CollectionSchema(fields, description="RAG documents")
doc_collection = Collection(name="rag_documents", schema=schema)

# åˆ›å»ºç´¢å¼•
index_params = {
    "metric_type": "COSINE",
    "index_type": "HNSW",
    "params": {"M": 16, "efConstruction": 200}
}
doc_collection.create_index(field_name="embedding", index_params=index_params)

# åŠ è½½åµŒå…¥æ¨¡å‹
embedder = SentenceTransformer('all-MiniLM-L6-v2')
```

**æ–‡æ¡£å¤„ç†æµç¨‹**ï¼š

```python
import uuid
from datetime import datetime
import hashlib

class DocumentProcessor:
    def __init__(self, chunk_size=500, chunk_overlap=50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def chunk_document(self, text, doc_id):
        """å°†æ–‡æ¡£åˆ‡åˆ†ä¸ºå—"""
        chunks = []
        start = 0
        chunk_idx = 0

        while start < len(text):
            end = start + self.chunk_size
            chunk = text[start:end]

            chunk_id = f"{doc_id}_chunk_{chunk_idx}"
            chunks.append({
                "chunk_id": chunk_id,
                "content": chunk,
                "start": start,
                "end": end
            })

            start = end - self.chunk_overlap
            chunk_idx += 1

        return chunks

    def embed_chunks(self, chunks):
        """ç”Ÿæˆå‘é‡åµŒå…¥"""
        texts = [chunk["content"] for chunk in chunks]
        embeddings = embedder.encode(texts).tolist()
        return embeddings

    def store_chunks(self, doc_id, chunks, embeddings, metadata=None):
        """å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“"""
        now = int(datetime.now().timestamp() * 1000)

        entities = [
            [doc_id] * len(chunks),  # doc_ids
            [chunk["chunk_id"] for chunk in chunks],  # chunk_ids
            [chunk["content"] for chunk in chunks],  # contents
            embeddings,  # embeddings
            [metadata] * len(chunks) if metadata else [{}] * len(chunks),  # metadata
            [now] * len(chunks)  # created_at
        ]

        doc_collection.insert(entities)
        doc_collection.flush()

# ä½¿ç”¨ç¤ºä¾‹
processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)

# å¤„ç†æ–‡æ¡£
document = """
äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œ
è‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
è¿™åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ç­‰èƒ½åŠ›ã€‚
"""

doc_id = str(uuid.uuid4())
chunks = processor.chunk_document(document, doc_id)
embeddings = processor.embed_chunks(chunks)

metadata = {
    "title": "äººå·¥æ™ºèƒ½ä»‹ç»",
    "author": "AI Expert",
    "category": "æŠ€æœ¯",
    "tags": ["AI", "æœºå™¨å­¦ä¹ ", "æ·±åº¦å­¦ä¹ "]
}

processor.store_chunks(doc_id, chunks, embeddings, metadata)

# åŠ è½½åˆ°å†…å­˜
doc_collection.load()
```

**æ£€ç´¢å¢å¼ºç”Ÿæˆ**ï¼š

```python
import openai

class RAGSystem:
    def __init__(self, collection, top_k=3):
        self.collection = collection
        self.top_k = top_k

    def retrieve(self, query, filters=None):
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = embedder.encode([query]).tolist()

        # æ„å»ºè¿‡æ»¤è¡¨è¾¾å¼
        expr = None
        if filters:
            conditions = []
            for key, value in filters.items():
                if isinstance(value, str):
                    conditions.append(f'metadata["{key}"] == "{value}"')
                else:
                    conditions.append(f'metadata["{key}"] == {value}')
            expr = " and ".join(conditions)

        # æœç´¢
        results = self.collection.search(
            data=query_embedding,
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"ef": 64}},
            limit=self.top_k,
            expr=expr,
            output_fields=["doc_id", "chunk_id", "content", "metadata"]
        )

        return results[0]

    def generate(self, query, context):
        """ç”Ÿæˆå›ç­”"""
        # æ„å»º prompt
        prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{query}

å›ç­”ï¼š"""

        # è°ƒç”¨ LLM
        response = openai.ChatCompletion.create(
            model="gpt-4",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”é—®é¢˜ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1000
        )

        return response.choices[0].message.content

    def query(self, query, filters=None):
        """å®Œæ•´ RAG æµç¨‹"""
        # 1. æ£€ç´¢
        results = self.retrieve(query, filters)

        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []
        for hit in results:
            context_parts.append(f"[æ¥æº: {hit.entity.get('metadata').get('title', 'Unknown')}]")
            context_parts.append(hit.entity.get('content'))

        context = "\n\n".join(context_parts)

        # 3. ç”Ÿæˆ
        answer = self.generate(query, context)

        return {
            "answer": answer,
            "sources": [
                {
                    "content": hit.entity.get('content'),
                    "metadata": hit.entity.get('metadata'),
                    "score": hit.distance
                }
                for hit in results
            ]
        }

# ä½¿ç”¨ç¤ºä¾‹
rag = RAGSystem(doc_collection, top_k=3)

query = "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ"
result = rag.query(query)

print(f"é—®é¢˜ï¼š{query}")
print(f"å›ç­”ï¼š{result['answer']}")
print("\nå‚è€ƒæ¥æºï¼š")
for source in result['sources']:
    print(f"- {source['metadata'].get('title')} (ç›¸å…³åº¦: {source['score']:.4f})")
```

---

## å‘é‡æœç´¢ä¼˜åŒ–

### æŸ¥è¯¢ä¼˜åŒ–

**1. åŠ¨æ€ Top-K**ï¼š

```python
def dynamic_search(query, min_score=0.7, max_results=10):
    """æ ¹æ®ç›¸å…³æ€§åŠ¨æ€è°ƒæ•´è¿”å›ç»“æœæ•°é‡"""
    query_embedding = embedder.encode([query]).tolist()

    # å…ˆè·å–æ›´å¤šå€™é€‰ç»“æœ
    results = doc_collection.search(
        data=query_embedding,
        anns_field="embedding",
        param={"metric_type": "COSINE", "params": {"ef": 64}},
        limit=max_results * 2,  # è·å–2å€æ•°é‡
        output_fields=["content", "metadata"]
    )[0]

    # è¿‡æ»¤ä½åˆ†ç»“æœ
    filtered_results = [r for r in results if r.distance >= min_score]

    # è¿”å›å‰ max_results ä¸ª
    return filtered_results[:max_results]
```

**2. æŸ¥è¯¢é‡å†™**ï¼š

```python
def query_expansion(query):
    """æŸ¥è¯¢æ‰©å±•ï¼Œæå‡å¬å›ç‡"""
    # ä½¿ç”¨ LLM ç”ŸæˆæŸ¥è¯¢çš„å¤šä¸ªå˜ä½“
    prompt = f"""ç”Ÿæˆä»¥ä¸‹æŸ¥è¯¢çš„3ä¸ªè¯­ä¹‰ç›¸ä¼¼çš„å˜ä½“ï¼Œç”¨äºä¿¡æ¯æ£€ç´¢ã€‚

åŸå§‹æŸ¥è¯¢ï¼š{query}

å˜ä½“æŸ¥è¯¢ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰ï¼š"""

    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.5,
        max_tokens=200
    )

    expanded_queries = [query]
    expanded_queries.extend(response.choices[0].message.content.strip().split('\n'))

    return expanded_queries

def expanded_search(query):
    """ä½¿ç”¨æŸ¥è¯¢æ‰©å±•è¿›è¡Œæœç´¢"""
    expanded_queries = query_expansion(query)

    all_results = []
    for q in expanded_queries:
        q_embedding = embedder.encode([q]).tolist()
        results = doc_collection.search(
            data=q_embedding,
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"ef": 64}},
            limit=5,
            output_fields=["content", "metadata"]
        )[0]
        all_results.extend(results)

    # å»é‡ï¼ˆåŸºäº chunk_idï¼‰
    seen = set()
    unique_results = []
    for result in all_results:
        chunk_id = result.entity.get('chunk_id')
        if chunk_id not in seen:
            seen.add(chunk_id)
            unique_results.append(result)

    # é‡æ–°æ’åº
    unique_results.sort(key=lambda x: x.distance, reverse=True)

    return unique_results[:10]
```

**3. æ··åˆæŸ¥è¯¢ç­–ç•¥**ï¼š

```python
from pymilvus import connections

class HybridSearcher:
    def __init__(self, collection):
        self.collection = collection

    def dense_search(self, query, top_k=10):
        """ç¨ å¯†å‘é‡æœç´¢"""
        query_embedding = embedder.encode([query]).tolist()
        results = self.collection.search(
            data=query_embedding,
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"ef": 64}},
            limit=top_k,
            output_fields=["content", "metadata"]
        )[0]
        return results

    def sparse_search(self, query, top_k=10):
        """ç¨€ç–æœç´¢ï¼ˆåŸºäº BM25ï¼‰"""
        # ä½¿ç”¨ Elasticsearch è¿›è¡Œå…³é”®è¯æœç´¢
        from elasticsearch import Elasticsearch

        es = Elasticsearch(["http://localhost:9200"])

        response = es.search(
            index="documents",
            body={
                "query": {
                    "match": {
                        "content": query
                    }
                },
                "size": top_k
            }
        )

        results = []
        for hit in response['hits']['hits']:
            results.append({
                'chunk_id': hit['_source']['chunk_id'],
                'content': hit['_source']['content'],
                'score': hit['_score']
            })

        return results

    def hybrid_search(self, query, alpha=0.5, top_k=10):
        """æ··åˆæœç´¢ï¼ˆç¨ å¯† + ç¨€ç–ï¼‰"""
        dense_results = self.dense_search(query, top_k * 2)
        sparse_results = self.sparse_search(query, top_k * 2)

        # å½’ä¸€åŒ–åˆ†æ•°
        dense_scores = {r.entity.get('chunk_id'): r.distance for r in dense_results}
        sparse_scores = {r['chunk_id']: r['score'] for r in sparse_results}

        # åˆå¹¶åˆ†æ•°
        all_chunk_ids = set(dense_scores.keys()) | set(sparse_scores.keys())

        combined_scores = {}
        for chunk_id in all_chunk_ids:
            dense_score = dense_scores.get(chunk_id, 0)
            sparse_score = sparse_scores.get(chunk_id, 0)

            # å½’ä¸€åŒ–åˆ° [0, 1]
            dense_norm = (dense_score - min(dense_scores.values())) / (max(dense_scores.values()) - min(dense_scores.values()) + 1e-6)
            sparse_norm = (sparse_score - min(sparse_scores.values())) / (max(sparse_scores.values()) - min(sparse_scores.values()) + 1e-6)

            # åŠ æƒç»„åˆ
            combined_scores[chunk_id] = alpha * dense_norm + (1 - alpha) * sparse_norm

        # æ’åº
        sorted_results = sorted(combined_scores.items(), key=lambda x: x[1], reverse=True)

        return sorted_results[:top_k]
```

### ç´¢å¼•ä¼˜åŒ–

**1. å¤šçº§ç´¢å¼•**ï¼š

```python
# ä¸ºä¸åŒç²’åº¦çš„æ•°æ®åˆ›å»ºå¤šä¸ªç´¢å¼•

# 1. ç²—ç²’åº¦ç´¢å¼•ï¼ˆæ–‡æ¡£çº§ï¼‰
doc_fields = [
    FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
    FieldSchema(name="title", dtype=DataType.VARCHAR, max_length=512),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384)
]
doc_schema = CollectionSchema(doc_fields)
doc_collection = Collection(name="documents", schema=doc_schema)

# 2. ç»†ç²’åº¦ç´¢å¼•ï¼ˆæ®µè½çº§ï¼‰
chunk_fields = [
    FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
    FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=100),
    FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=65535),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384)
]
chunk_schema = CollectionSchema(chunk_fields)
chunk_collection = Collection(name="chunks", schema=chunk_schema)

# 3. å¥å­çº§ç´¢å¼•
sentence_fields = [
    FieldSchema(name="sentence_id", dtype=DataType.VARCHAR, max_length=100, is_primary=True),
    FieldSchema(name="chunk_id", dtype=DataType.VARCHAR, max_length=100),
    FieldSchema(name="content", dtype=DataType.VARCHAR, max_length=1024),
    FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=384)
]
sentence_schema = CollectionSchema(sentence_fields)
sentence_collection = Collection(name="sentences", schema=sentence_schema)
```

**2. è‡ªé€‚åº”ç´¢å¼•**ï¼š

```python
def adaptive_indexing(collection, data_size_threshold=100000):
    """æ ¹æ®æ•°æ®é‡è‡ªåŠ¨é€‰æ‹©ç´¢å¼•ç±»å‹"""

    # è·å–å½“å‰æ•°æ®é‡
    stats = collection.num_entities

    if stats < data_size_threshold:
        # å°æ•°æ®é‡ï¼šä½¿ç”¨ FLAT ç´¢å¼•
        index_params = {
            "metric_type": "COSINE",
            "index_type": "FLAT"
        }
    elif stats < data_size_threshold * 10:
        # ä¸­ç­‰æ•°æ®é‡ï¼šä½¿ç”¨ IVF_FLAT
        index_params = {
            "metric_type": "COSINE",
            "index_type": "IVF_FLAT",
            "params": {"nlist": 128}
        }
    else:
        # å¤§æ•°æ®é‡ï¼šä½¿ç”¨ HNSW
        index_params = {
            "metric_type": "COSINE",
            "index_type": "HNSW",
            "params": {"M": 16, "efConstruction": 200}
        }

    # åˆ é™¤æ—§ç´¢å¼•å¹¶åˆ›å»ºæ–°ç´¢å¼•
    collection.drop_index()
    collection.create_index(field_name="embedding", index_params=index_params)
    collection.load()

    return index_params
```

---

## æ··åˆæ£€ç´¢

### å¤šæ¨¡æ€æ£€ç´¢

```python
from transformers import CLIPProcessor, CLIPModel
from PIL import Image

class MultiModalRetriever:
    def __init__(self):
        # åŠ è½½ CLIP æ¨¡å‹ï¼ˆæ”¯æŒå›¾åƒå’Œæ–‡æœ¬ï¼‰
        self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
        self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

    def embed_image(self, image_path):
        """ç”Ÿæˆå›¾åƒåµŒå…¥"""
        image = Image.open(image_path)
        inputs = self.processor(images=image, return_tensors="pt")
        image_features = self.model.get_image_features(**inputs)
        return image_features.detach().numpy()[0].tolist()

    def embed_text(self, text):
        """ç”Ÿæˆæ–‡æœ¬åµŒå…¥"""
        inputs = self.processor(text=[text], return_tensors="pt", padding=True)
        text_features = self.model.get_text_features(**inputs)
        return text_features.detach().numpy()[0].tolist()

    def search_by_text(self, text_query, collection, top_k=10):
        """æ–‡æœ¬æŸ¥è¯¢å›¾åƒ"""
        text_embedding = self.embed_text(text_query)

        results = collection.search(
            data=[text_embedding],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"ef": 64}},
            limit=top_k,
            output_fields=["image_path", "caption"]
        )

        return results[0]

    def search_by_image(self, image_path, collection, top_k=10):
        """å›¾åƒæŸ¥è¯¢å›¾åƒ"""
        image_embedding = self.embed_image(image_path)

        results = collection.search(
            data=[image_embedding],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"ef": 64}},
            limit=top_k,
            output_fields=["image_path", "caption"]
        )

        return results[0]
```

### é‡æ’åºï¼ˆRerankingï¼‰

```python
class Reranker:
    def __init__(self):
        from sentence_transformers import CrossEncoder
        self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

    def rerank(self, query, candidates, top_k=10):
        """ä½¿ç”¨ CrossEncoder é‡æ’åº"""
        # æ„å»ºæŸ¥è¯¢-æ–‡æ¡£å¯¹
        pairs = [[query, candidate['content']] for candidate in candidates]

        # è®¡ç®—åˆ†æ•°
        scores = self.model.predict(pairs)

        # ç»„åˆç»“æœ
        reranked = list(zip(candidates, scores))
        reranked.sort(key=lambda x: x[1], reverse=True)

        # è¿”å› Top-K
        return [item[0] for item in reranked[:top_k]]

# ä½¿ç”¨ç¤ºä¾‹
reranker = Reranker()

# åˆå§‹æ£€ç´¢
initial_results = rag.retrieve(query, top_k=50)

# é‡æ’åº
candidates = [
    {
        'content': hit.entity.get('content'),
        'metadata': hit.entity.get('metadata')
    }
    for hit in initial_results
]

reranked_results = reranker.rerank(query, candidates, top_k=10)
```

---

## æ€§èƒ½è°ƒä¼˜

### æ‰¹é‡å¤„ç†

```python
def batch_embed_and_store(texts, batch_size=32):
    """æ‰¹é‡ç”ŸæˆåµŒå…¥å¹¶å­˜å‚¨"""
    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        embeddings = embedder.encode(batch).tolist()
        all_embeddings.extend(embeddings)

    return all_embeddings

def batch_search(queries, batch_size=10):
    """æ‰¹é‡æœç´¢"""
    results_list = []

    for i in range(0, len(queries), batch_size):
        batch = queries[i:i + batch_size]
        batch_embeddings = embedder.encode(batch).tolist()

        results = doc_collection.search(
            data=batch_embeddings,
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {"ef": 64}},
            limit=10,
            output_fields=["content", "metadata"]
        )

        results_list.extend(results)

    return results_list
```

### ç¼“å­˜ç­–ç•¥

```python
import hashlib
import json
from functools import lru_cache

class CachedRAG:
    def __init__(self, rag_system):
        self.rag = rag_system
        self.cache = {}

    def _get_cache_key(self, query, filters):
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_data = {"query": query, "filters": filters}
        key_str = json.dumps(key_data, sort_keys=True)
        return hashlib.md5(key_str.encode()).hexdigest()

    def query(self, query, filters=None, use_cache=True):
        """å¸¦ç¼“å­˜çš„æŸ¥è¯¢"""
        cache_key = self._get_cache_key(query, filters)

        if use_cache and cache_key in self.cache:
            print("ä»ç¼“å­˜è¿”å›ç»“æœ")
            return self.cache[cache_key]

        # æ‰§è¡ŒæŸ¥è¯¢
        result = self.rag.query(query, filters)

        # å­˜å…¥ç¼“å­˜
        if use_cache:
            self.cache[cache_key] = result

        return result

    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self.cache.clear()
```

### å¹¶å‘æŸ¥è¯¢

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

async def async_search(queries, max_workers=5):
    """å¼‚æ­¥å¹¶å‘æœç´¢"""
    loop = asyncio.get_event_loop()
    executor = ThreadPoolExecutor(max_workers=max_workers)

    def sync_search(query):
        return rag.retrieve(query, top_k=10)

    tasks = [
        loop.run_in_executor(executor, sync_search, query)
        for query in queries
    ]

    results = await asyncio.gather(*tasks)

    executor.shutdown()
    return results

# ä½¿ç”¨ç¤ºä¾‹
queries = ["ä»€ä¹ˆæ˜¯AIï¼Ÿ", "æ·±åº¦å­¦ä¹ åŸç†", "ç¥ç»ç½‘ç»œç»“æ„"]
results = asyncio.run(async_search(queries))
```

---

## å®æˆ˜é¡¹ç›®ï¼šAI æœç´¢ç³»ç»Ÿ

### ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            AI æœç´¢ç³»ç»Ÿæ¶æ„                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚           å‰ç«¯ç•Œé¢                        â”‚       â”‚
â”‚  â”‚  - æœç´¢æ¡†                                â”‚       â”‚
â”‚  â”‚  - è¿‡æ»¤å™¨ï¼ˆåˆ†ç±»ã€æ—¥æœŸç­‰ï¼‰                  â”‚       â”‚
â”‚  â”‚  - ç»“æœå±•ç¤º                              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                    â”‚                                  â”‚
â”‚                    â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚           API å±‚ (FastAPI)               â”‚       â”‚
â”‚  â”‚  - æœç´¢æ¥å£                              â”‚       â”‚
â”‚  â”‚  - æ–‡æ¡£ä¸Šä¼                               â”‚       â”‚
â”‚  â”‚  - ç»“æœè¿”å›                              â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                    â”‚                                  â”‚
â”‚                    â–¼                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚          ä¸šåŠ¡é€»è¾‘å±‚                       â”‚       â”‚
â”‚  â”‚  - RAG ç³»ç»Ÿ                              â”‚       â”‚
â”‚  â”‚  - æŸ¥è¯¢é‡å†™                              â”‚       â”‚
â”‚  â”‚  - ç»“æœé‡æ’åº                            â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                    â”‚                                  â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚         â”‚          â”‚          â”‚                      â”‚
â”‚         â–¼          â–¼          â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚Milvus    â”‚ â”‚Elasticsearchâ”‚â”‚PostgreSQLâ”‚            â”‚
â”‚  â”‚(å‘é‡)    â”‚ â”‚(å…¨æ–‡)    â”‚ â”‚(å…ƒæ•°æ®)  â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### åç«¯å®ç°

```python
from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

app = FastAPI(title="AI Search System")

# æ•°æ®æ¨¡å‹
class SearchRequest(BaseModel):
    query: str
    filters: Optional[dict] = None
    top_k: int = 10

class SearchResult(BaseModel):
    content: str
    metadata: dict
    score: float

class SearchResponse(BaseModel):
    answer: str
    sources: List[SearchResult]

# API ç«¯ç‚¹
@app.post("/search", response_model=SearchResponse)
async def search(request: SearchRequest):
    """æœç´¢æ¥å£"""
    try:
        # æ‰§è¡Œ RAG æœç´¢
        result = rag.query(
            query=request.query,
            filters=request.filters
        )

        return SearchResponse(
            answer=result['answer'],
            sources=[
                SearchResult(
                    content=source['content'],
                    metadata=source['metadata'],
                    score=source['score']
                )
                for source in result['sources']
            ]
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£"""
    try:
        # è¯»å–æ–‡ä»¶å†…å®¹
        content = await file.read()

        # å¤„ç†æ–‡æ¡£
        text = content.decode('utf-8')
        doc_id = str(uuid.uuid4())

        # åˆ†å—
        chunks = processor.chunk_document(text, doc_id)

        # åµŒå…¥
        embeddings = processor.embed_chunks(chunks)

        # å­˜å‚¨
        metadata = {
            "filename": file.filename,
            "size": len(content)
        }
        processor.store_chunks(doc_id, chunks, embeddings, metadata)

        return {"status": "success", "doc_id": doc_id}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}

# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### å‰ç«¯å®ç°

```html
<!DOCTYPE html>
<html>
<head>
    <title>AI æœç´¢ç³»ç»Ÿ</title>
    <style>
        .search-container {
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
        }
        .search-box {
            display: flex;
            gap: 10px;
            margin-bottom: 20px;
        }
        .search-input {
            flex: 1;
            padding: 10px;
            font-size: 16px;
        }
        .search-button {
            padding: 10px 20px;
            background-color: #007bff;
            color: white;
            border: none;
            cursor: pointer;
        }
        .results {
            margin-top: 20px;
        }
        .answer {
            background-color: #f8f9fa;
            padding: 15px;
            border-radius: 5px;
            margin-bottom: 20px;
        }
        .source {
            border: 1px solid #ddd;
            padding: 10px;
            margin-bottom: 10px;
            border-radius: 5px;
        }
        .upload-container {
            margin-top: 30px;
            padding: 20px;
            border: 2px dashed #ddd;
            text-align: center;
        }
    </style>
</head>
<body>
    <div class="search-container">
        <h1>AI æœç´¢ç³»ç»Ÿ</h1>

        <div class="search-box">
            <input type="text" id="queryInput" class="search-input" placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜...">
            <button onclick="search()" class="search-button">æœç´¢</button>
        </div>

        <div id="results" class="results"></div>

        <div class="upload-container">
            <h3>ä¸Šä¼ æ–‡æ¡£</h3>
            <input type="file" id="fileInput">
            <button onclick="uploadFile()">ä¸Šä¼ </button>
        </div>
    </div>

    <script>
        async function search() {
            const query = document.getElementById('queryInput').value;
            const resultsDiv = document.getElementById('results');

            resultsDiv.innerHTML = '<p>æœç´¢ä¸­...</p>';

            try {
                const response = await fetch('/search', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                        query: query,
                        top_k: 5
                    })
                });

                const data = await response.json();

                let html = '<div class="answer"><h3>å›ç­”ï¼š</h3><p>' + data.answer + '</p></div>';
                html += '<h3>å‚è€ƒæ¥æºï¼š</h3>';

                data.sources.forEach(source => {
                    html += '<div class="source">';
                    html += '<p><strong>åˆ†æ•°ï¼š</strong>' + source.score.toFixed(4) + '</p>';
                    html += '<p>' + source.content + '</p>';
                    html += '</div>';
                });

                resultsDiv.innerHTML = html;

            } catch (error) {
                resultsDiv.innerHTML = '<p style="color: red;">æœç´¢å‡ºé”™ï¼š' + error.message + '</p>';
            }
        }

        async function uploadFile() {
            const fileInput = document.getElementById('fileInput');
            const file = fileInput.files[0];

            if (!file) {
                alert('è¯·é€‰æ‹©æ–‡ä»¶');
                return;
            }

            const formData = new FormData();
            formData.append('file', file);

            try {
                const response = await fetch('/upload', {
                    method: 'POST',
                    body: formData
                });

                const data = await response.json();
                alert('ä¸Šä¼ æˆåŠŸï¼æ–‡æ¡£ID: ' + data.doc_id);

            } catch (error) {
                alert('ä¸Šä¼ å¤±è´¥ï¼š' + error.message);
            }
        }

        // æ”¯æŒå›è½¦æœç´¢
        document.getElementById('queryInput').addEventListener('keypress', function(e) {
            if (e.key === 'Enter') {
                search();
            }
        });
    </script>
</body>
</html>
```

---

## âœ… æœ¬ç« å°ç»“

### å­¦ä¹ æ£€æŸ¥æ¸…å•

å®Œæˆæœ¬ç« å­¦ä¹ åï¼Œè¯·ç¡®è®¤ä½ èƒ½å¤Ÿï¼š

- [ ] ç†è§£ RAG çš„åŸç†å’Œä¼˜åŠ¿
- [ ] è®¾è®¡å¹¶å®ç° RAG ç³»ç»Ÿ
- [ ] ä¼˜åŒ–å‘é‡æœç´¢æ€§èƒ½ï¼ˆæŸ¥è¯¢ä¼˜åŒ–ã€ç´¢å¼•ä¼˜åŒ–ï¼‰
- [ ] å®ç°æ··åˆæ£€ç´¢ï¼ˆç¨ å¯† + ç¨€ç–ï¼‰
- [ ] ä½¿ç”¨é‡æ’åºæå‡ç»“æœè´¨é‡
- [ ] å®ç°å¤šæ¨¡æ€æ£€ç´¢
- [ ] è¿›è¡Œæ€§èƒ½è°ƒä¼˜ï¼ˆæ‰¹é‡å¤„ç†ã€ç¼“å­˜ã€å¹¶å‘ï¼‰
- [ ] æ„å»ºå®Œæ•´çš„ AI æœç´¢ç³»ç»Ÿ

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **RAG æ¶æ„**ï¼šæ£€ç´¢ + ç”Ÿæˆï¼Œå‡å°‘ LLM å¹»è§‰
2. **å‘é‡ä¼˜åŒ–**ï¼šæŸ¥è¯¢é‡å†™ã€æ··åˆæœç´¢ã€é‡æ’åº
3. **æ··åˆæ£€ç´¢**ï¼šç¨ å¯†å‘é‡ + ç¨€ç–å…³é”®è¯
4. **æ€§èƒ½è°ƒä¼˜**ï¼šæ‰¹é‡å¤„ç†ã€ç¼“å­˜ã€å¹¶å‘æŸ¥è¯¢
5. **å®æˆ˜åº”ç”¨**ï¼šAI æœç´¢ç³»ç»Ÿï¼ˆå‰ç«¯ + åç«¯ + æ•°æ®åº“ï¼‰

## ğŸ“š å»¶ä¼¸é˜…è¯»

- [ç¬¬28ç« ï¼šMilvus å‘é‡æ•°æ®åº“ â†’](./chapter-27)
- [ç¬¬26ç« ï¼šInfluxDB æ—¶åºæ•°æ®åº“ â†’](./chapter-25)
- [LangChain æ–‡æ¡£](https://docs.langchain.com/)
- [RAG æŠ€æœ¯æŒ‡å—](https://www.anthropic.com/index/retrieval-augmented-generation)

---

**æ›´æ–°æ—¶é—´**ï¼š2026å¹´2æœˆ | **ç‰ˆæœ¬**ï¼šv1.0
