# DevOps ç»¼åˆå®žæˆ˜é¡¹ç›®

æ¬¢è¿Žæ¥åˆ° DevOps ç»¼åˆå®žæˆ˜ï¼æœ¬ç« å°†å¸¦ä½ å®Œæˆ 4 ä¸ªä¼ä¸šçº§å®žæˆ˜é¡¹ç›®ï¼Œå°†å‰é¢å­¦åˆ°çš„æ‰€æœ‰æŠ€æœ¯æ•´åˆåº”ç”¨ã€‚

## å®žæˆ˜é¡¹ç›®æ¦‚è§ˆ

```
ðŸš€ DevOps ç»¼åˆå®žæˆ˜
â”œâ”€ å®žæˆ˜1ï¼šå¾®æœåŠ¡æž¶æž„å®Œæ•´éƒ¨ç½² [â±ï¸ 8å°æ—¶]
â”‚   â”œâ”€ ä¸‰å±‚æž¶æž„åº”ç”¨
â”‚   â”œâ”€ Docker + Kubernetes
â”‚   â”œâ”€ Jenkins + Argo CD
â”‚   â”œâ”€ Prometheus + Grafana
â”‚   â””â”€ ELK Stack
â”‚
â”œâ”€ å®žæˆ˜2ï¼šå¤šçŽ¯å¢ƒç®¡ç†å®žæˆ˜ [â±ï¸ 6å°æ—¶]
â”‚   â”œâ”€ dev/staging/prod ä¸‰çŽ¯å¢ƒ
â”‚   â”œâ”€ GitOps å¤šçŽ¯å¢ƒç­–ç•¥
â”‚   â”œâ”€ çŽ¯å¢ƒéš”ç¦»ä¸Žé…ç½®ç®¡ç†
â”‚   â””â”€ è“ç»¿/é‡‘ä¸é›€å‘å¸ƒ
â”‚
â”œâ”€ å®žæˆ˜3ï¼šäº‘å¹³å°å®Œæ•´éƒ¨ç½² [â±ï¸ 10å°æ—¶]
â”‚   â”œâ”€ Terraform åˆ›å»ºåŸºç¡€è®¾æ–½
â”‚   â”œâ”€ AWS EKS / é˜¿é‡Œäº‘ ACK
â”‚   â”œâ”€ RDS / Redis äº‘æœåŠ¡
â”‚   â””â”€ CI/CD + GitOps å®Œæ•´æµç¨‹
â”‚
â””â”€ å®žæˆ˜4ï¼šç¾éš¾æ¢å¤ä¸Žé«˜å¯ç”¨ [â±ï¸ 8å°æ—¶]
    â”œâ”€ å¤šåŒºåŸŸéƒ¨ç½²
    â”œâ”€ è‡ªåŠ¨å¤‡ä»½ä¸Žæ¢å¤
    â”œâ”€ æ•…éšœè½¬ç§»
    â””â”€ æ¼”ç»ƒè®¡åˆ’
```

---

## å®žæˆ˜1ï¼šå¾®æœåŠ¡æž¶æž„å®Œæ•´éƒ¨ç½²

### é¡¹ç›®æž¶æž„

æˆ‘ä»¬å°†éƒ¨ç½²ä¸€ä¸ªå®Œæ•´çš„ç”µå•†ç³»ç»Ÿï¼ŒåŒ…å«ä»¥ä¸‹å¾®æœåŠ¡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   API Gateway               â”‚
â”‚              (Nginx / Kong)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           â”‚           â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ User  â”‚  â”‚ Productâ”‚  â”‚ Order â”‚    â”‚ Payment   â”‚
â”‚ Serviceâ”‚  â”‚ Serviceâ”‚  â”‚ Serviceâ”‚    â”‚ Service   â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
    â”‚          â”‚          â”‚              â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚           PostgreSQL (ä¸»æ•°æ®åº“)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           Redis (ç¼“å­˜)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚         RabbitMQ (æ¶ˆæ¯é˜Ÿåˆ—)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### é¡¹ç›®1.1ï¼šå‡†å¤‡åº”ç”¨ä»£ç 

**åˆ›å»ºé¡¹ç›®ç»“æž„**

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir microshop && cd microshop
mkdir -p services/{user-service,product-service,order-service,payment-service}
mkdir -f k8s/{base,overlays/{dev,staging,prod}}
mkdir -f ci/{jenkins,argocd}
mkdir -f monitor/{prometheus,grafana}
```

**1. ç”¨æˆ·æœåŠ¡**

`services/user-service/app.py`ï¼š
```python
from flask import Flask, jsonify, request
from flask_sqlalchemy import SQLAlchemy
import os

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DB_URL', 'postgresql://user:pass@db:5432/microshop')
db = SQLAlchemy(app)

class User(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    username = db.Column(db.String(80), unique=True, nullable=False)
    email = db.Column(db.String(120), unique=True, nullable=False)

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy', 'service': 'user-service'})

@app.route('/users', methods=['GET', 'POST'])
def users():
    if request.method == 'POST':
        data = request.json
        user = User(username=data['username'], email=data['email'])
        db.session.add(user)
        db.session.commit()
        return jsonify({'id': user.id}), 201
    users = User.query.all()
    return jsonify([{'id': u.id, 'username': u.username} for u in users])

if __name__ == '__main__':
    with app.app_context():
        db.create_all()
    app.run(host='0.0.0.0', port=5001)
```

`services/user-service/Dockerfile`ï¼š
```dockerfile
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY app.py .

EXPOSE 5001
HEALTHCHECK --interval=30s --timeout=3s \
  CMD curl -f http://localhost:5001/health || exit 1

CMD ["python", "app.py"]
```

`services/user-service/requirements.txt`ï¼š
```txt
Flask==3.0.0
Flask-SQLAlchemy==3.1.1
psycopg2-binary==2.9.9
```

**2. äº§å“æœåŠ¡**

`services/product-service/app.py`ï¼š
```python
from flask import Flask, jsonify, request
from flask_sqlalchemy import SQLAlchemy
import os
import redis

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DB_URL', 'postgresql://user:pass@db:5432/microshop')
db = SQLAlchemy(app)

# Redis ç¼“å­˜
cache = redis.Redis(host=os.getenv('REDIS_HOST', 'redis'), port=6379, decode_responses=True)

class Product(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(120), nullable=False)
    price = db.Column(db.Float, nullable=False)
    stock = db.Column(db.Integer, nullable=False)

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy', 'service': 'product-service'})

@app.route('/products', methods=['GET', 'POST'])
def products():
    if request.method == 'POST':
        data = request.json
        product = Product(name=data['name'], price=data['price'], stock=data['stock'])
        db.session.add(product)
        db.session.commit()
        # æ¸…é™¤ç¼“å­˜
        cache.delete('products:all')
        return jsonify({'id': product.id}), 201

    # å°è¯•ä»Žç¼“å­˜èŽ·å–
    cached = cache.get('products:all')
    if cached:
        return jsonify(eval(cached))

    products = Product.query.all()
    result = [{'id': p.id, 'name': p.name, 'price': p.price} for p in products]
    # ç¼“å­˜1å°æ—¶
    cache.setex('products:all', 3600, str(result))
    return jsonify(result)

if __name__ == '__main__':
    with app.app_context():
        db.create_all()
    app.run(host='0.0.0.0', port=5002)
```

**3. è®¢å•æœåŠ¡**

`services/order-service/app.py`ï¼š
```python
from flask import Flask, jsonify, request
from flask_sqlalchemy import SQLAlchemy
import os
import pika
import json

app = Flask(__name__)
app.config['SQLALCHEMY_DATABASE_URI'] = os.getenv('DB_URL', 'postgresql://user:pass@db:5432/microshop')
db = SQLAlchemy(app)

class Order(db.Model):
    id = db.Column(db.Integer, primary_key=True)
    user_id = db.Column(db.Integer, nullable=False)
    product_id = db.Column(db.Integer, nullable=False)
    status = db.Column(db.String(50), default='pending')

@app.route('/health', methods=['GET'])
def health():
    return jsonify({'status': 'healthy', 'service': 'order-service'})

@app.route('/orders', methods=['POST'])
def create_order():
    data = request.json
    order = Order(user_id=data['user_id'], product_id=data['product_id'])
    db.session.add(order)
    db.session.commit()

    # å‘é€æ¶ˆæ¯åˆ° RabbitMQ
    connection = pika.BlockingConnection(pika.ConnectionParameters(host='rabbitmq'))
    channel = connection.channel()
    channel.queue_declare(queue='orders')
    channel.basic_publish(exchange='', routing_key='orders', body=json.dumps({
        'order_id': order.id,
        'user_id': order.user_id,
        'product_id': order.product_id
    }))
    connection.close()

    return jsonify({'id': order.id}), 201

if __name__ == '__main__':
    with app.app_context():
        db.create_all()
    app.run(host='0.0.0.0', port=5003)
```

**4. API Gateway**

`services/api-gateway/nginx.conf`ï¼š
```nginx
upstream user_service {
    least_conn;
    server user-service:5001;
    server user-service-2:5001;
}

upstream product_service {
    least_conn;
    server product-service:5002;
}

upstream order_service {
    least_conn;
    server order-service:5003;
}

server {
    listen 80;

    location /api/users {
        proxy_pass http://user_service;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    location /api/products {
        proxy_pass http://product_service;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    location /api/orders {
        proxy_pass http://order_service;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }

    location /health {
        access_log off;
        return 200 "healthy\n";
        add_header Content-Type text/plain;
    }
}
```

`services/api-gateway/Dockerfile`ï¼š
```dockerfile
FROM nginx:alpine
COPY nginx.conf /etc/nginx/conf.d/default.conf
EXPOSE 80
```

### é¡¹ç›®1.2ï¼šKubernetes éƒ¨ç½²

**å‘½åç©ºé—´å’Œé…ç½®**

`k8s/base/namespace.yaml`ï¼š
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: microshop
```

**ConfigMap**

`k8s/base/configmap.yaml`ï¼š
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: microshop
data:
  DB_URL: "postgresql://user:password@postgres:5432/microshop"
  REDIS_HOST: "redis"
```

**Secret**

`k8s/base/secret.yaml`ï¼š
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: app-secret
  namespace: microshop
type: Opaque
data:
  DB_PASSWORD: cGFzc3dvcmQxMjM=  # base64 encoded
```

**PostgreSQL éƒ¨ç½²**

`k8s/base/postgres.yaml`ï¼š
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: postgres-pvc
  namespace: microshop
spec:
  accessModes: [ReadWriteOnce]
  resources:
    requests:
      storage: 10Gi

---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: postgres
  namespace: microshop
spec:
  serviceName: postgres
  replicas: 1
  selector:
    matchLabels:
      app: postgres
  template:
    metadata:
      labels:
        app: postgres
    spec:
      containers:
      - name: postgres
        image: postgres:15-alpine
        ports:
        - containerPort: 5432
        env:
        - name: POSTGRES_DB
          value: microshop
        - name: POSTGRES_USER
          value: user
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: app-secret
              key: DB_PASSWORD
        volumeMounts:
        - name: postgres-storage
          mountPath: /var/lib/postgresql/data
        livenessProbe:
          exec:
            command: ["pg_isready", "-U", "user"]
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          exec:
            command: ["pg_isready", "-U", "user"]
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: postgres-storage
        persistentVolumeClaim:
          claimName: postgres-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: postgres
  namespace: microshop
spec:
  selector:
    app: postgres
  ports:
  - port: 5432
    targetPort: 5432
```

**Redis éƒ¨ç½²**

`k8s/base/redis.yaml`ï¼š
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
  namespace: microshop
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
apiVersion: v1
kind: Service
metadata:
  name: redis
  namespace: microshop
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
```

**RabbitMQ éƒ¨ç½²**

`k8s/base/rabbitmq.yaml`ï¼š
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rabbitmq
  namespace: microshop
spec:
  replicas: 1
  selector:
    matchLabels:
      app: rabbitmq
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      containers:
      - name: rabbitmq
        image: rabbitmq:3.12-management-alpine
        ports:
        - containerPort: 5672
        - containerPort: 15672
        env:
        - name: RABBITMQ_DEFAULT_USER
          value: admin
        - name: RABBITMQ_DEFAULT_PASS
          value: admin123

---
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq
  namespace: microshop
spec:
  selector:
    app: rabbitmq
  ports:
  - name: amqp
    port: 5672
    targetPort: 5672
  - name: management
    port: 15672
    targetPort: 15672
```

**ç”¨æˆ·æœåŠ¡éƒ¨ç½²**

`k8s/base/user-service.yaml`ï¼š
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service
  namespace: microshop
spec:
  replicas: 2
  selector:
    matchLabels:
      app: user-service
  template:
    metadata:
      labels:
        app: user-service
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "5001"
    spec:
      containers:
      - name: user-service
        image: microshop/user-service:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 5001
        env:
        - name: DB_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: DB_URL
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"
        livenessProbe:
          httpGet:
            path: /health
            port: 5001
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 5001
          initialDelaySeconds: 5
          periodSeconds: 5

---
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: microshop
spec:
  selector:
    app: user-service
  ports:
  - port: 5001
    targetPort: 5001

---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: user-service-hpa
  namespace: microshop
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: user-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
```

**äº§å“æœåŠ¡éƒ¨ç½²**

`k8s/base/product-service.yaml`ï¼š
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: product-service
  namespace: microshop
spec:
  replicas: 2
  selector:
    matchLabels:
      app: product-service
  template:
    metadata:
      labels:
        app: product-service
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "5002"
    spec:
      containers:
      - name: product-service
        image: microshop/product-service:latest
        imagePullPolicy: IfNotPresent
        ports:
        - containerPort: 5002
        env:
        - name: DB_URL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: DB_URL
        - name: REDIS_HOST
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: REDIS_HOST
        resources:
          requests:
            memory: "128Mi"
            cpu: "100m"
          limits:
            memory: "256Mi"
            cpu: "200m"

---
apiVersion: v1
kind: Service
metadata:
  name: product-service
  namespace: microshop
spec:
  selector:
    app: product-service
  ports:
  - port: 5002
    targetPort: 5002
```

**API Gateway éƒ¨ç½²**

`k8s/base/api-gateway.yaml`ï¼š
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
  namespace: microshop
spec:
  replicas: 2
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
    spec:
      containers:
      - name: nginx
        image: microshop/api-gateway:latest
        ports:
        - containerPort: 80
        resources:
          requests:
            memory: "64Mi"
            cpu: "50m"
          limits:
            memory: "128Mi"
            cpu: "100m"

---
apiVersion: v1
kind: Service
metadata:
  name: api-gateway
  namespace: microshop
spec:
  selector:
    app: api-gateway
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 80
```

**Ingress**

`k8s/base/ingress.yaml`ï¼š
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: microshop-ingress
  namespace: microshop
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/rate-limit: "100"
spec:
  tls:
  - hosts:
    - api.microshop.com
    secretName: microshop-tls
  rules:
  - host: api.microshop.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: api-gateway
            port:
              number: 80
```

### é¡¹ç›®1.3ï¼šKustomize å¤šçŽ¯å¢ƒé…ç½®

**å¼€å‘çŽ¯å¢ƒ**

`k8s/overlays/dev/kustomization.yaml`ï¼š
```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: microshop-dev

resources:
- ../../base

replicas:
- name: user-service
  count: 1
- name: product-service
  count: 1

images:
- name: microshop/user-service
  newTag: dev
- name: microshop/product-service
  newTag: dev
```

**ç”Ÿäº§çŽ¯å¢ƒ**

`k8s/overlays/prod/kustomization.yaml`ï¼š
```yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: microshop-prod

resources:
- ../../base

replicas:
- name: user-service
  count: 3
- name: product-service
  count: 3

patches:
- patch: |-
    - op: add
      path: /spec/template/spec/containers/0/resources/limits
      value:
        memory: "512Mi"
        cpu: "500m"
  target:
    kind: Deployment
```

### é¡¹ç›®1.4ï¼šCI/CD æµæ°´çº¿

**Jenkins Pipeline**

`ci/jenkins/Jenkinsfile`ï¼š
```groovy
pipeline {
    agent any

    environment {
        REGISTRY = 'your-registry.com'
        PROJECT = 'microshop'
    }

    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Build') {
            parallel {
                stage('Build User Service') {
                    steps {
                        dir('services/user-service') {
                            sh 'docker build -t ${REGISTRY}/${PROJECT}/user-service:${BUILD_NUMBER} .'
                        }
                    }
                }
                stage('Build Product Service') {
                    steps {
                        dir('services/product-service') {
                            sh 'docker build -t ${REGISTRY}/${PROJECT}/product-service:${BUILD_NUMBER} .'
                        }
                    }
                }
            }
        }

        stage('Security Scan') {
            steps {
                script {
                    def images = [
                        "${REGISTRY}/${PROJECT}/user-service:${BUILD_NUMBER}",
                        "${REGISTRY}/${PROJECT}/product-service:${BUILD_NUMBER}"
                    ]
                    images.each { img ->
                        sh "trivy image --severity HIGH,CRITICAL ${img}"
                    }
                }
            }
        }

        stage('Push') {
            steps {
                script {
                    def images = ['user-service', 'product-service']
                    withDockerRegistry([url: "https://${REGISTRY}", credentialsId: 'docker-registry']) {
                        images.each { img ->
                            sh """
                                docker push ${REGISTRY}/${PROJECT}/${img}:${BUILD_NUMBER}
                                docker tag ${REGISTRY}/${PROJECT}/${img}:${BUILD_NUMBER} ${REGISTRY}/${PROJECT}/${img}:latest
                                docker push ${REGISTRY}/${PROJECT}/${img}:latest
                            """
                        }
                    }
                }
            }
        }

        stage('Deploy to Dev') {
            steps {
                dir('k8s/overlays/dev') {
                    sh "kubectl set image deployment/user-service user-service=${REGISTRY}/${PROJECT}/user-service:${BUILD_NUMBER} -n microshop-dev"
                    sh "kubectl set image deployment/product-service product-service=${REGISTRY}/${PROJECT}/product-service:${BUILD_NUMBER} -n microshop-dev"
                }
            }
        }

        stage('Run Tests') {
            steps {
                sh '''
                    # ç­‰å¾…éƒ¨ç½²å°±ç»ª
                    kubectl rollout status deployment/user-service -n microshop-dev
                    kubectl rollout status deployment/product-service -n microshop-dev

                    # è¿è¡Œé›†æˆæµ‹è¯•
                    python tests/integration.py
                '''
            }
        }

        stage('Deploy to Staging') {
            when {
                branch 'main'
            }
            steps {
                input message: 'éƒ¨ç½²åˆ°æµ‹è¯•çŽ¯å¢ƒï¼Ÿ', ok: 'éƒ¨ç½²'
                dir('k8s/overlays/staging') {
                    sh "kubectl apply -k ."
                }
            }
        }

        stage('Deploy to Production') {
            when {
                branch 'main'
            }
            steps {
                input message: 'éƒ¨ç½²åˆ°ç”Ÿäº§çŽ¯å¢ƒï¼Ÿ', ok: 'ç¡®è®¤éƒ¨ç½²'
                dir('k8s/overlays/prod') {
                    sh "kubectl apply -k ."
                }
            }
        }
    }

    post {
        success {
            emailext(
                subject: "éƒ¨ç½²æˆåŠŸ: ${env.JOB_NAME} #${env.BUILD_NUMBER}",
                body: "æž„å»º ${env.BUILD_NUMBER} å·²æˆåŠŸéƒ¨ç½²åˆ°ç”Ÿäº§çŽ¯å¢ƒ",
                to: 'team@example.com'
            )
        }
        failure {
            emailext(
                subject: "éƒ¨ç½²å¤±è´¥: ${env.JOB_NAME} #${env.BUILD_NUMBER}",
                body: "æž„å»º ${env.BUILD_NUMBER} å¤±è´¥",
                to: 'team@example.com'
            )
        }
    }
}
```

### é¡¹ç›®1.5ï¼šArgo CD GitOps

**Application**

`ci/argocd/microshop-app.yaml`ï¼š
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: microshop
  namespace: argocd
spec:
  project: default

  source:
    repoURL: https://github.com/yourorg/microshop-k8s.git
    targetRevision: main
    path: k8s/overlays/dev

  destination:
    server: https://kubernetes.default.svc
    namespace: microshop-dev

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
    - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
```

### é¡¹ç›®1.6ï¼šç›‘æŽ§å’Œæ—¥å¿—

**Prometheus ServiceMonitor**

`monitor/prometheus/servicemonitor.yaml`ï¼š
```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: microshop-services
  namespace: microshop
spec:
  selector:
    matchLabels:
      app: user-service
  endpoints:
  - port: http
    path: /metrics
    interval: 30s
```

**Grafana Dashboard**

`monitor/grafana/dashboards/microshop.json`ï¼š
```json
{
  "title": "MicroShop ç›‘æŽ§",
  "panels": [
    {
      "title": "è¯·æ±‚é‡",
      "targets": [
        {
          "expr": "sum(rate(http_requests_total{namespace='microshop'}[5m]))"
        }
      ]
    },
    {
      "title": "é”™è¯¯çŽ‡",
      "targets": [
        {
          "expr": "sum(rate(http_requests_total{status=~'5..'}[5m])) / sum(rate(http_requests_total[5m]))"
        }
      ]
    },
    {
      "title": "P95 å»¶è¿Ÿ",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, http_request_duration_seconds_bucket)"
        }
      ]
    }
  ]
}
```

**Filebeat æ—¥å¿—æ”¶é›†**

`monitor/filebeat/filebeat-configmap.yaml`ï¼š
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: filebeat-config
  namespace: microshop
data:
  filebeat.yml: |-
    filebeat.inputs:
    - type: container
      enabled: true
      paths:
        - /var/log/containers/*.log
      processors:
        - add_kubernetes_metadata:
            host: ${NODE_NAME}
            matchers:
            - logs_path:
                logs_path: "/var/log/containers/"

    output.elasticsearch:
      hosts: ["elasticsearch:9200"]
      index: "microshop-%{+yyyy.MM.dd}"

    setup.template.name: "microshop"
    setup.template.pattern: "microshop-*"
```

---

## å®žæˆ˜2ï¼šå¤šçŽ¯å¢ƒç®¡ç†å®žæˆ˜

### é¡¹ç›®2.1ï¼šçŽ¯å¢ƒç­–ç•¥

**çŽ¯å¢ƒå®šä¹‰**

| çŽ¯å¢ƒ | ç”¨é€” | å‰¯æœ¬æ•° | èµ„æºé™åˆ¶ | è‡ªåŠ¨ä¼¸ç¼© |
|------|------|--------|----------|----------|
| **dev** | å¼€å‘æµ‹è¯• | 1 | å° | ç¦ç”¨ |
| **staging** | é¢„å‘å¸ƒ | 2 | ä¸­ | å¯ç”¨ |
| **prod** | ç”Ÿäº§çŽ¯å¢ƒ | 3+ | å¤§ | å¯ç”¨ |

**Kustomize ç›®å½•ç»“æž„**

```
k8s/
â”œâ”€â”€ base/                 # åŸºç¡€é…ç½®
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”œâ”€â”€ service.yaml
â”‚   â””â”€â”€ configmap.yaml
â”œâ”€â”€ overlays/
â”‚   â”œâ”€â”€ dev/             # å¼€å‘çŽ¯å¢ƒ
â”‚   â”‚   â””â”€â”€ kustomization.yaml
â”‚   â”œâ”€â”€ staging/         # æµ‹è¯•çŽ¯å¢ƒ
â”‚   â”‚   â”œâ”€â”€ kustomization.yaml
â”‚   â”‚   â””â”€â”€ patches/
â”‚   â””â”€â”€ prod/            # ç”Ÿäº§çŽ¯å¢ƒ
â”‚       â”œâ”€â”€ kustomization.yaml
â”‚       â””â”€â”€ patches/
```

### é¡¹ç›®2.2ï¼šçŽ¯å¢ƒéš”ç¦»

**å‘½åç©ºé—´éš”ç¦»**

`k8s/overlays/prod/namespace.yaml`ï¼š
```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: microshop-prod
  labels:
    environment: production
    team: backend

---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: microshop-prod
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi

---
apiVersion: v1
kind: LimitRange
metadata:
  name: limit-range
  namespace: microshop-prod
spec:
  limits:
  - default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "100m"
      memory: "128Mi"
    type: Container
```

**ç½‘ç»œç­–ç•¥**

`k8s/overlays/prod/network-policy.yaml`ï¼š
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
  namespace: microshop-prod
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-same-namespace
  namespace: microshop-prod
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector: {}
  egress:
  - to:
    - podSelector: {}
  - to:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    ports:
    - protocol: UDP
      port: 53

---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-ingress
  namespace: microshop-prod
spec:
  podSelector:
    matchLabels:
      app: api-gateway
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 80
```

### é¡¹ç›®2.3ï¼šé…ç½®ç®¡ç†

**çŽ¯å¢ƒé…ç½®**

`k8s/overlays/dev/configmap.yaml`ï¼š
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: microshop-dev
data:
  LOG_LEVEL: "DEBUG"
  DB_POOL_SIZE: "5"
  CACHE_TTL: "3600"
  FEATURE_FLAGS: |
    new-ui: true
    beta-api: true
```

`k8s/overlays/prod/configmap.yaml`ï¼š
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: microshop-prod
data:
  LOG_LEVEL: "INFO"
  DB_POOL_SIZE: "20"
  CACHE_TTL: "7200"
  FEATURE_FLAGS: |
    new-ui: true
    beta-api: false
```

### é¡¹ç›®2.4ï¼šéƒ¨ç½²ç­–ç•¥

**è“ç»¿éƒ¨ç½²**

`k8s/overlays/prod/blue-green-service.yaml`ï¼š
```yaml
apiVersion: v1
kind: Service
metadata:
  name: user-service
  namespace: microshop-prod
spec:
  selector:
    app: user-service
    version: blue  # æˆ– green
  ports:
  - port: 5001
    targetPort: 5001
```

**é‡‘ä¸é›€éƒ¨ç½²**

`k8s/overlays/prod/canary-deployment.yaml`ï¼š
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: user-service-canary
  namespace: microshop-prod
spec:
  replicas: 1  # 10% æµé‡
  selector:
    matchLabels:
      app: user-service
      track: canary
  template:
    metadata:
      labels:
        app: user-service
        track: canary
        version: v2.0
    spec:
      containers:
      - name: user-service
        image: microshop/user-service:v2.0
```

**ä½¿ç”¨ Argo Rollouts**

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Rollout
metadata:
  name: user-service
  namespace: microshop-prod
spec:
  replicas: 5
  strategy:
    canary:
      steps:
      - setWeight: 10
      - pause: {duration: 10m}
      - setWeight: 30
      - pause: {duration: 10m}
      - setWeight: 50
      - pause: {duration: 10m}
      - setWeight: 100
      canaryService: user-service-canary
      stableService: user-service-stable
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: user-service
  template:
    # ... deployment template
```

---

## å®žæˆ˜3ï¼šäº‘å¹³å°å®Œæ•´éƒ¨ç½²

### é¡¹ç›®3.1ï¼šTerraform åŸºç¡€è®¾æ–½

**AWS EKS é›†ç¾¤**

`terraform/main.tf`ï¼š
```hcl
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.0"
    }
  }
}

provider "aws" {
  region = var.region
}

# VPC
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "5.0.0"

  name = "${var.project_name}-vpc"
  cidr = "10.0.0.0/16"

  azs             = ["${var.region}a", "${var.region}b", "${var.region}c"]
  private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
  public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]

  enable_nat_gateway = true
  single_nat_gateway = true

  tags = {
    Environment = var.environment
    Project     = var.project_name
  }
}

# EKS Cluster
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "19.17.2"

  cluster_name    = "${var.project_name}-eks"
  cluster_version = "1.27"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  cluster_endpoint_public_access  = true
  cluster_endpoint_private_access = true

  eks_managed_node_groups = {
    general = {
      desired_size = 3
      min_size     = 2
      max_size     = 10

      instance_types = ["t3.medium"]
      capacity_type  = "ON_DEMAND"

      labels = {
        Environment = var.environment
      }
    }

    spot = {
      desired_size = 2
      min_size     = 0
      max_size     = 5

      instance_types = ["t3a.small"]
      capacity_type  = "SPOT"

      labels = {
        Environment = var.environment
        Type        = "spot"
      }
    }
  }

  tags = {
    Environment = var.environment
  }
}

# RDS PostgreSQL
module "db" {
  source  = "terraform-aws-modules/rds/aws"
  version = "~> 5.0"

  identifier = "${var.project_name}-db"

  engine            = "postgres"
  engine_version    = "15.3"
  instance_class    = "db.t3.medium"
  allocated_storage = 20

  db_name  = "microshop"
  username = "admin"
  port     = 5432

  vpc_security_group_ids = [module.security_group.security_group_id]
  db_subnet_group_name   = module.vpc.database_subnet_group

  managed_database_family = "postgres15"

  tags = {
    Environment = var.environment
  }
}

# ElastiCache Redis
module "elasticache" {
  source  = "terraform-aws-modules/elasticache/aws"
  version = "~> 1.0"

  cluster_id      = "${var.project_name}-redis"
  engine_version  = "7.0"
  node_type       = "cache.t3.small"
  num_cache_nodes = 2

  subnet_group_name  = module.vpc.redis_subnet_group
  security_group_ids = [module.security_group.security_group_id]

  tags = {
    Environment = var.environment
  }
}

# Outputs
output "cluster_endpoint" {
  description = "EKS cluster endpoint"
  value       = module.eks.cluster_endpoint
}

output "db_endpoint" {
  description = "RDS endpoint"
  value       = module.db.db_instance_endpoint
}

output "redis_endpoint" {
  description = "Redis endpoint"
  value       = module.elasticache.cluster_endpoint
}
```

`terraform/variables.tf`ï¼š
```hcl
variable "region" {
  description = "AWS region"
  type        = string
  default     = "us-east-1"
}

variable "project_name" {
  description = "Project name"
  type        = string
  default     = "microshop"
}

variable "environment" {
  description = "Environment name"
  type        = string
  default     = "prod"

  validation {
    condition     = contains(["dev", "staging", "prod"], var.environment)
    error_message = "Environment must be dev, staging, or prod."
  }
}
```

**éƒ¨ç½²åŸºç¡€è®¾æ–½**

```bash
# åˆå§‹åŒ–
terraform init

# è§„åˆ’
terraform plan -var="environment=prod"

# åº”ç”¨
terraform apply -var="environment=prod" -auto-approve
```

### é¡¹ç›®3.2ï¼šé…ç½® kubectl

```bash
# æ›´æ–° kubeconfig
aws eks update-kubeconfig --region us-east-1 --name microshop-eks

# éªŒè¯
kubectl get nodes
kubectl get svc
```

### é¡¹ç›®3.3ï¼šéƒ¨ç½²åº”ç”¨

```bash
# åˆ›å»º secrets
kubectl create secret generic db-credentials \
  --from-literal=username=admin \
  --from-literal=password=$(aws secretsmanager get-secret-value --secret-id prod/db/password --query SecretString --output text)

# éƒ¨ç½²åº”ç”¨
kubectl apply -k k8s/overlays/prod

# ç­‰å¾…å°±ç»ª
kubectl rollout status deployment/user-service -n microshop-prod
```

---

## å®žæˆ˜4ï¼šç¾éš¾æ¢å¤ä¸Žé«˜å¯ç”¨

### é¡¹ç›®4.1ï¼šå¤šåŒºåŸŸéƒ¨ç½²

**è·¨åŒºåŸŸæž¶æž„**

```
ä¸»åŒºåŸŸ (us-east-1)        å¤‡åŒºåŸŸ (us-west-2)
     â”‚                          â”‚
     â”œâ”€ EKS Cluster             â”œâ”€ EKS Cluster
     â”œâ”€ RDS (Multi-AZ)          â”œâ”€ RDS Read Replica
     â”œâ”€ ElastiCache             â”œâ”€ ElastiCache
     â””â”€ S3                      â””â”€ S3 (Cross-Region Replication)
```

**Terraform å¤šåŒºåŸŸé…ç½®**

```hcl
# ä¸»åŒºåŸŸ
module "primary" {
  source = "./modules/infrastructure"

  region      = "us-east-1"
  environment = "prod"

  enable_secondary = false
}

# å¤‡åŒºåŸŸ
module "secondary" {
  source = "./modules/infrastructure"

  region      = "us-west-2"
  environment = "prod"

  # ä¸»åŒºåŸŸä¿¡æ¯
  primary_region     = "us-east-1"
  primary_vpc_cidr   = module.primary.vpc_cidr
  primary_db_endpoint = module.primary.db_endpoint

  enable_secondary = true
}
```

### é¡¹ç›®4.2ï¼šè‡ªåŠ¨å¤‡ä»½

**RDS è‡ªåŠ¨å¤‡ä»½**

```hcl
resource "aws_db_instance" "primary" {
  # ...
  backup_retention_period = 30  # ä¿ç•™30å¤©
  backup_window         = "03:00-04:00"

  # å¼€å¯ PITR (Point-In-Time Recovery)
  skip_final_snapshot = false
  final_snapshot_identifier = "microshop-final-snapshot"
}
```

**EBS å¿«ç…§**

```hcl
resource "aws_ebs_volume" "data" {
  # ...
}

resource "aws_backup_vault" "microshop" {
  name = "microshop-backup-vault"
}

resource "aws_backup_plan" "daily" {
  name = "daily-backup-plan"

  rule {
    rule_name           = "daily-backup"
    target_vault_name   = aws_backup_vault.microshop.name
    schedule_expression = "cron(0 2 * * ? *)"

    lifecycle {
      delete_after = 30
    }
  }
}
```

### é¡¹ç›®4.3ï¼šæ•…éšœè½¬ç§»

**DNS æ•…éšœè½¬ç§»**

```hcl
# Route53 Health Check
resource "aws_route53_health_check" "primary" {
  fqdn              = "api.microshop.com"
  port              = 443
  type              = "HTTPS"
  resource_path     = "/health"
  request_interval  = 30
  failure_threshold = 3
}

# Primary Record
resource "aws_route53_record" "primary" {
  zone_id = var.hosted_zone_id
  name    = "api.microshop.com"
  type    = "A"

  alias {
    name                   = aws_lb.primary.dns_name
    zone_id                = aws_lb.primary.zone_id
    evaluate_target_health = true
  }

  failover_routing_policy {
    type = "PRIMARY"
  }

  set_identifier = "primary-region"
  health_check_id = aws_route53_health_check.primary.id
}

# Secondary Record
resource "aws_route53_record" "secondary" {
  zone_id = var.hosted_zone_id
  name    = "api.microshop.com"
  type    = "A"

  alias {
    name                   = aws_lb.secondary.dns_name
    zone_id                = aws_lb.secondary.zone_id
    evaluate_target_health = true
  }

  failover_routing_policy {
    type = "SECONDARY"
  }

  set_identifier = "secondary-region"
}
```

### é¡¹ç›®4.4ï¼šç¾éš¾æ¢å¤æ¼”ç»ƒ

**æ¼”ç»ƒè®¡åˆ’**

```yaml
# disaster-recovery-drill.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: drill-plan
  namespace: microshop-prod
data:
  plan: |
    æ¼”ç»ƒè®¡åˆ’ï¼šåŒºåŸŸæ•…éšœè½¬ç§»

    é¢‘çŽ‡ï¼šæ¯å­£åº¦ä¸€æ¬¡

    æ­¥éª¤ï¼š
    1. å‡†å¤‡é˜¶æ®µ
       - é€šçŸ¥å›¢é˜Ÿ
       - å¤‡ä»½å½“å‰é…ç½®
       - è®°å½•å¼€å§‹æ—¶é—´

    2. æ¨¡æ‹Ÿæ•…éšœ
       - åœæ­¢ä¸»åŒºåŸŸ ALB
       - ç›‘æŽ§å¥åº·æ£€æŸ¥
       - éªŒè¯ DNS åˆ‡æ¢

    3. éªŒè¯åŠŸèƒ½
       - æ£€æŸ¥å¤‡åŒºåŸŸæœåŠ¡
       - è¿è¡Œé›†æˆæµ‹è¯•
       - éªŒè¯æ•°æ®åŒæ­¥

    4. æ¢å¤ä¸»åŒºåŸŸ
       - æ¢å¤ä¸»åŒºåŸŸ ALB
       - éªŒè¯ DNS å›žåˆ‡
       - æ•°æ®åŒæ­¥éªŒè¯

    5. å¤ç›˜
       - è®°å½• RTO (Recovery Time Objective)
       - è®°å½• RPO (Recovery Point Objective)
       - æ€»ç»“é—®é¢˜å’Œæ”¹è¿›ç‚¹
```

**è‡ªåŠ¨åŒ–æ¼”ç»ƒè„šæœ¬**

```bash
#!/bin/bash
# drill-test.sh

set -e

DRILL_DATE=$(date +%Y%m%d)
LOG_FILE="drill-${DRILL_DATE}.log"

log() {
  echo "[$(date '+%Y-%m-%d %H:%M:%S')] $1" | tee -a $LOG_FILE
}

log "å¼€å§‹ç¾éš¾æ¢å¤æ¼”ç»ƒ"

# 1. å¤‡ä»½å½“å‰é…ç½®
log "å¤‡ä»½é…ç½®..."
kubectl get all -n microshop-prod -o yaml > backup-${DRILL_DATE}.yaml

# 2. æ¨¡æ‹Ÿæ•…éšœ
log "æ¨¡æ‹Ÿä¸»åŒºåŸŸæ•…éšœ..."
aws elbv2 set-load-balancer-attributes \
  --load-balancer-arn $PRIMARY_ALB_ARN \
  --attributes Key=access_logs.s3.enabled,Value=false

# 3. ç­‰å¾… DNS åˆ‡æ¢
log "ç­‰å¾… DNS åˆ‡æ¢..."
sleep 300

# 4. éªŒè¯æœåŠ¡
log "éªŒè¯æœåŠ¡çŠ¶æ€..."
for i in {1..10}; do
  if curl -f https://api.microshop.com/health; then
    log "æœåŠ¡æ­£å¸¸"
    break
  fi
  log "ç­‰å¾…æœåŠ¡å°±ç»ª... ($i/10)"
  sleep 30
done

# 5. è¿è¡Œæµ‹è¯•
log "è¿è¡Œé›†æˆæµ‹è¯•..."
pytest tests/integration/ --region=secondary

# 6. æ¢å¤
log "æ¢å¤ä¸»åŒºåŸŸ..."
aws elbv2 set-load-balancer-attributes \
  --load-balancer-arn $PRIMARY_ALB_ARN \
  --attributes Key=access_logs.s3.enabled,Value=true

log "æ¼”ç»ƒå®Œæˆï¼"
```

---

## å­¦ä¹ å»ºè®®

### å®žæˆ˜é¡ºåº

1. **å…ˆå®žæˆ˜1**ï¼šæŽŒæ¡å®Œæ•´çš„å¾®æœåŠ¡éƒ¨ç½²æµç¨‹
2. **å†å®žæˆ˜2**ï¼šå­¦ä¹ å¤šçŽ¯å¢ƒç®¡ç†ç­–ç•¥
3. **ç„¶åŽå®žæˆ˜3**ï¼šå®žè·µäº‘å¹³å°éƒ¨ç½²
4. **æœ€åŽå®žæˆ˜4**ï¼šæŽŒæ¡é«˜å¯ç”¨å’Œç¾éš¾æ¢å¤

### å®žè·µå»ºè®®

1. **æœ¬åœ°éªŒè¯**ï¼šå…ˆåœ¨æœ¬åœ° Docker/K8s æµ‹è¯•
2. **é€æ­¥éƒ¨ç½²**ï¼šä¸€ä¸ªæœåŠ¡ä¸€ä¸ªæœåŠ¡åœ°éƒ¨ç½²
3. **å……åˆ†æµ‹è¯•**ï¼šæ¯ä¸ªé˜¶æ®µéƒ½è¦éªŒè¯åŠŸèƒ½
4. **è®°å½•æ—¥å¿—**ï¼šè¯¦ç»†è®°å½•éƒ¨ç½²è¿‡ç¨‹å’Œé—®é¢˜
5. **å›¢é˜Ÿåä½œ**ï¼šæœ€å¥½2-3äººä¸€èµ·å®Œæˆ

### ç»ƒä¹ ä»»åŠ¡

- [ ] å®Œæˆå®žæˆ˜1ï¼šå¾®æœåŠ¡éƒ¨ç½²
- [ ] å®Œæˆå®žæˆ˜2ï¼šå¤šçŽ¯å¢ƒç®¡ç†
- [ ] å®Œæˆå®žæˆ˜3ï¼šäº‘å¹³å°éƒ¨ç½²
- [ ] å®Œæˆå®žæˆ˜4ï¼šç¾éš¾æ¢å¤æ¼”ç»ƒ
- [ ] æ€»ç»“å®žæˆ˜ç»éªŒ

## æ€»ç»“

é€šè¿‡è¿™ 4 ä¸ªç»¼åˆå®žæˆ˜é¡¹ç›®ï¼Œä½ å°†ï¼š

- æŽŒæ¡ä¼ä¸šçº§å¾®æœåŠ¡æž¶æž„çš„å®Œæ•´éƒ¨ç½²
- ç†è§£å¤šçŽ¯å¢ƒç®¡ç†å’Œé…ç½®ç­–ç•¥
- å­¦ä¼šåœ¨äº‘å¹³å°ä¸Šæž„å»ºåŸºç¡€è®¾æ–½
- å…·å¤‡ç¾éš¾æ¢å¤å’Œé«˜å¯ç”¨èƒ½åŠ›

è¿™äº›éƒ½æ˜¯ DevOps å·¥ç¨‹å¸ˆå¿…å¤‡çš„æ ¸å¿ƒæŠ€èƒ½ï¼

## æŽ¨èèµ„æº

- [Kubernetes å®˜æ–¹æ–‡æ¡£](https://kubernetes.io/docs/)
- [AWS EKS æœ€ä½³å®žè·µ](https://docs.aws.amazon.com/eks/)
- [Terraform Registry](https://registry.terraform.io/)
- [Argo CD æ–‡æ¡£](https://argoproj.github.io/argo-cd/)

---

---

# é™„å½•Dï¼š2024-2026ä¼ä¸šçº§DevOpså®žæˆ˜é¡¹ç›® {#-é™„å½•d2024-2026ä¼ä¸šçº§devopså®žæˆ˜é¡¹ç›®}

> **2024-2026 DevOpsæŠ€æœ¯è¶‹åŠ¿**
>
> æ ¹æ®[æœ€æ–°DevOpsè¶‹åŠ¿åˆ†æž](https://devops.com/top-15-devops-trends-to-watch-in-2026/)ï¼š
> - **Platform Engineering** è¶…è¶Šä¼ ç»ŸDevOpsï¼Œæˆä¸º2025-2026ä¸»æµ
> - **GitOps** æˆä¸ºäº‘åŽŸç”Ÿåº”ç”¨éƒ¨ç½²çš„æ ‡å‡†å®žè·µ
> - **DevSecOps** å®‰å…¨å·¦ç§»æˆä¸ºæ ‡é…
> - **AIOps** AIé©±åŠ¨çš„è‡ªåŠ¨åŒ–è¿ç»´
> - **Internal Developer Platforms (IDPs)** æå‡å¼€å‘è€…ä½“éªŒ
>
> åŸºäºŽè¿™äº›è¶‹åŠ¿ï¼Œæˆ‘ä»¬æ–°å¢ž **2ä¸ªä¼ä¸šçº§DevOpså®žæˆ˜é¡¹ç›®**ï¼Œæ¶µç›–Platform Engineeringå’ŒAIOpsç­‰å‰æ²¿æŠ€æœ¯ã€‚

---

## å®žæˆ˜5ï¼šPlatform Engineering - å†…éƒ¨å¼€å‘è€…å¹³å°ï¼ˆIDPï¼‰

### æŠ€æœ¯æ ˆï¼ˆ2024-2026ä¸»æµï¼‰

æ ¹æ®[Platform Engineeringè¶‹åŠ¿](https://medium.com/@orlando1409/beyond-kubernetes-platform-engineering-trends-for-2026-8f82e09e27e0)ï¼š

```
ðŸ—ï¸ Backstageï¼ˆSpotifyå¼€æºIDPæ¡†æž¶ï¼‰
â˜¸ï¸ Kubernetes + Helm
ðŸ”„ Argo CDï¼ˆGitOpsï¼‰
ðŸ“Š Prometheus + Grafanaï¼ˆç›‘æŽ§ï¼‰
ðŸ” Vaultï¼ˆå¯†é’¥ç®¡ç†ï¼‰
ðŸŽ¨ Traefikï¼ˆAPI Gatewayï¼‰
ðŸ¤– OPAï¼ˆç­–ç•¥å¼•æ“Žï¼‰
ðŸ“š Tech Docsï¼ˆæ–‡æ¡£ï¼‰
```

### é¡¹ç›®ç®€ä»‹

æž„å»ºä¸€ä¸ªå®Œæ•´çš„å†…éƒ¨å¼€å‘è€…å¹³å°ï¼ˆIDPï¼‰ï¼Œç®€åŒ–å¼€å‘è€…ä½“éªŒï¼Œè‡ªåŠ¨åŒ–åŸºç¡€è®¾æ–½ç®¡ç†ã€‚

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
```
ðŸŽ¯ æœåŠ¡ç›®å½•ï¼šè‡ªåŠ¨å‘çŽ°å’Œæ³¨å†Œæ‰€æœ‰æœåŠ¡
ðŸš€ ä¸€é”®éƒ¨ç½²ï¼šé€šè¿‡æ¨¡æ¿å¿«é€Ÿåˆ›å»ºæœåŠ¡
ðŸ“Š å¯è§‚æµ‹æ€§ï¼šç»Ÿä¸€çš„ç›‘æŽ§ã€æ—¥å¿—ã€è¿½è¸ªä»ªè¡¨ç›˜
ðŸ” æƒé™ç®¡ç†ï¼šåŸºäºŽè§’è‰²çš„è®¿é—®æŽ§åˆ¶
ðŸ“š æ–‡æ¡£ç®¡ç†ï¼šè‡ªåŠ¨ç”Ÿæˆå’Œæ›´æ–°æœåŠ¡æ–‡æ¡£
ðŸ”„ è‡ªåŠ©æœåŠ¡ï¼šå¼€å‘è€…è‡ªåŠ©ç®¡ç†èµ„æº
ðŸŽ¨ æ’ä»¶ç³»ç»Ÿï¼šå¯æ‰©å±•çš„æ’ä»¶æž¶æž„
ðŸ¤– AIåŠ©æ‰‹ï¼šæ™ºèƒ½å»ºè®®å’Œæ•…éšœæŽ’æŸ¥
```

### é¡¹ç›®æž¶æž„

```
platform-engineering/
â”œâ”€â”€ backstage/                    # Backstageåº”ç”¨
â”‚   â”œâ”€â”€ app/                      # åº”ç”¨é…ç½®
â”‚   â”‚   â”œâ”€â”€ project.ts           # é¡¹ç›®é…ç½®
â”‚   â”‚   â””â”€â”€ plugins/             # æ’ä»¶é…ç½®
â”‚   â”œâ”€â”€ plugins/                  # è‡ªå®šä¹‰æ’ä»¶
â”‚   â”‚   â”œâ”€â”€ service-template/    # æœåŠ¡æ¨¡æ¿æ’ä»¶
â”‚   â”‚   â”œâ”€â”€ deployment/          # éƒ¨ç½²æ’ä»¶
â”‚   â”‚   â”œâ”€â”€ monitoring/          # ç›‘æŽ§æ’ä»¶
â”‚   â”‚   â””â”€â”€ ai-assistant/        # AIåŠ©æ‰‹æ’ä»¶
â”‚   â”œâ”€â”€ templates/                # æœåŠ¡æ¨¡æ¿
â”‚   â”‚   â”œâ”€â”€ microservice/        # å¾®æœåŠ¡æ¨¡æ¿
â”‚   â”‚   â”œâ”€â”€ serverless/          # Serverlessæ¨¡æ¿
â”‚   â”‚   â””â”€â”€ ml-pipeline/         # MLæµæ°´çº¿æ¨¡æ¿
â”‚   â””â”€â”€ catalog-info.yaml        # æœåŠ¡ç›®å½•é…ç½®
â”‚
â”œâ”€â”€ infrastructure/              # åŸºç¡€è®¾æ–½
â”‚   â”œâ”€â”€ terraform/               # Terraformé…ç½®
â”‚   â”‚   â”œâ”€â”€ modules/             # å¯å¤ç”¨æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ environments/        # çŽ¯å¢ƒé…ç½®
â”‚   â”‚   â””â”€â”€ examples/            # ä½¿ç”¨ç¤ºä¾‹
â”‚   â”œâ”€â”€ kubernetes/              # K8sé…ç½®
â”‚   â”‚   â”œâ”€â”€ base/                # åŸºç¡€é…ç½®
â”‚   â”‚   â”œâ”€â”€ overlays/            # çŽ¯å¢ƒè¦†ç›–
â”‚   â”‚   â””â”€â”€ helm-charts/         # Helm charts
â”‚   â””â”€â”€ ansible/                 # Ansible playbooks
â”‚
â”œâ”€â”€ pipelines/                   # CI/CDæµæ°´çº¿
â”‚   â”œâ”€â”€ jenkins/                 # Jenkinsæµæ°´çº¿
â”‚   â”œâ”€â”€ github-actions/          # GitHub Actions
â”‚   â””â”€â”€ gitlab-ci/               # GitLab CI
â”‚
â”œâ”€â”€ monitoring/                  # ç›‘æŽ§ç³»ç»Ÿ
â”‚   â”œâ”€â”€ prometheus/              # Prometheusé…ç½®
â”‚   â”œâ”€â”€ grafana/                 # Grafanaä»ªè¡¨ç›˜
â”‚   â”œâ”€â”€ loki/                    # æ—¥å¿—èšåˆ
â”‚   â””â”€â”€ tempo/                   # åˆ†å¸ƒå¼è¿½è¸ª
â”‚
â”œâ”€â”€ security/                    # å®‰å…¨é…ç½®
â”‚   â”œâ”€â”€ vault/                   # Vaulté…ç½®
â”‚   â”œâ”€â”€ cert-manager/            # è¯ä¹¦ç®¡ç†
â”‚   â””â”€â”€ policies/                # OPAç­–ç•¥
â”‚
â””â”€â”€ docs/                        # æ–‡æ¡£
    â”œâ”€â”€ architecture.md          # æž¶æž„æ–‡æ¡£
    â”œâ”€â”€ getting-started.md       # å¿«é€Ÿå¼€å§‹
    â””â”€â”€ api-documentation.md     # APIæ–‡æ¡£
```

### æ ¸å¿ƒå®žçŽ°

**1. Backstageé…ç½®**

```yaml
# backstage/app-config.yaml
app:
  title: My Developer Platform
  baseUrl: http://localhost:3000

organization:
  name: My Company

backend:
  baseUrl: http://localhost:7000
  listen:
    port: 7000
  csp:
    connect-src: ["'self'", 'http:', 'https:']
  cors:
    origin: http://localhost:3000
    methods: [GET, HEAD, POST]
    credentials: true

integrations:
  github:
    - host: github.com
      token: ${GITHUB_TOKEN}

  argocd:
    - name: ArgoCD
      url: ${ARGOCD_URL}
      username: ${ARGOCD_USERNAME}
      password: ${ARGOCD_PASSWORD}
      appSelector:
        - matchExpressions:
            - key: app
              operator: In
              values: ["backstage"]

  kubernetes:
    - name: production
      url: ${K8S_PROD_URL}
      caData: ${K8S_PROD_CA_DATA}
      skipTLSVerify: true
      authProvider: serviceAccount
    - name: staging
      url: ${K8S_STAGING_URL}

  prometheus:
    - name: prometheus
      url: ${PROMETHEUS_URL}
      basicAuth:
        username: ${PROMETHEUS_USERNAME}
        password: ${PROMETHEUS_PASSWORD}

proxy:
  '/argocd':
    target: ${ARGOCD_URL}/api/v1
    changeOrigin: true
    secure: false
    headers:
      Cookie:
        $env: ARGOCD_SESSION_TOKEN

  '/prometheus':
    target: ${PROMETHEUS_URL}
    changeOrigin: true

catalog:
  import:
    entityFilename: catalog-info.yaml
    pullRequestBranchName: backstage-integration
  rules:
    - allow: [Component, System, API, Resource, Location, Template]
  locations:
    # æ‰«æGitHubä¸Šçš„æ‰€æœ‰catalog-info.yamlæ–‡ä»¶
    - type: url
      target: https://github.com/myorg/all-services/blob/master/catalog-info.yaml

    # æœ¬åœ°æ–‡ä»¶
    - type: file
      target: ./catalog-info.yaml
      rules:
        - allow: [Template]

techdocs:
  builder: 'local'
  generator:
    runIn: 'local'
  publisher:
    type: 'local'

lighthouse:
  storageUrl: gs://my-org-lighthouse-reports
```

**2. æœåŠ¡ç›®å½•é…ç½®**

```yaml
# backstage/catalog-info.yaml
apiVersion: backstage.io/v1alpha1
kind: Component
metadata:
  name: developer-platform
  description: Internal Developer Platform
  tags:
    - platform
    - developer-experience
    - kubernetes
  annotations:
    github.com/project-slug: myorg/developer-platform
    argocd/app-name: developer-platform
spec:
  type: service
  lifecycle: production
  owner: platform-team
  dependsOn:
    - resource:database
    - resource:cache
    - service:auth-service

  # æä¾›è€…
  providesApis:
    - platform-api

  # æ¶ˆè´¹è€…
  consumesApis:
    - github-api
    - argocd-api
    - prometheus-api
```

**3. æœåŠ¡æ¨¡æ¿æ’ä»¶**

```typescript
// backstage/plugins/service-template/src/plugin.ts
import {
  createPlugin,
  createRoutableExtension,
  discoveryApiRef,
  fetchApiRef,
} from '@backstage/core-plugin-api';
import { scmIntegrationsApiRef } from '@backstage/integration-react';
import { techdocsApiRef } from '@backstage/plugin-techdocs';

export const serviceTemplatePlugin = createPlugin({
  id: 'service-template',
  apis: [],
});

export const ServiceTemplatePage = serviceTemplatePlugin.provide(
  createRoutableExtension({
    name: 'ServiceTemplatePage',
    component: () => import('./components/TemplatePage'),
    mountPoint: rootRouteRef,
  }),
);

// æ¨¡æ¿é…ç½®
export const templates = {
  microservice: {
    title: 'Microservice',
    description: 'Create a new microservice with Kubernetes deployment',
    icon: 'service',
    categories: ['service', 'kubernetes'],
    schema: {
      required: ['name', 'owner'],
      properties: {
        name: {
          type: 'string',
          title: 'Service Name',
          description: 'The name of the service',
        },
        owner: {
          type: 'string',
          title: 'Owner',
          description: 'The team that owns this service',
        },
        language: {
          type: 'string',
          title: 'Programming Language',
          enum: ['python', 'nodejs', 'go', 'java'],
          default: 'python',
        },
        port: {
          type: 'number',
          title: 'Service Port',
          default: 8080,
        },
        replicas: {
          type: 'number',
          title: 'Number of Replicas',
          default: 2,
        },
      },
    },
  },

  serverless: {
    title: 'Serverless Function',
    description: 'Create a new serverless function',
    icon: 'cloud',
    categories: ['serverless', 'function'],
    schema: {
      required: ['name', 'runtime'],
      properties: {
        name: {
          type: 'string',
          title: 'Function Name',
        },
        runtime: {
          type: 'string',
          title: 'Runtime',
          enum: ['python3.11', 'nodejs20', 'go1.21'],
        },
        handler: {
          type: 'string',
          title: 'Handler',
          description: 'Entry point for the function',
        },
        memory: {
          type: 'number',
          title: 'Memory Size (MB)',
          default: 512,
        },
        timeout: {
          type: 'number',
          title: 'Timeout (seconds)',
          default: 30,
        },
      },
    },
  },
};
```

**4. éƒ¨ç½²æ’ä»¶ï¼ˆé›†æˆArgoCDï¼‰**

```typescript
// backstage/plugins/deployment/src/components/DeploymentPage.tsx
import React, { useState, useEffect } from 'react';
import {
  Table,
  TableColumn,
  Progress,
  ResponseErrorPanel,
} from '@backstage/core-components';
import { useApi } from '@backstage/core-plugin-api';

export const DeploymentPage = ({ appName }) => {
  const [app, setApp] = useState(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState();

  const fetchApi = useApi(fetchApiRef);
  const discoveryApi = useApi(discoveryApiRef);

  useEffect(() => {
    const fetchApp = async () => {
      try {
        const baseUrl = await discoveryApi.getBaseUrl('argocd');
        const response = await fetchApi(`${baseUrl}/applications/${appName}`);
        const data = await response.json();
        setApp(data);
      } catch (err) {
        setError(err);
      } finally {
        setLoading(false);
      }
    };

    fetchApp();
  }, [appName]);

  if (loading) return <Progress />;
  if (error) return <ResponseErrorPanel error={error} />

  const handleSync = async () => {
    await fetchApi(`${baseUrl}/applications/${appName}/sync`, {
      method: 'POST',
    });
  };

  const handleRollback = async () => {
    await fetchApi(`${baseUrl}/applications/${appName}/rollback`, {
      method: 'POST',
    });
  };

  return (
    <div>
      <h2>{app?.name}</h2>
      <div>Health Status: {app?.status.health}</div>
      <div>Sync Status: {app?.status.sync}</div>

      <Table
        title="Resources"
        columns={[
          { title: 'Name', field: 'name' },
          { title: 'Kind', field: 'kind' },
          { title: 'Namespace', field: 'namespace' },
          { title: 'Status', field: 'status' },
        ]}
        data={app?.status.resources || []}
      />

      <button onClick={handleSync}>Sync</button>
      <button onClick={handleRollback}>Rollback</button>
    </div>
  );
};
```

**5. AIåŠ©æ‰‹æ’ä»¶**

```python
# backstage/plugins/ai-assistant/src/service.py
from langchain.chat_models import ChatOpenAI
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.tools import Tool

class AIAssistantService:
    """AIåŠ©æ‰‹æœåŠ¡"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4-turbo-preview")
        self.tools = self._create_tools()
        self.agent = self._create_agent()

    def _create_tools(self):
        """åˆ›å»ºå·¥å…·é›†"""
        return [
            Tool(
                name="GetServiceLogs",
                func=self._get_service_logs,
                description="èŽ·å–æœåŠ¡çš„æ—¥å¿—"
            ),
            Tool(
                name="AnalyzeMetrics",
                func=self._analyze_metrics,
                description="åˆ†æžæœåŠ¡æŒ‡æ ‡"
            ),
            Tool(
                name="CheckDeploymentStatus",
                func=self._check_deployment_status,
                description="æ£€æŸ¥éƒ¨ç½²çŠ¶æ€"
            ),
            Tool(
                name="SuggestScaling",
                func=self._suggest_scaling,
                description="å»ºè®®æ‰©å®¹ç­–ç•¥"
            ),
        ]

    def _create_agent(self):
        """åˆ›å»ºAgent"""
        prompt = PromptTemplate.from_template("""
        ä½ æ˜¯ä¸€ä¸ªDevOpsåŠ©æ‰‹ï¼Œå¸®åŠ©å¼€å‘è€…è§£å†³é—®é¢˜ã€‚

        å·¥å…·ï¼š
        {tools}

        ä½¿ç”¨æ ¼å¼ï¼š
        Question: è¾“å…¥é—®é¢˜
        Thought: æ€è€ƒåº”è¯¥ä½¿ç”¨ä»€ä¹ˆå·¥å…·
        Action: å·¥å…·åç§°
        Action Input: å·¥å…·è¾“å…¥
        Observation: å·¥å…·è¾“å‡º
        ... (å¯ä»¥é‡å¤Thought/Action/Observation)
        Thought: æˆ‘çŽ°åœ¨çŸ¥é“æœ€ç»ˆç­”æ¡ˆäº†
        Final Answer: æœ€ç»ˆç­”æ¡ˆ

        å¼€å§‹ï¼

        Question: {input}
        Thought: {agent_scratchpad}
        """)

        return create_openai_functions_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )

    async def ask(self, question: str) -> str:
        """è¯¢é—®AIåŠ©æ‰‹"""
        agent_executor = AgentExecutor(
            agent=self.agent,
            tools=self.tools,
            verbose=True
        )

        result = await agent_executor.ainvoke({"input": question})
        return result["output"]

    async def troubleshoot(self, service_name: str) -> dict:
        """æ•…éšœæŽ’æŸ¥"""
        prompt = f"""
        å¸®æˆ‘æŽ’æŸ¥æœåŠ¡ {service_name} çš„é—®é¢˜ã€‚

        è¯·æ£€æŸ¥ï¼š
        1. æœåŠ¡æ—¥å¿—
        2. æŒ‡æ ‡æ•°æ®
        3. éƒ¨ç½²çŠ¶æ€
        4. ä¾èµ–å…³ç³»

        ç„¶åŽç»™å‡ºï¼š
        - é—®é¢˜è¯Šæ–­
        - å¯èƒ½åŽŸå› 
        - è§£å†³å»ºè®®
        """

        diagnosis = await self.ask(prompt)

        return {
            "service": service_name,
            "diagnosis": diagnosis,
            "recommendations": await self._generate_recommendations(service_name)
        }
```

**6. Infrastructure as Codeï¼ˆTerraformæ¨¡å—ï¼‰**

```hcl
# infrastructure/terraform/modules/service/main.tf
resource "kubernetes_deployment" "service" {
  metadata {
    name      = var.name
    namespace = var.namespace
  }

  spec {
    replicas = var.replicas

    selector {
      match_labels = {
        app = var.name
      }
    }

    template {
      metadata {
        labels = {
          app = var.name
        }
      }

      spec {
        container {
          name  = var.name
          image = var.image

          port {
            container_port = var.port
          }

          resources {
            limits = {
              cpu    = var.cpu_limit
              memory = var.memory_limit
            }
            requests = {
              cpu    = var.cpu_request
              memory = var.memory_request
            }
          }

          env_from {
            config_map_ref {
              name = "${var.name}-config"
            }
          }

          env_from {
            secret_ref {
              name = "${var.name}-secret"
            }
          }

          liveness_probe {
            http_get {
              path = "/health"
              port = var.port
            }
            initial_delay_seconds = 30
            period_seconds        = 10
          }

          readiness_probe {
            http_get {
              path = "/ready"
              port = var.port
            }
            initial_delay_seconds = 5
            period_seconds        = 5
          }
        }
      }
    }
  }
}

resource "kubernetes_service" "service" {
  metadata {
    name      = var.name
    namespace = var.namespace
  }

  spec {
    selector = {
      app = var.name
    }

    port {
      port        = var.port
      target_port = var.port
    }

    type = "ClusterIP"
  }
}

resource "kubernetes_horizontal_pod_autoscaler" "hpa" {
  metadata {
    name      = "${var.name}-hpa"
    namespace = var.namespace
  }

  spec {
    scale_target_ref {
      api_version = "apps/v1"
      kind       = "Deployment"
      name       = var.name
    }

    min_replicas = var.min_replicas
    max_replicas = var.max_replicas

    metric {
      type = "Resource"
      resource {
        name = "cpu"
        target {
          type                = "Utilization"
          average_utilization = 70
        }
      }
    }
  }
}

# ServiceMonitor for Prometheus
resource "kubernetes_manifest" "servicemonitor" {
  manifest = {
    apiVersion = "monitoring.coreos.com/v1"
    kind       = "ServiceMonitor"
    metadata = {
      name      = var.name
      namespace = var.namespace
      labels = {
        app = var.name
      }
    }
    spec = {
      selector = {
        matchLabels = {
          app = var.name
        }
      }
      endpoints = [{
        port = var.port
        path = "/metrics"
      }]
    }
  }
}
```

### Platform Engineeringæœ€ä½³å®žè·µ

**1. Golden Pathï¼ˆé»„é‡‘è·¯å¾„ï¼‰**

```yaml
# templates/golden-path/template.yaml
apiVersion: backstage.io/v1alpha1
kind: Template
metadata:
  name: golden-path
  title: Golden Path Template
  description: Production-ready service template with best practices

spec:
  type: service
  owner: platform-team
  lifecycle: production

  parameters:
    - title: Service Name
      name: serviceName
      type: string

    - title: Owner
      name: owner
      type: string

    - title: Language
      name: language
      type: enum
      options:
        - python
        - nodejs
        - go

  steps:
    - id: scaffold
      name: Scaffold Project
      action: scaffold:cookiecutter
      input:
        url: ./templates/microservice
        values:
          name: ${{ parameters.serviceName }}
          language: ${{ parameters.language }}

    - id: create-repo
      name: Create Repository
      action: publish:github
      input:
        repoUrl: ${{ steps.scaffold.output.repoUrl }}
        description: 'Service ${{ parameters.serviceName }}'
        topics:
          - service
          - microservice

    - id: deploy
      name: Deploy to Staging
      action: argocd:create
      input:
        appName: ${{ parameters.serviceName }}
        repoUrl: ${{ steps.create-repo.output.repoUrl }}
        namespace: staging

    - id: monitor
      name: Setup Monitoring
      action: prometheus:setup
      input:
        service: ${{ parameters.serviceName }}
        namespace: staging
```

**2. Scorecardsï¼ˆè¯„åˆ†å¡ï¼‰**

```yaml
# .github/scorecard.yml
apiVersion: scorecard.dev/v1alpha1
kind: ScorecardConfig

metadata:
  name: My Organization Scorecard

checks:
  - id: deployment-exists
    name: Deployment exists
    description: Check if Kubernetes deployment exists
    type: kubernetes
    resource: deployment
    weight: 10

  - id: service-exists
    name: Service exists
    description: Check if Kubernetes service exists
    type: kubernetes
    resource: service
    weight: 10

  - id: monitoring-enabled
    name: Monitoring enabled
    description: Check if ServiceMonitor exists
    type: kubernetes
    resource: servicemonitor
    weight: 15

  - id: has-docs
    name: Has documentation
    description: Check if README.md exists
    type: file
    pattern: README.md
    weight: 10

  - id: has-tests
    name: Has tests
    description: Check if tests exist
    type: file
    pattern: "**/*test*.py"
    weight: 15

  - id: helm-chart-exists
    name: Helm chart exists
    description: Check if Helm chart exists
    type: file
    pattern: "helm/**/Chart.yaml"
    weight: 10

  - id: security-scan-passed
    name: Security scan passed
    description: Check if Trivy scan passed
    type: security
    scanner: trivy
    weight: 15

  - id: license-compliant
    name: License compliant
    description: Check if license is approved
    type: license
    approved: ["Apache-2.0", "MIT"]
    weight: 5

scorecard:
  passing: 70
```

---

## å®žæˆ˜6ï¼šAIOps - AIé©±åŠ¨çš„è‡ªåŠ¨åŒ–è¿ç»´

### æŠ€æœ¯æ ˆï¼ˆ2024-2026ä¸»æµï¼‰

```
ðŸ¤– OpenAI GPT-4 / Claude 3.5 Sonnet
ðŸ“Š Prometheus + Grafana
ðŸ” Elastic Stackï¼ˆELKï¼‰
ðŸŽ¯ Opsgenieï¼ˆå‘Šè­¦ç®¡ç†ï¼‰
ðŸ”„ Ansibleï¼ˆè‡ªåŠ¨åŒ–ï¼‰
â˜¸ï¸ Kubernetes
ðŸ¦™ LangChain
ðŸ Python 3.11+
```

### é¡¹ç›®ç®€ä»‹

ä¸€ä¸ªAIé©±åŠ¨çš„æ™ºèƒ½è¿ç»´ç³»ç»Ÿï¼Œå®žçŽ°æ•…éšœé¢„æµ‹ã€è‡ªåŠ¨è¯Šæ–­ã€è‡ªæ„ˆèƒ½åŠ›ã€‚

**æ ¸å¿ƒåŠŸèƒ½**ï¼š
```
ðŸ”® å¼‚å¸¸é¢„æµ‹ï¼šåŸºäºŽåŽ†å²æ•°æ®é¢„æµ‹æ½œåœ¨æ•…éšœ
ðŸ¤– è‡ªåŠ¨è¯Šæ–­ï¼šAIåˆ†æžæ•…éšœæ ¹å› 
ðŸ’Š è‡ªåŠ¨è‡ªæ„ˆï¼šè‡ªåŠ¨æ‰§è¡Œä¿®å¤æ“ä½œ
ðŸ“Š æ™ºèƒ½å‘Šè­¦ï¼šå‡å°‘å‘Šè­¦å™ªéŸ³ï¼Œç²¾å‡†å‘Šè­¦
ðŸŽ¯ å®¹é‡è§„åˆ’ï¼šé¢„æµ‹èµ„æºéœ€æ±‚
ðŸ“ˆ è¶‹åŠ¿åˆ†æžï¼šè¯†åˆ«ç³»ç»Ÿè¶‹åŠ¿
ðŸ” æ—¥å¿—åˆ†æžï¼šæ™ºèƒ½æ—¥å¿—åˆ†æž
ðŸš¨ äº‹ä»¶å…³è”ï¼šå…³è”ç›¸å…³äº‹ä»¶
```

### é¡¹ç›®æž¶æž„

```python
# aiops/ai_ops_system.py
from langchain.agents import AgentExecutor, create_openai_functions_agent
from langchain.tools import Tool
from prometheus_api import PrometheusAPI
from elasticsearch_api import ElasticsearchAPI
from kubernetes_api import KubernetesAPI

class AIOpsSystem:
    """AIOpsç³»ç»Ÿ"""

    def __init__(self):
        self.prometheus = PrometheusAPI()
        self.elastic = ElasticsearchAPI()
        self.k8s = KubernetesAPI()

        self.llm = ChatOpenAI(model="gpt-4-turbo-preview")
        self.tools = self._create_tools()
        self.agent = self._create_agent()

    def _create_tools(self):
        """åˆ›å»ºå·¥å…·é›†"""
        return [
            Tool(
                name="GetMetrics",
                func=self._get_metrics,
                description="èŽ·å–PrometheusæŒ‡æ ‡æ•°æ®"
            ),
            Tool(
                name="QueryLogs",
                func=self._query_logs,
                description="æŸ¥è¯¢Elasticsearchæ—¥å¿—"
            ),
            Tool(
                name="GetPodStatus",
                func=self._get_pod_status,
                description="èŽ·å–PodçŠ¶æ€"
            ),
            Tool(
                name="ScaleDeployment",
                func=self._scale_deployment,
                description="æ‰©å®¹Deployment"
            ),
            Tool(
                name="RestartPod",
                func=self._restart_pod,
                description="é‡å¯Pod"
            ),
            Tool(
                name="AnalyzeAnomaly",
                func=self._analyze_anomaly,
                description="åˆ†æžå¼‚å¸¸"
            ),
            Tool(
                name="PredictFailure",
                func=self._predict_failure,
                description="é¢„æµ‹æ•…éšœ"
            ),
        ]

    async def _get_metrics(self, query: str, time_range: str = "1h") -> dict:
        """èŽ·å–æŒ‡æ ‡"""
        result = await self.prometheus.query(query, time_range)
        return result

    async def _query_logs(self, query: str, time_range: str = "1h") -> list:
        """æŸ¥è¯¢æ—¥å¿—"""
        result = await self.elastic.search(query, time_range)
        return result

    async def _get_pod_status(self, namespace: str, pod: str) -> dict:
        """èŽ·å–PodçŠ¶æ€"""
        status = await self.k8s.get_pod_status(namespace, pod)
        return status

    async def _scale_deployment(self, namespace: str, deployment: str, replicas: int):
        """æ‰©å®¹Deployment"""
        await self.k8s.scale_deployment(namespace, deployment, replicas)

    async def _restart_pod(self, namespace: str, pod: str):
        """é‡å¯Pod"""
        await self.k8s.delete_pod(namespace, pod)

    async def _analyze_anomaly(self, service: str) -> dict:
        """åˆ†æžå¼‚å¸¸"""
        # èŽ·å–æŒ‡æ ‡
        cpu_usage = await self._get_metrics(f'rate(container_cpu_usage_seconds_total{{service="{service}"}}[5m])')
        memory_usage = await self._get_metrics(f'container_memory_usage_bytes{{service="{service}"}}')
        error_rate = await self._get_metrics(f'rate(http_requests_total{{service="{service}",status=~"5.."}}[5m])')

        # æŸ¥è¯¢æ—¥å¿—
        logs = await self._query_logs(f'service:"{service}" AND (level:"ERROR" OR level:"WARN")')

        # AIåˆ†æž
        analysis_prompt = f"""
        åˆ†æžä»¥ä¸‹æœåŠ¡çš„å¼‚å¸¸æƒ…å†µï¼š

        CPUä½¿ç”¨çŽ‡ï¼š{cpu_usage}
        å†…å­˜ä½¿ç”¨çŽ‡ï¼š{memory_usage}
        é”™è¯¯çŽ‡ï¼š{error_rate}
        é”™è¯¯æ—¥å¿—ï¼š{logs[:10]}

        è¯·åˆ†æžï¼š
        1. å¼‚å¸¸ç±»åž‹
        2. å¯èƒ½åŽŸå› 
        3. å½±å“èŒƒå›´
        4. å»ºè®®æŽªæ–½
        """

        analysis = await self.llm.ainvoke(analysis_prompt)

        return {
            "service": service,
            "analysis": analysis.content,
            "metrics": {
                "cpu": cpu_usage,
                "memory": memory_usage,
                "error_rate": error_rate,
            },
            "logs": logs[:10],
        }

    async def _predict_failure(self, service: str, hours: int = 24) -> dict:
        """é¢„æµ‹æ•…éšœ"""
        # èŽ·å–åŽ†å²æ•°æ®
        historical_data = await self._get_metrics(
            f'rate(container_cpu_usage_seconds_total{{service="{service}"}}[5m])',
            f"{hours}h"
        )

        # ä½¿ç”¨AIæ¨¡åž‹é¢„æµ‹
        prediction_prompt = f"""
        åŸºäºŽä»¥ä¸‹åŽ†å²æ•°æ®é¢„æµ‹æœªæ¥1å°æ—¶çš„æ•…éšœæ¦‚çŽ‡ï¼š

        åŽ†å²æ•°æ®ï¼š{historical_data}

        è¯·é¢„æµ‹ï¼š
        1. æ•…éšœæ¦‚çŽ‡ï¼ˆ0-100%ï¼‰
        2. é¢„è®¡æ•…éšœæ—¶é—´
        3. å¯èƒ½çš„æ•…éšœç±»åž‹
        4. é¢„é˜²æŽªæ–½
        """

        prediction = await self.llm.ainvoke(prediction_prompt)

        return {
            "service": service,
            "prediction": prediction.content,
            "historical_data": historical_data,
        }

    async def auto_heal(self, service: str, issue: dict) -> dict:
        """è‡ªåŠ¨ä¿®å¤"""
        # åˆ†æžé—®é¢˜
        analysis = await self._analyze_anomaly(service)

        # å†³ç­–ä¿®å¤ç­–ç•¥
        decision_prompt = f"""
        åŸºäºŽä»¥ä¸‹åˆ†æžï¼Œå†³å®šä¿®å¤ç­–ç•¥ï¼š

        é—®é¢˜åˆ†æžï¼š{analysis}

        å¯ç”¨æ“ä½œï¼š
        1. æ‰©å®¹Deployment
        2. é‡å¯Pod
        3. å›žæ»šç‰ˆæœ¬
        4. ä¿®æ”¹é…ç½®
        5. äººå·¥ä»‹å…¥

        è¯·å†³å®šï¼š
        1. æœ€ä½³ä¿®å¤ç­–ç•¥
        2. å…·ä½“æ“ä½œæ­¥éª¤
        3. é¢„æœŸæ•ˆæžœ
        """

        decision = await self.llm.ainvoke(decision_prompt)

        # æ‰§è¡Œä¿®å¤
        if "æ‰©å®¹" in decision.content:
            await self._scale_deployment(
                service.split(":")[0],
                service.split(":")[1],
                replicas=analysis.get("suggested_replicas", 3)
            )
        elif "é‡å¯" in decision.content:
            await self._restart_pod(
                service.split(":")[0],
                service.split(":")[1]
            )

        return {
            "service": service,
            "issue": issue,
            "decision": decision.content,
            "action_taken": "executed",
        }

    async def intelligent_alerting(self, alert: dict) -> bool:
        """æ™ºèƒ½å‘Šè­¦ï¼šå‡å°‘å‘Šè­¦å™ªéŸ³"""
        # èŽ·å–åŽ†å²å‘Šè­¦
        historical_alerts = await self._query_similar_alerts(alert)

        # AIåˆ¤æ–­æ˜¯å¦éœ€è¦å‘Šè­¦
        alerting_prompt = f"""
        åˆ¤æ–­ä»¥ä¸‹å‘Šè­¦æ˜¯å¦éœ€è¦é€šçŸ¥è¿ç»´äººå‘˜ï¼š

        å½“å‰å‘Šè­¦ï¼š{alert}
        åŽ†å²ç›¸ä¼¼å‘Šè­¦ï¼š{historical_alerts}

        åˆ¤æ–­æ ‡å‡†ï¼š
        1. æ˜¯å¦ä¸ºæ–°é—®é¢˜
        2. å½±å“ç¨‹åº¦
        3. æ˜¯å¦å·²è‡ªåŠ¨å¤„ç†
        4. æ˜¯å¦éœ€è¦äººå·¥ä»‹å…¥

        è¿”å›žï¼štrue/false åŠåŽŸå› 
        """

        decision = await self.llm.ainvoke(alerting_prompt)

        return "true" in decision.content.lower()

    async def capacity_planning(self, service: str, days: int = 30) -> dict:
        """å®¹é‡è§„åˆ’"""
        # èŽ·å–åŽ†å²æ•°æ®
        cpu_data = await self._get_metrics(
            f'avg(rate(container_cpu_usage_seconds_total{{service="{service}"}}[5m]))',
            f"{days}d"
        )
        memory_data = await self._get_metrics(
            f'avg(container_memory_usage_bytes{{service="{service}"}})',
            f"{days}d"
        )
        request_data = await self._get_metrics(
            f'rate(http_requests_total{{service="{service}"}}[5m])',
            f"{days}d"
        )

        # AIé¢„æµ‹
        planning_prompt = f"""
        åŸºäºŽä»¥ä¸‹æ•°æ®åˆ¶å®šå®¹é‡è§„åˆ’ï¼š

        CPUä½¿ç”¨è¶‹åŠ¿ï¼š{cpu_data}
        å†…å­˜ä½¿ç”¨è¶‹åŠ¿ï¼š{memory_data}
        è¯·æ±‚é‡è¶‹åŠ¿ï¼š{request_data}

        è¯·é¢„æµ‹æœªæ¥7å¤©å¹¶å»ºè®®ï¼š
        1. æŽ¨èå®žä¾‹æ•°é‡
        2. CPU/å†…å­˜é…ç½®
        3. æ‰©å®¹æ—¶é—´ç‚¹
        4. æˆæœ¬é¢„ä¼°
        """

        plan = await self.llm.ainvoke(planning_prompt)

        return {
            "service": service,
            "plan": plan.content,
            "current_usage": {
                "cpu": cpu_data,
                "memory": memory_data,
                "requests": request_data,
            },
        }
```

### å®žé™…åº”ç”¨åœºæ™¯

**åœºæ™¯1ï¼šè‡ªåŠ¨æ•…éšœè¯Šæ–­å’Œä¿®å¤**

```python
# aiops/scenarios/auto_healing.py
async def auto_healing_scenario():
    """è‡ªåŠ¨æ•…éšœè¯Šæ–­å’Œä¿®å¤åœºæ™¯"""
    aiops = AIOpsSystem()

    # 1. ç›‘æŽ§å‘çŽ°å¼‚å¸¸
    alert = {
        "service": "user-service",
        "namespace": "production",
        "type": "high_error_rate",
        "value": 0.15,  # 15%é”™è¯¯çŽ‡
        "threshold": 0.05  # é˜ˆå€¼5%
    }

    # 2. æ™ºèƒ½å‘Šè­¦åˆ¤æ–­
    should_alert = await aiops.intelligent_alerting(alert)
    if not should_alert:
        print("å‘Šè­¦è¢«è¿‡æ»¤ï¼Œæ— éœ€äººå·¥ä»‹å…¥")
        return

    # 3. è‡ªåŠ¨è¯Šæ–­
    diagnosis = await aiops._analyze_anomaly(alert["service"])
    print(f"è¯Šæ–­ç»“æžœï¼š{diagnosis}")

    # 4. å°è¯•è‡ªåŠ¨ä¿®å¤
    if diagnosis["severity"] == "high":
        healing_result = await aiops.auto_heal(alert["service"], diagnosis)
        print(f"ä¿®å¤ç»“æžœï¼š{healing_result}")

        # 5. éªŒè¯ä¿®å¤æ•ˆæžœ
        await asyncio.sleep(30)  # ç­‰å¾…30ç§’
        verification = await aiops._analyze_anomaly(alert["service"])
        if verification["status"] == "healthy":
            print("è‡ªåŠ¨ä¿®å¤æˆåŠŸï¼")
        else:
            print("è‡ªåŠ¨ä¿®å¤å¤±è´¥ï¼Œéœ€è¦äººå·¥ä»‹å…¥")
            # åˆ›å»ºäººå·¥å·¥å•
            await create_incident(alert, diagnosis, healing_result)
```

**åœºæ™¯2ï¼šé¢„æµ‹æ€§ç»´æŠ¤**

```python
# aiops/scenarios/predictive_maintenance.py
async def predictive_maintenance_scenario():
    """é¢„æµ‹æ€§ç»´æŠ¤åœºæ™¯"""
    aiops = AIOpsSystem()

    services = ["user-service", "product-service", "order-service"]

    for service in services:
        # é¢„æµ‹æ•…éšœ
        prediction = await aiops._predict_failure(service, hours=24)

        if prediction["probability"] > 0.7:
            print(f"âš ï¸ {service} æ•…éšœæ¦‚çŽ‡ï¼š{prediction['probability']}%")
            print(f"é¢„è®¡æ—¶é—´ï¼š{prediction['estimated_time']}")
            print(f"å»ºè®®æŽªæ–½ï¼š{prediction['recommendations']}")

            # æå‰é‡‡å–æŽªæ–½
            await aiops.auto_heal(service, prediction)
```

---

## å­¦ä¹ å»ºè®®

### æŽ¨èå­¦ä¹ é¡ºåº

```
ç¬¬1é˜¶æ®µï¼šåŸºç¡€å·©å›ºï¼ˆå·²å®Œæˆçš„4ä¸ªå®žæˆ˜ï¼‰
â”œâ”€ å®žæˆ˜1ï¼šå¾®æœåŠ¡æž¶æž„å®Œæ•´éƒ¨ç½²
â”œâ”€ å®žæˆ˜2ï¼šå¤šçŽ¯å¢ƒç®¡ç†å®žæˆ˜
â”œâ”€ å®žæˆ˜3ï¼šäº‘å¹³å°å®Œæ•´éƒ¨ç½²
â””â”€ å®žæˆ˜4ï¼šç¾éš¾æ¢å¤ä¸Žé«˜å¯ç”¨

ç¬¬2é˜¶æ®µï¼šPlatform Engineeringï¼ˆ3-4å‘¨ï¼‰
â”œâ”€ æ­å»ºBackstage IDP
â”œâ”€ é…ç½®ArgoCD GitOps
â”œâ”€ åˆ›å»ºæœåŠ¡æ¨¡æ¿
â””â”€ å®žçŽ°è‡ªåŠ©æœåŠ¡å¹³å°

ç¬¬3é˜¶æ®µï¼šAIOpsï¼ˆ2-3å‘¨ï¼‰
â”œâ”€ é›†æˆPrometheusç›‘æŽ§
â”œâ”€ å®žçŽ°AIæ•…éšœè¯Šæ–­
â”œâ”€ å¼€å‘è‡ªæ„ˆèƒ½åŠ›
â””â”€ æ™ºèƒ½å‘Šè­¦å’Œå®¹é‡è§„åˆ’
```

### 2024-2026 DevOpsæŠ€æœ¯è¦ç‚¹

æ ¹æ®[æœ€æ–°DevOpsè¶‹åŠ¿](https://www.n-ix.com/devops-trends/)ï¼š

- âœ… **Platform Engineering**ï¼šä»ŽDevOpsè¿›åŒ–è€Œæ¥
- âœ… **Internal Developer Platforms**ï¼šæå‡å¼€å‘è€…ä½“éªŒ
- âœ… **Golden Paths**ï¼šæ ‡å‡†åŒ–æœ€ä½³å®žè·µ
- âœ… **AIOps**ï¼šAIé©±åŠ¨çš„è‡ªåŠ¨åŒ–è¿ç»´
- âœ… **GitOps**ï¼šåŸºç¡€è®¾æ–½å³ä»£ç çš„æ ‡å‡†å®žè·µ
- âœ… **DevSecOps**ï¼šå®‰å…¨å·¦ç§»
- âœ… **Service Mesh**ï¼šå¾®æœåŠ¡é€šä¿¡ç®¡ç†
- âœ… **Observability**ï¼šå¯è§‚æµ‹æ€§ä¸‰å¤§æ”¯æŸ±ï¼ˆMetricsã€Logsã€Tracesï¼‰

---

## å‚è€ƒèµ„æº

### å®˜æ–¹æ–‡æ¡£
- [Backstage](https://backstage.io/docs)
- [Argo CD](https://argoproj.github.io/argo-cd/)
- [Prometheus](https://prometheus.io/docs/)
- [Grafana](https://grafana.com/docs/)
- [Kubernetes](https://kubernetes.io/docs/)

### æŠ€æœ¯è¶‹åŠ¿æŠ¥å‘Š
- [DevOps Trends 2026](https://devops.com/top-15-devops-trends-to-watch-in-2026/)
- [Platform Engineering 2026](https://medium.com/@orlando1409/beyond-kubernetes-platform-engineering-trends-for-2026-8f82e09e27e0)
- [AIOps Guide](https://www.xmatters.com/blog/the-future-of-ops/)

---

**æœ€åŽæ›´æ–°**ï¼š2025å¹´2æœˆ
**ç‰ˆæœ¬**ï¼šv3.0ï¼ˆ2024-2026æŠ€æœ¯æ ˆï¼‰
**ä½œè€…**ï¼šå°å¾
**é‚®ç®±**ï¼šesimonx@163.com