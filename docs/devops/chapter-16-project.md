# å®æˆ˜é¡¹ç›®1ï¼šKuberneteså¤šé›†ç¾¤ç®¡ç†ç³»ç»Ÿ

> **é¡¹ç›®éš¾åº¦**ï¼šâ­â­â­â­â­
> **é¢„è®¡æ—¶é—´**ï¼š60-80å°æ—¶
> **æŠ€æœ¯æ ˆ**ï¼šKubernetes | Helm | Argo CD | Prometheus | Terraform | Go

## é¡¹ç›®æ¦‚è¿°

æ„å»ºä¸€ä¸ªä¼ä¸šçº§Kuberneteså¤šé›†ç¾¤ç®¡ç†å¹³å°ï¼Œç»Ÿä¸€ç®¡ç†å¼€å‘ã€æµ‹è¯•ã€ç”Ÿäº§ç­‰å¤šä¸ªé›†ç¾¤ï¼Œæä¾›å¯è§†åŒ–çš„é›†ç¾¤ç®¡ç†ã€åº”ç”¨éƒ¨ç½²ã€ç›‘æ§å‘Šè­¦ç­‰èƒ½åŠ›ã€‚

### æ ¸å¿ƒåŠŸèƒ½

```
ğŸ¯ å¤šé›†ç¾¤ç®¡ç†ï¼šç»Ÿä¸€ç®¡ç†å¤šä¸ªK8sé›†ç¾¤
ğŸš€ åº”ç”¨éƒ¨ç½²ï¼šæ”¯æŒHelm Chartå’ŒKustomize
ğŸ“Š ç›‘æ§å‘Šè­¦ï¼šPrometheus + Grafanaç›‘æ§
ğŸ”„ GitOpsï¼šArgo CDæŒç»­éƒ¨ç½²
ğŸ” æƒé™ç®¡ç†ï¼šåŸºäºRBACçš„æƒé™æ§åˆ¶
ğŸ“ˆ èµ„æºå¯è§†åŒ–ï¼šé›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µå¯è§†åŒ–
ğŸ” æ—¥å¿—èšåˆï¼šå¤šé›†ç¾¤æ—¥å¿—ç»Ÿä¸€æŸ¥è¯¢
ğŸš¨ è‡ªåŠ¨ä¼¸ç¼©ï¼šHPA/VPAè‡ªåŠ¨ä¼¸ç¼©ç­–ç•¥
```

### æŠ€æœ¯æ¶æ„

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Multi-Cluster Manager â”‚
                    â”‚      (Frontend + API)   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Control Plane Cluster  â”‚
                    â”‚  (ArgoCD + Prometheus)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Dev Cluster   â”‚     â”‚ Staging Cluster â”‚     â”‚ Prod Cluster    â”‚
â”‚ - 3 nodes     â”‚     â”‚ - 5 nodes       â”‚     â”‚ - 10+ nodes     â”‚
â”‚ - Dev env     â”‚     â”‚ - Staging env   â”‚     â”‚ - Multi-AZ      â”‚
â”‚ - Single AZ   â”‚     â”‚ - Single AZ     â”‚     â”‚ - High Avail    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## é¡¹ç›®æ¶æ„è®¾è®¡

### 1. é¡¹ç›®ç»“æ„

```bash
multi-cluster-manager/
â”œâ”€â”€ backend/                    # Goåç«¯æœåŠ¡
â”‚   â”œâ”€â”€ api/                   # APIå±‚
â”‚   â”œâ”€â”€ services/              # ä¸šåŠ¡é€»è¾‘
â”‚   â”œâ”€â”€ repositories/          # æ•°æ®è®¿é—®
â”‚   â”œâ”€â”€ models/                # æ•°æ®æ¨¡å‹
â”‚   â””â”€â”€ cmd/                   # å‘½ä»¤è¡Œå·¥å…·
â”‚
â”œâ”€â”€ frontend/                   # Vue3å‰ç«¯
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ views/             # é¡µé¢
â”‚   â”‚   â”œâ”€â”€ components/        # ç»„ä»¶
â”‚   â”‚   â””â”€â”€ api/               # APIè°ƒç”¨
â”‚   â””â”€â”€ package.json
â”‚
â”œâ”€â”€ infrastructure/             # åŸºç¡€è®¾æ–½
â”‚   â”œâ”€â”€ terraform/             # Terraformé…ç½®
â”‚   â”œâ”€â”€ helm-charts/           # Helm Charts
â”‚   â””â”€â”€ kubernetes/            # K8s manifests
â”‚
â”œâ”€â”€ monitoring/                 # ç›‘æ§é…ç½®
â”‚   â”œâ”€â”€ prometheus/            # Prometheusè§„åˆ™
â”‚   â”œâ”€â”€ grafana/               # Grafanaä»ªè¡¨ç›˜
â”‚   â””â”€â”€ alertmanager/          # å‘Šè­¦è§„åˆ™
â”‚
â”œâ”€â”€ pipelines/                  # CI/CDæµæ°´çº¿
â”‚   â”œâ”€â”€ github-actions/        # GitHub Actions
â”‚   â””â”€â”€ argocd/                # Argo CDé…ç½®
â”‚
â””â”€â”€ docs/                       # æ–‡æ¡£
    â”œâ”€â”€ architecture.md        # æ¶æ„æ–‡æ¡£
    â”œâ”€â”€ api.md                 # APIæ–‡æ¡£
    â””â”€â”€ deployment.md          # éƒ¨ç½²æ–‡æ¡£
```

### 2. æŠ€æœ¯é€‰å‹

| ç»„ä»¶ | æŠ€æœ¯é€‰å‹ | è¯´æ˜ |
|------|---------|------|
| **åç«¯æ¡†æ¶** | Go 1.21 + Gin | é«˜æ€§èƒ½APIæœåŠ¡ |
| **å‰ç«¯æ¡†æ¶** | Vue 3 + TypeScript | ç°ä»£åŒ–å‰ç«¯ |
| **æ•°æ®åº“** | PostgreSQL + Redis | å…³ç³»å‹æ•°æ®åº“ + ç¼“å­˜ |
| **é›†ç¾¤ç®¡ç†** | Kubernetes | å®¹å™¨ç¼–æ’ |
| **åº”ç”¨éƒ¨ç½²** | Helm + Kustomize | åŒ…ç®¡ç† + é…ç½®ç®¡ç† |
| **GitOps** | Argo CD | æŒç»­éƒ¨ç½² |
| **ç›‘æ§** | Prometheus + Grafana | ç›‘æ§å‘Šè­¦ |
| **æ—¥å¿—** | Loki + Promtail | æ—¥å¿—èšåˆ |
| **æœåŠ¡ç½‘æ ¼** | Istio | æœåŠ¡é—´é€šä¿¡ |
| **åŸºç¡€è®¾æ–½** | Terraform | IaC |

---

## æ ¸å¿ƒåŠŸèƒ½å®ç°

### 1. å¤šé›†ç¾¤ç®¡ç†

**åç«¯ï¼šé›†ç¾¤æ³¨å†Œå’Œç®¡ç†**

```go
// backend/services/cluster_service.go
package services

import (
    "context"
    "fmt"
    "github.com/rs/xid"
    "meta/backend/models"
    "meta/backend/repositories"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/rest"
    "k8s.io/client-go/tools/clientcmd"
)

type ClusterService struct {
    clusterRepo repositories.ClusterRepository
    clients     map[string]*kubernetes.Clientset
}

func NewClusterService(repo repositories.ClusterRepository) *ClusterService {
    return &ClusterService{
        clusterRepo: repo,
        clients:     make(map[string]*kubernetes.Clientset),
    }
}

// RegisterCluster æ³¨å†Œæ–°é›†ç¾¤
func (s *ClusterService) RegisterCluster(ctx context.Context, req models.RegisterClusterRequest) (*models.Cluster, error) {
    // 1. éªŒè¯kubeconfig
    config, err := clientcmd.RESTConfigFromKubeConfig([]byte(req.KubeConfig))
    if err != nil {
        return nil, fmt.Errorf("invalid kubeconfig: %w", err)
    }

    // 2. åˆ›å»ºclientsetæµ‹è¯•è¿æ¥
    clientset, err := kubernetes.NewForConfig(config)
    if err != nil {
        return nil, fmt.Errorf("failed to create clientset: %w", err)
    }

    // 3. è·å–é›†ç¾¤ä¿¡æ¯
    version, err := clientset.Discovery().ServerVersion()
    if err != nil {
        return nil, fmt.Errorf("failed to get server version: %w", err)
    }

    nodes, err := clientset.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
    if err != nil {
        return nil, fmt.Errorf("failed to list nodes: %w", err)
    }

    // 4. ä¿å­˜é›†ç¾¤ä¿¡æ¯
    cluster := &models.Cluster{
        ID:          xid.New().String(),
        Name:        req.Name,
        Environment: req.Environment,
        KubeConfig:  req.KubeConfig,
        Version:     version.GitVersion,
        NodeCount:   len(nodes.Items),
        Status:      "active",
    }

    if err := s.clusterRepo.Create(ctx, cluster); err != nil {
        return nil, fmt.Errorf("failed to save cluster: %w", err)
    }

    // 5. ç¼“å­˜clientset
    s.clients[cluster.ID] = clientset

    return cluster, nil
}

// GetCluster è·å–é›†ç¾¤ä¿¡æ¯
func (s *ClusterService) GetCluster(ctx context.Context, id string) (*models.Cluster, error) {
    return s.clusterRepo.FindByID(ctx, id)
}

// ListClusters åˆ—å‡ºæ‰€æœ‰é›†ç¾¤
func (s *ClusterService) ListClusters(ctx context.Context) ([]*models.Cluster, error) {
    return s.clusterRepo.FindAll(ctx)
}

// GetClusterStats è·å–é›†ç¾¤ç»Ÿè®¡ä¿¡æ¯
func (s *ClusterService) GetClusterStats(ctx context.Context, id string) (*models.ClusterStats, error) {
    clientset, ok := s.clients[id]
    if !ok {
        return nil, fmt.Errorf("cluster not found or not connected")
    }

    // è·å–èŠ‚ç‚¹åˆ—è¡¨
    nodes, err := clientset.CoreV1().Nodes().List(ctx, metav1.ListOptions{})
    if err != nil {
        return nil, err
    }

    // è·å–Podåˆ—è¡¨
    pods, err := clientset.CoreV1().Pods("").List(ctx, metav1.ListOptions{})
    if err != nil {
        return nil, err
    }

    // è·å–å‘½åç©ºé—´åˆ—è¡¨
    namespaces, err := clientset.CoreV1().Namespaces().List(ctx, metav1.ListOptions{})
    if err != nil {
        return nil, err
    }

    stats := &models.ClusterStats{
        NodeCount:       len(nodes.Items),
        PodCount:        len(pods.Items),
        NamespaceCount:  len(namespaces.Items),
        RunningPods:     0,
        PendingPods:     0,
        FailedPods:      0,
    }

    // ç»Ÿè®¡PodçŠ¶æ€
    for _, pod := range pods.Items {
        switch pod.Status.Phase {
        case "Running":
            stats.RunningPods++
        case "Pending":
            stats.PendingPods++
        default:
            stats.FailedPods++
        }
    }

    return stats, nil
}

// GetClusterResources è·å–é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ
func (s *ClusterService) GetClusterResources(ctx context.Context, id string) (*models.ClusterResources, error) {
    clientset, ok := s.clients[id]
    if !ok {
        return nil, fmt.Errorf("cluster not found or not connected")
    }

    // è·å–Metrics API
    metricsAPI := clientset.MetricsV1beta1()

    // è·å–èŠ‚ç‚¹æŒ‡æ ‡
    nodeMetrics, err := metricsAPI.NodeMetricses().List(ctx, metav1.ListOptions{})
    if err != nil {
        return nil, err
    }

    // è·å–PodæŒ‡æ ‡
    podMetrics, err := metricsAPI.PodMetricses("").List(ctx, metav1.ListOptions{})
    if err != nil {
        return nil, err
    }

    resources := &models.ClusterResources{
        Nodes: make([]models.NodeResource, 0, len(nodeMetrics.Items)),
        Pods:  make([]models.PodResource, 0, len(podMetrics.Items)),
    }

    // æ±‡æ€»èŠ‚ç‚¹èµ„æº
    for _, nodeMetric := range nodeMetrics.Items {
        node := models.NodeResource{
            Name:       nodeMetric.Name,
            CPU:        nodeMetric.Usage.Cpu().MilliValue(),
            Memory:     nodeMetric.Usage.Memory().Value() / (1024 * 1024), // MB
        }
        resources.Nodes = append(resources.Nodes, node)
    }

    // æ±‡æ€»Podèµ„æº
    for _, podMetric := range podMetrics.Items {
        pod := models.PodResource{
            Namespace: podMetric.Namespace,
            Name:      podMetric.Name,
            CPU:       podMetric.Usage[0].MilliValue(),
            Memory:    podMetric.Usage[1].Value() / (1024 * 1024), // MB
        }
        resources.Pods = append(resources.Pods, pod)
    }

    return resources, nil
}

// DeleteCluster åˆ é™¤é›†ç¾¤
func (s *ClusterService) DeleteCluster(ctx context.Context, id string) error {
    // ä»ç¼“å­˜ä¸­åˆ é™¤clientset
    delete(s.clients, id)

    // ä»æ•°æ®åº“åˆ é™¤
    return s.clusterRepo.Delete(ctx, id)
}
```

**APIå±‚ï¼šé›†ç¾¤ç®¡ç†æ¥å£**

```go
// backend/api/cluster_api.go
package api

import (
    "net/http"
    "github.com/gin-gonic/gin"
    "meta/backend/services"
)

type ClusterAPI struct {
    service *services.ClusterService
}

func NewClusterAPI(service *services.ClusterService) *ClusterAPI {
    return &ClusterAPI{service: service}
}

// RegisterCluster æ³¨å†Œé›†ç¾¤
// @Summary æ³¨å†Œé›†ç¾¤
// @Tags cluster
// @Accept json
// @Produce json
// @Param request body models.RegisterClusterRequest true "é›†ç¾¤ä¿¡æ¯"
// @Success 200 {object} models.Cluster
// @Router /api/v1/clusters [post]
func (api *ClusterAPI) RegisterCluster(c *gin.Context) {
    var req models.RegisterClusterRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
        return
    }

    cluster, err := api.service.RegisterCluster(c.Request.Context(), req)
    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }

    c.JSON(http.StatusOK, cluster)
}

// GetCluster è·å–é›†ç¾¤è¯¦æƒ…
func (api *ClusterAPI) GetCluster(c *gin.Context) {
    id := c.Param("id")

    cluster, err := api.service.GetCluster(c.Request.Context(), id)
    if err != nil {
        c.JSON(http.StatusNotFound, gin.H{"error": err.Error()})
        return
    }

    c.JSON(http.StatusOK, cluster)
}

// ListClusters åˆ—å‡ºæ‰€æœ‰é›†ç¾¤
func (api *ClusterAPI) ListClusters(c *gin.Context) {
    clusters, err := api.service.ListClusters(c.Request.Context())
    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }

    c.JSON(http.StatusOK, gin.H{
        "clusters": clusters,
        "total":    len(clusters),
    })
}

// GetClusterStats è·å–é›†ç¾¤ç»Ÿè®¡ä¿¡æ¯
func (api *ClusterAPI) GetClusterStats(c *gin.Context) {
    id := c.Param("id")

    stats, err := api.service.GetClusterStats(c.Request.Context(), id)
    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }

    c.JSON(http.StatusOK, stats)
}

// GetClusterResources è·å–é›†ç¾¤èµ„æºä½¿ç”¨æƒ…å†µ
func (api *ClusterAPI) GetClusterResources(c *gin.Context) {
    id := c.Param("id")

    resources, err := api.service.GetClusterResources(c.Request.Context(), id)
    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }

    c.JSON(http.StatusOK, resources)
}

// DeleteCluster åˆ é™¤é›†ç¾¤
func (api *ClusterAPI) DeleteCluster(c *gin.Context) {
    id := c.Param("id")

    if err := api.service.DeleteCluster(c.Request.Context(), id); err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }

    c.JSON(http.StatusOK, gin.H{"message": "Cluster deleted successfully"})
}
```

### 2. åº”ç”¨éƒ¨ç½²ç®¡ç†

**Helm Chartéƒ¨ç½²**

```go
// backend/services/helm_service.go
package services

import (
    "context"
    "fmt"
    "helm.sh/helm/v3/pkg/action"
    "helm.sh/helm/v3/pkg/chart"
    "helm.sh/helm/v3/pkg/cli"
    "helm.sh/helm/v3/pkg/release"
    "helm.sh/helm/v3/pkg/repo"
    "meta/backend/models"
)

type HelmService struct {
    clusters map[string]*ActionConfig
}

func NewHelmService() *HelmService {
    return &HelmService{
        clusters: make(map[string]*action.Configuration),
    }
}

// InstallChart å®‰è£…Helm Chart
func (s *HelmService) InstallChart(ctx context.Context, req *models.InstallChartRequest) (*release.Release, error) {
    // è·å–é›†ç¾¤é…ç½®
    actionConfig, ok := s.clusters[req.ClusterID]
    if !ok {
        return nil, fmt.Errorf("cluster not found")
    }

    // åˆ›å»ºå®‰è£…action
    install := action.NewInstall(actionConfig)
    install.ReleaseName = req.ReleaseName
    install.Namespace = req.Namespace
    install.CreateNamespace = true

    // åŠ è½½Chart
    chart, err := loader.Load(req.ChartPath)
    if err != nil {
        return nil, fmt.Errorf("failed to load chart: %w", err)
    }

    // è§£ævalues
    values := make(map[string]interface{})
    if req.Values != "" {
        if err := yaml.Unmarshal([]byte(req.Values), &values); err != nil {
            return nil, fmt.Errorf("failed to parse values: %w", err)
        }
    }

    // æ‰§è¡Œå®‰è£…
    rel, err := install.RunWithContext(ctx, chart, values)
    if err != nil {
        return nil, fmt.Errorf("failed to install: %w", err)
    }

    return rel, nil
}

// UpgradeChart å‡çº§Helm Chart
func (s *HelmService) UpgradeChart(ctx context.Context, req *models.UpgradeChartRequest) (*release.Release, error) {
    actionConfig, ok := s.clusters[req.ClusterID]
    if !ok {
        return nil, fmt.Errorf("cluster not found")
    }

    upgrade := action.NewUpgrade(actionConfig)
    upgrade.Namespace = req.Namespace

    chart, err := loader.Load(req.ChartPath)
    if err != nil {
        return nil, fmt.Errorf("failed to load chart: %w", err)
    }

    values := make(map[string]interface{})
    if req.Values != "" {
        if err := yaml.Unmarshal([]byte(req.Values), &values); err != nil {
            return nil, fmt.Errorf("failed to parse values: %w", err)
        }
    }

    rel, err := upgrade.RunWithContext(ctx, req.ReleaseName, chart, values)
    if err != nil {
        return nil, fmt.Errorf("failed to upgrade: %w", err)
    }

    return rel, nil
}

// UninstallRelease å¸è½½Release
func (s *HelmService) UninstallRelease(ctx context.Context, clusterID, namespace, releaseName string) error {
    actionConfig, ok := s.clusters[clusterID]
    if !ok {
        return fmt.Errorf("cluster not found")
    }

    uninstall := action.NewUninstall(actionConfig)
    if _, err := uninstall.RunWithContext(ctx, releaseName); err != nil {
        return fmt.Errorf("failed to uninstall: %w", err)
    }

    return nil
}

// ListReleases åˆ—å‡ºæ‰€æœ‰Release
func (s *HelmService) ListReleases(ctx context.Context, clusterID string) ([]*release.Release, error) {
    actionConfig, ok := s.clusters[clusterID]
    if !ok {
        return nil, fmt.Errorf("cluster not found")
    }

    list := action.NewList(actionConfig)
    list.All = true
    list.AllNamespaces = true

    releases, err := list.Run()
    if err != nil {
        return nil, fmt.Errorf("failed to list releases: %w", err)
    }

    return releases, nil
}

// GetReleaseStatus è·å–ReleaseçŠ¶æ€
func (s *HelmService) GetReleaseStatus(ctx context.Context, clusterID, namespace, releaseName string) (*models.ReleaseStatus, error) {
    actionConfig, ok := s.clusters[clusterID]
    if !ok {
        return nil, fmt.Errorf("cluster not found")
    }

    status := action.NewStatus(actionConfig)
    rel, err := status.Run(releaseName)
    if err != nil {
        return nil, fmt.Errorf("failed to get status: %w", err)
    }

    return &models.ReleaseStatus{
        Name:       rel.Name,
        Namespace:  rel.Namespace,
        Version:    rel.Version,
        Status:     rel.Info.Status.String(),
        Chart:      rel.Chart.Metadata.Name,
        AppVersion: rel.Chart.Metadata.AppVersion,
        Updated:    rel.Info.LastDeployed,
    }, nil
}

// RollbackRelease å›æ»šRelease
func (s *HelmService) RollbackRelease(ctx context.Context, clusterID, namespace, releaseName string, revision int) error {
    actionConfig, ok := s.clusters[clusterID]
    if !ok {
        return fmt.Errorf("cluster not found")
    }

    rollback := action.NewRollback(actionConfig)
    rollback.Version = revision

    if err := rollback.RunWithContext(ctx, releaseName); err != nil {
        return fmt.Errorf("failed to rollback: %w", err)
    }

    return nil
}
```

### 3. ç›‘æ§å‘Šè­¦

**Prometheusé…ç½®**

```yaml
# monitoring/prometheus/prometheus-values.yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

rule_files:
    - "/etc/prometheus/rules/*.yml"

scrape_configs:
  # Kubernetesç»„ä»¶
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - source_labels: [__address__]
        regex: '(.*):10250'
        replacement: '${1}:9100'
        target_label: __address__

  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__

  # åº”ç”¨æŒ‡æ ‡
  - job_name: 'multi-cluster-manager'
    static_configs:
      - targets: ['multi-cluster-manager-backend:8080']
    metrics_path: '/metrics'
```

**å‘Šè­¦è§„åˆ™**

```yaml
# monitoring/prometheus/rules/alerts.yml
groups:
  - name: cluster_alerts
    interval: 30s
    rules:
      # é›†ç¾¤èŠ‚ç‚¹å‘Šè­¦
      - alert: NodeNotReady
        expr: kube_node_status_condition{condition="Ready",status!="true"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Node {{ $labels.node }} is not ready"
          description: "Node {{ $labels.node }} has been not ready for more than 5 minutes"

      - alert: NodeMemoryPressure
        expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Node {{ $labels.node }} has memory pressure"

      - alert: NodeDiskPressure
        expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Node {{ $labels.node }} has disk pressure"

      # Podå‘Šè­¦
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"

      - alert: PodNotReady
        expr: kube_pod_status_ready{condition="false"} == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is not ready"

      # èµ„æºä½¿ç”¨å‘Šè­¦
      - alert: HighCPUUsage
        expr: sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (node) > 0.8
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage on node {{ $labels.node }}"
          description: "CPU usage is above 80% for more than 10 minutes"

      - alert: HighMemoryUsage
        expr: sum(container_memory_working_set_bytes{container!=""}) by (node) / sum(kube_node_status_capacity{resource="memory"}) by (node) > 0.9
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on node {{ $labels.node }}"
          description: "Memory usage is above 90% for more than 10 minutes"

      # Deploymentå‘Šè­¦
      - alert: DeploymentReplicasMismatch
        expr: kube_deployment_spec_replicas != kube_deployment_status_replicas_available
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"

  - name: application_alerts
    interval: 30s
    rules:
      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.05
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate for {{ $labels.service }}"
          description: "Error rate is above 5% for more than 5 minutes"

      - alert: HighLatency
        expr: histogram_quantile(0.95, http_request_duration_seconds_bucket) > 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High latency for {{ $labels.service }}"
          description: "P95 latency is above 1s for more than 5 minutes"
```

**Grafanaä»ªè¡¨ç›˜**

```json
{
  "dashboard": {
    "title": "Multi-Cluster Overview",
    "panels": [
      {
        "title": "Cluster Overview",
        "type": "stat",
        "targets": [
          {
            "expr": "count(kube_node_info)"
          }
        ]
      },
      {
        "title": "Pod Status by Cluster",
        "type": "piechart",
        "targets": [
          {
            "expr": "sum by (cluster, status) (kube_pod_status_phase)"
          }
        ]
      },
      {
        "title": "CPU Usage by Cluster",
        "type": "graph",
        "targets": [
          {
            "expr": "sum by (cluster) (rate(container_cpu_usage_seconds_total{container!=\"\"}[5m]))"
          }
        ]
      },
      {
        "title": "Memory Usage by Cluster",
        "type": "graph",
        "targets": [
          {
            "expr": "sum by (cluster) (container_memory_working_set_bytes{container!=\"\"})"
          }
        ]
      },
      {
        "title": "Network Traffic by Cluster",
        "type": "graph",
        "targets": [
          {
            "expr": "sum by (cluster) (rate(container_network_receive_bytes_total[5m]))"
          },
          {
            "expr": "sum by (cluster) (rate(container_network_transmit_bytes_total[5m]))"
          }
        ]
      }
    ]
  }
}
```

### 4. GitOpséƒ¨ç½²ï¼ˆArgo CDï¼‰

**Applicationé…ç½®**

```yaml
# infrastructure/argocd/applications/multi-cluster-manager-app.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: multi-cluster-manager
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default

  source:
    repoURL: https://github.com/yourorg/multi-cluster-manager.git
    targetRevision: main
    path: infrastructure/kubernetes/apps/multi-cluster-manager
    helm:
      valueFiles:
        - values-dev.yaml

  destination:
    server: https://kubernetes.default.svc
    namespace: multi-cluster-manager

  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    syncOptions:
      - CreateNamespace=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
```

**å¤šé›†ç¾¤éƒ¨ç½²**

```yaml
# infrastructure/argocd/applications/dev-cluster.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: app-dev
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/yourorg/app.git
    targetRevision: main
    path: helm/app
  destination:
    server: https://dev-cluster.example.com
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

---
# infrastructure/argocd/applications/staging-cluster.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: app-staging
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/yourorg/app.git
    targetRevision: main
    path: helm/app
  destination:
    server: https://staging-cluster.example.com
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true

---
# infrastructure/argocd/applications/prod-cluster.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: app-prod
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/yourorg/app.git
    targetRevision: main
    path: helm/app
  destination:
    server: https://prod-cluster.example.com
    namespace: production
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
```

### 5. å‰ç«¯ç•Œé¢

**é›†ç¾¤åˆ—è¡¨é¡µé¢**

```vue
<!-- frontend/src/views/ClusterList.vue -->
<template>
  <div class="cluster-list">
    <div class="header">
      <h1>Kubernetes é›†ç¾¤ç®¡ç†</h1>
      <el-button type="primary" @click="showAddDialog = true">
        æ³¨å†Œé›†ç¾¤
      </el-button>
    </div>

    <el-table :data="clusters" v-loading="loading">
      <el-table-column prop="name" label="é›†ç¾¤åç§°" />
      <el-table-column prop="environment" label="ç¯å¢ƒ">
        <template #default="{ row }">
          <el-tag :type="getEnvironmentType(row.environment)">
            {{ row.environment }}
          </el-tag>
        </template>
      </el-table-column>
      <el-table-column prop="version" label="ç‰ˆæœ¬" />
      <el-table-column prop="nodeCount" label="èŠ‚ç‚¹æ•°" />
      <el-table-column prop="status" label="çŠ¶æ€">
        <template #default="{ row }">
          <el-tag :type="row.status === 'active' ? 'success' : 'danger'">
            {{ row.status }}
          </el-tag>
        </template>
      </el-table-column>
      <el-table-column label="æ“ä½œ" width="300">
        <template #default="{ row }">
          <el-button size="small" @click="viewStats(row)">ç»Ÿè®¡</el-button>
          <el-button size="small" @click="viewResources(row)">èµ„æº</el-button>
          <el-button size="small" type="danger" @click="deleteCluster(row)">åˆ é™¤</el-button>
        </template>
      </el-table-column>
    </el-table>

    <!-- æ·»åŠ é›†ç¾¤å¯¹è¯æ¡† -->
    <el-dialog v-model="showAddDialog" title="æ³¨å†Œé›†ç¾¤" width="600px">
      <el-form :model="clusterForm" label-width="120px">
        <el-form-item label="é›†ç¾¤åç§°">
          <el-input v-model="clusterForm.name" />
        </el-form-item>
        <el-form-item label="ç¯å¢ƒ">
          <el-select v-model="clusterForm.environment">
            <el-option label="å¼€å‘" value="dev" />
            <el-option label="æµ‹è¯•" value="staging" />
            <el-option label="ç”Ÿäº§" value="prod" />
          </el-select>
        </el-form-item>
        <el-form-item label="KubeConfig">
          <el-input
            v-model="clusterForm.kubeConfig"
            type="textarea"
            :rows="10"
            placeholder="ç²˜è´´kubeconfigå†…å®¹"
          />
        </el-form-item>
      </el-form>
      <template #footer>
        <el-button @click="showAddDialog = false">å–æ¶ˆ</el-button>
        <el-button type="primary" @click="registerCluster">ç¡®å®š</el-button>
      </template>
    </el-dialog>
  </div>
</template>

<script setup lang="ts">
import { ref, onMounted } from 'vue'
import { ElMessage } from 'element-plus'
import { clusterApi } from '@/api/cluster'

const clusters = ref([])
const loading = ref(false)
const showAddDialog = ref(false)

const clusterForm = ref({
  name: '',
  environment: 'dev',
  kubeConfig: ''
})

const fetchClusters = async () => {
  loading.value = true
  try {
    const { data } = await clusterApi.listClusters()
    clusters.value = data.clusters
  } catch (error) {
    ElMessage.error('è·å–é›†ç¾¤åˆ—è¡¨å¤±è´¥')
  } finally {
    loading.value = false
  }
}

const registerCluster = async () => {
  try {
    await clusterApi.registerCluster(clusterForm.value)
    ElMessage.success('é›†ç¾¤æ³¨å†ŒæˆåŠŸ')
    showAddDialog.value = false
    fetchClusters()
  } catch (error) {
    ElMessage.error('é›†ç¾¤æ³¨å†Œå¤±è´¥')
  }
}

const deleteCluster = async (cluster: any) => {
  try {
    await clusterApi.deleteCluster(cluster.id)
    ElMessage.success('é›†ç¾¤åˆ é™¤æˆåŠŸ')
    fetchClusters()
  } catch (error) {
    ElMessage.error('é›†ç¾¤åˆ é™¤å¤±è´¥')
  }
}

const viewStats = (cluster: any) => {
  // è·³è½¬åˆ°ç»Ÿè®¡é¡µé¢
}

const viewResources = (cluster: any) => {
  // è·³è½¬åˆ°èµ„æºé¡µé¢
}

const getEnvironmentType = (env: string) => {
  const map: Record<string, string> = {
    dev: '',
    staging: 'warning',
    prod: 'danger'
  }
  return map[env] || ''
}

onMounted(() => {
  fetchClusters()
})
</script>
```

---

## éƒ¨ç½²æŒ‡å—

### 1. ä½¿ç”¨Terraformåˆ›å»ºé›†ç¾¤

```hcl
# infrastructure/terraform/main.tf
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.0"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "~> 2.0"
    }
  }
}

provider "aws" {
  region = var.region
}

# EKSé›†ç¾¤
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "19.17.2"

  cluster_name    = "${var.project_name}-${var.environment}"
  cluster_version = "1.27"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  eks_managed_node_groups = {
    general = {
      desired_size = var.node_count
      min_size     = 2
      max_size     = 10

      instance_types = ["t3.medium"]
    }
  }
}
```

### 2. ä½¿ç”¨Helméƒ¨ç½²åº”ç”¨

```bash
# æ·»åŠ Helmä»“åº“
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo add argo https://argoproj.github.io/argo-helm
helm repo update

# å®‰è£…Prometheus
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --create-namespace \
  --values monitoring/prometheus/prometheus-values.yaml

# å®‰è£…Argo CD
helm install argocd argo/argo-cd \
  --namespace argocd \
  --create-namespace \
  --set server.service.type=LoadBalancer

# éƒ¨ç½²åº”ç”¨
helm install multi-cluster-manager ./helm-charts/multi-cluster-manager \
  --namespace multi-cluster-manager \
  --create-namespace \
  --set image.tag=latest
```

### 3. é…ç½®GitOps

```bash
# åˆ›å»ºsecret
kubectl create secret generic git-creds \
  --from-literal=username=YOUR_USERNAME \
  --from-literal=password=YOUR_PASSWORD \
  -n argocd

# åº”ç”¨Argo CDé…ç½®
kubectl apply -f infrastructure/argocd/applications/
```

---

## å­¦ä¹ æˆæœ

å®Œæˆæœ¬é¡¹ç›®åï¼Œä½ å°†æŒæ¡ï¼š

âœ… **Kubernetesé›†ç¾¤ç®¡ç†**
- å¤šé›†ç¾¤ç»Ÿä¸€ç®¡ç†
- é›†ç¾¤èµ„æºç›‘æ§
- è·¨é›†ç¾¤åº”ç”¨éƒ¨ç½²

âœ… **äº‘åŸç”ŸæŠ€æœ¯æ ˆ**
- Helm Chartå¼€å‘
- Kustomizeé…ç½®ç®¡ç†
- Argo CD GitOpså®è·µ

âœ… **å¯è§‚æµ‹æ€§**
- Prometheusç›‘æ§é…ç½®
- Grafanaä»ªè¡¨ç›˜è®¾è®¡
- å‘Šè­¦è§„åˆ™ç¼–å†™

âœ… **åŸºç¡€è®¾æ–½å³ä»£ç **
- Terraformç¼–å†™
- å¤šç¯å¢ƒç®¡ç†
- æŒç»­éƒ¨ç½²æµç¨‹

âœ… **ä¼ä¸šçº§å®è·µ**
- RBACæƒé™æ§åˆ¶
- é«˜å¯ç”¨æ¶æ„
- ç¾éš¾æ¢å¤

---

## æ‰©å±•ç»ƒä¹ 

- [ ] å®ç°åº”ç”¨å•†åº—åŠŸèƒ½
- [ ] é›†æˆIstioæœåŠ¡ç½‘æ ¼
- [ ] å®ç°è‡ªåŠ¨æ‰©ç¼©å®¹ç­–ç•¥
- [ ] æ·»åŠ æˆæœ¬ç®¡ç†åŠŸèƒ½
- [ ] å®ç°å¤šç§Ÿæˆ·éš”ç¦»

---

**é¡¹ç›®éš¾åº¦**ï¼šâ­â­â­â­â­
**é¢„è®¡æ—¶é—´**ï¼š60-80å°æ—¶
**é€‚åˆäººç¾¤**ï¼šæœ‰KubernetesåŸºç¡€ï¼Œæƒ³æ·±å…¥å­¦ä¹ ä¼ä¸šçº§å¤šé›†ç¾¤ç®¡ç†
