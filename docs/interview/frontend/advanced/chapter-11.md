---
title: AI + å‰ç«¯ç»“åˆé¢è¯•é¢˜
---

# AI + å‰ç«¯ç»“åˆé¢è¯•é¢˜

## å‰ç«¯é›†æˆAIèƒ½åŠ›

### WebLLMä½¿ç”¨ï¼Ÿ

```javascript
// ä½¿ç”¨WebLLMåœ¨æµè§ˆå™¨ä¸­è¿è¡Œå¤§è¯­è¨€æ¨¡å‹
import * as webllm from '@mlc-ai/web-llm';

class ChatEngine {
  constructor() {
    this.engine = null;
    this.messages = [];
  }

  async init() {
    const selectedModel = 'Llama-3-8B-Instruct-q4f16_1-MLC';

    this.engine = await webllm.CreateMLCEngine(
      selectedModel,
      {
        initProgressCallback: (report) => {
          console.log('Init progress:', report.progress);
        }
      }
    );
  }

  async sendMessage(userMessage) {
    this.messages.push({
      role: 'user',
      content: userMessage
    });

    const reply = await this.engine.chat.completions.create({
      messages: this.messages
    });

    const assistantMessage = reply.choices[0].message.content;
    this.messages.push({
      role: 'assistant',
      content: assistantMessage
    });

    return assistantMessage;
  }

  async generateText(prompt) {
    const reply = await this.engine.chat.completions.create({
      messages: [
        { role: 'user', content: prompt }
      ],
      temperature: 0.7,
      max_tokens: 500
    });

    return reply.choices[0].message.content;
  }
}

// ä½¿ç”¨
const chat = new ChatEngine();
await chat.init();

const response = await chat.sendMessage('Hello, how are you?');
console.log(response);
```

### Transformers.jsï¼Ÿ

```javascript
// ä½¿ç”¨Transformers.jsåœ¨æµè§ˆå™¨ä¸­è¿è¡ŒTransformeræ¨¡å‹
import { pipeline, env } from '@xenova/transformers';

// ç¦ç”¨æœ¬åœ°æ¨¡å‹æ£€æŸ¥
env.allowLocalModels = false;
env.useBrowserCache = true;

class AIHelper {
  constructor() {
    this.classifier = null;
    this.summarizer = null;
    this.translator = null;
  }

  // æƒ…æ„Ÿåˆ†æ
  async initClassifier() {
    this.classifier = await pipeline(
      'sentiment-analysis',
      'Xenova/distilbert-base-uncased-finetuned-sst-2-english'
    );
  }

  async analyzeSentiment(text) {
    const result = await this.classifier(text);
    return result;
  }

  // æ–‡æœ¬æ‘˜è¦
  async initSummarizer() {
    this.summarizer = await pipeline(
      'summarization',
      'Xenova/distilbart-cnn-6-6'
    );
  }

  async summarize(text) {
    const summary = await this.summarizer(text);
    return summary[0].summary_text;
  }

  // ç¿»è¯‘
  async initTranslator() {
    this.translator = await pipeline(
      'translation',
      'Xenova/opus-mt-zh-en'
    );
  }

  async translate(text) {
    const result = await this.translator(text);
    return result[0].translation_text;
  }

  // æ–‡æœ¬ç”Ÿæˆ
  async initGenerator() {
    this.generator = await pipeline(
      'text-generation',
      'Xenova/gpt2'
    );
  }

  async generate(prompt, maxLength = 100) {
    const result = await this.generator(prompt, {
      max_length: maxLength,
      temperature: 0.7
    });
    return result[0].generated_text;
  }
}

// Vueç»„ä»¶ä¸­ä½¿ç”¨
<script setup>
import { ref, onMounted } from 'vue';

const aiHelper = new AIHelper();
const sentiment = ref(null);
const summary = ref(null);

onMounted(async () => {
  await aiHelper.initClassifier();
  await aiHelper.initSummarizer();
});

async function analyzeSentiment(text) {
  sentiment.value = await aiHelper.analyzeSentiment(text);
}

async function summarizeText(text) {
  summary.value = await aiHelper.summarize(text);
}
</script>

<template>
  <div>
    <textarea v-model="inputText" />
    <button @click="analyzeSentiment(inputText)">
      Analyze Sentiment
    </button>
    <p v-if="sentiment">
      {{ sentiment[0].label }}: {{ sentiment[0].score }}
    </p>
  </div>
</template>
```

## AI Copilotå®ç°

### ä»£ç è¡¥å…¨é›†æˆï¼Ÿ

```javascript
// åŸºäºAIçš„ä»£ç è¡¥å…¨
class CodeAssistant {
  constructor(apiKey) {
    this.apiKey = apiKey;
    this.endpoint = 'https://api.openai.com/v1/completions';
  }

  async completeCode(prompt, language = 'javascript') {
    const response = await fetch(this.endpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo-instruct',
        prompt: this.buildPrompt(prompt, language),
        temperature: 0.3,
        max_tokens: 200,
        stop: ['\n\n', '/*', '*/']
      })
    });

    const data = await response.json();
    return data.choices[0].text.trim();
  }

  buildPrompt(code, language) {
    return `Complete the following ${language} code:\n${code}`;
  }

  // å®æ—¶ä»£ç å»ºè®®
  async getSuggestions(code, cursorPosition) {
    const codeBeforeCursor = code.substring(0, cursorPosition);

    const suggestions = await this.completeCode(codeBeforeCursor);

    return {
      text: suggestions,
      position: cursorPosition
    };
  }
}

// Monaco Editoré›†æˆ
import * as monaco from 'monaco-editor';

class MonacoAIAssistant {
  constructor(editor, apiKey) {
    this.editor = editor;
    this.assistant = new CodeAssistant(apiKey);
    this.debounceTimer = null;
  }

  init() {
    this.editor.onDidChangeModelContent(() => {
      this.handleContentChange();
    });
  }

  handleContentChange() {
    clearTimeout(this.debounceTimer);

    this.debounceTimer = setTimeout(async () => {
      const position = this.editor.getPosition();
      const model = this.editor.getModel();
      const code = model.getValue();

      const suggestions = await this.assistant.getSuggestions(
        code,
        model.getOffsetAt(position)
      );

      this.showSuggestions(suggestions);
    }, 500);
  }

  showSuggestions(suggestions) {
    // æ˜¾ç¤ºå»ºè®®
    monaco.editor.showSuggestionsWidget({
      position: this.editor.getPosition(),
      suggestions: [
        {
          label: 'AI Suggestion',
          kind: monaco.languages.CompletionItemKind.Snippet,
          insertText: suggestions.text,
          detail: 'AI-powered suggestion'
        }
      ]
    });
  }
}
```

### æ™ºèƒ½è¡¨å•å¡«å……ï¼Ÿ

```javascript
// AIé©±åŠ¨çš„è¡¨å•è‡ªåŠ¨å¡«å……
class SmartFormFiller {
  constructor(apiKey) {
    this.apiKey = apiKey;
  }

  async extractFormData(text) {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [
          {
            role: 'system',
            content: 'Extract form data from the text and return as JSON'
          },
          {
            role: 'user',
            content: text
          }
        ],
        functions: [
          {
            name: 'extractFormData',
            description: 'Extract form fields from text',
            parameters: {
              type: 'object',
              properties: {
                name: { type: 'string' },
                email: { type: 'string' },
                phone: { type: 'string' },
                address: { type: 'string' }
              }
            }
          }
        ],
        function_call: { name: 'extractFormData' }
      })
    });

    const data = await response.json();
    const functionCall = data.choices[0].message.function_call;

    return JSON.parse(functionCall.arguments);
  }

  // Vueç»„ä»¶
  async fillFormFromText(text) {
    const formData = await this.extractFormData(text);

    // è‡ªåŠ¨å¡«å……è¡¨å•
    return {
      name: formData.name || '',
      email: formData.email || '',
      phone: formData.phone || '',
      address: formData.address || ''
    };
  }
}

// ä½¿ç”¨
<script setup>
import { ref } from 'vue';
import { SmartFormFiller } from '@/utils/smartForm';

const filler = new SmartFormFiller(import.meta.env.VITE_OPENAI_API_KEY);

const formData = ref({
  name: '',
  email: '',
  phone: '',
  address: ''
});

const inputText = ref('');

async function autoFill() {
  const extracted = await filler.fillFormFromText(inputText.value);
  formData.value = extracted;
}
</script>

<template>
  <div>
    <textarea
      v-model="inputText"
      placeholder="ç²˜è´´æ–‡æœ¬ä¿¡æ¯ï¼Œè‡ªåŠ¨æå–è¡¨å•æ•°æ®..."
    />
    <button @click="autoFill">è‡ªåŠ¨å¡«å……</button>

    <form>
      <input v-model="formData.name" placeholder="å§“å" />
      <input v-model="formData.email" placeholder="é‚®ç®±" />
      <input v-model="formData.phone" placeholder="ç”µè¯" />
      <input v-model="formData.address" placeholder="åœ°å€" />
    </form>
  </div>
</template>
```

## AIè¾…åŠ©UIç”Ÿæˆ

### æ–‡æœ¬ç”ŸæˆUIï¼Ÿ

```javascript
// AIé©±åŠ¨çš„UIç”Ÿæˆ
class UIGenerator {
  constructor(apiKey) {
    this.apiKey = apiKey;
  }

  async generateUI(description) {
    const prompt = `Generate Vue component code for: ${description}

Requirements:
- Use Vue 3 Composition API
- Use Tailwind CSS for styling
- Include proper props and emits
- Add TypeScript types
- Make it responsive`;

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [
          {
            role: 'system',
            content: 'You are a Vue.js expert. Generate clean, production-ready Vue components.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.7,
        max_tokens: 1500
      })
    });

    const data = await response.json();
    return data.choices[0].message.content;
  }

  // ç”Ÿæˆç»„ä»¶é…ç½®
  async generateComponentConfig(description) {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [
          {
            role: 'user',
            content: `Generate JSON configuration for a component: ${description}`
          }
        ],
        response_format: { type: 'json_object' }
      })
    });

    const data = await response.json();
    return JSON.parse(data.choices[0].message.content);
  }
}

// ä½¿ç”¨
<script setup>
import { ref } from 'vue';
import { UIGenerator } from '@/utils/uiGenerator';

const generator = new UIGenerator(import.meta.env.VITE_OPENAI_API_KEY);

const description = ref('');
const generatedCode = ref('');

async function generate() {
  generatedCode.value = await generator.generateUI(description.value);
}
</script>

<template>
  <div>
    <textarea
      v-model="description"
      placeholder="æè¿°ä½ éœ€è¦çš„ç»„ä»¶..."
    />
    <button @click="generate">ç”Ÿæˆç»„ä»¶</button>

    <pre v-if="generatedCode">{{ generatedCode }}</pre>
  </div>
</template>
```

### å›¾åƒç”ŸæˆUIï¼Ÿ

```javascript
// ä½¿ç”¨AIç”Ÿæˆå›¾åƒå¹¶è½¬æ¢ä¸ºUI
class ImageToUI {
  constructor(apiKey) {
    this.apiKey = apiKey;
  }

  async generateImage(description) {
    const response = await fetch('https://api.openai.com/v1/images/generations', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: 'dall-e-3',
        prompt: `UI design for ${description}, modern, clean, professional`,
        n: 1,
        size: '1024x1024'
      })
    });

    const data = await response.json();
    return data.data[0].url;
  }

  // ä½¿ç”¨å›¾åƒè¯†åˆ«æå–UIå…ƒç´ 
  async analyzeUI(imageUrl) {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: 'gpt-4-vision-preview',
        messages: [
          {
            role: 'user',
            content: [
              {
                type: 'text',
                text: 'Analyze this UI design and generate HTML/CSS code'
              },
              {
                type: 'image_url',
                image_url: {
                  url: imageUrl
                }
              }
            ]
          }
        ],
        max_tokens: 1000
      })
    });

    const data = await response.json();
    return data.choices[0].message.content;
  }
}
```

## å®æ—¶è¯­éŸ³è¯†åˆ«

### Web Speech APIï¼Ÿ

```javascript
// æµè§ˆå™¨åŸç”Ÿè¯­éŸ³è¯†åˆ«
class VoiceAssistant {
  constructor() {
    this.recognition = null;
    this.synthesis = window.speechSynthesis;
    this.isListening = false;
  }

  init() {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    if (!SpeechRecognition) {
      console.error('Speech recognition not supported');
      return;
    }

    this.recognition = new SpeechRecognition();
    this.recognition.continuous = true;
    this.recognition.interimResults = true;
    this.recognition.lang = 'zh-CN';

    this.recognition.onresult = (event) => {
      const transcript = Array.from(event.results)
        .map(result => result[0].transcript)
        .join('');

      this.onResult(transcript);
    };

    this.recognition.onerror = (event) => {
      console.error('Speech recognition error:', event.error);
      this.onError(event.error);
    };

    this.recognition.onend = () => {
      if (this.isListening) {
        this.recognition.start();
      }
    };
  }

  start() {
    if (this.recognition && !this.isListening) {
      this.isListening = true;
      this.recognition.start();
    }
  }

  stop() {
    if (this.recognition && this.isListening) {
      this.isListening = false;
      this.recognition.stop();
    }
  }

  speak(text) {
    const utterance = new SpeechSynthesisUtterance(text);
    utterance.lang = 'zh-CN';
    utterance.rate = 1;
    utterance.pitch = 1;
    this.synthesis.speak(utterance);
  }

  onResult(transcript) {
    console.log('Speech result:', transcript);
  }

  onError(error) {
    console.error('Speech error:', error);
  }
}

// Vueç»„ä»¶
<script setup>
import { ref, onMounted, onUnmounted } from 'vue';
import { VoiceAssistant } from '@/utils/voice';

const assistant = new VoiceAssistant();
const transcript = ref('');
const isListening = ref(false);

onMounted(() => {
  assistant.init();
  assistant.onResult = (text) => {
    transcript.value = text;
  };
});

onUnmounted(() => {
  assistant.stop();
});

function toggleListening() {
  if (isListening.value) {
    assistant.stop();
    isListening.value = false;
  } else {
    assistant.start();
    isListening.value = true;
  }
}

function speak(text) {
  assistant.speak(text);
}
</script>

<template>
  <div>
    <button @click="toggleListening">
      {{ isListening ? 'åœæ­¢' : 'å¼€å§‹' }}è¯†åˆ«
    </button>

    <p>{{ transcript }}</p>

    <button @click="speak(transcript)">
      æœ—è¯»
    </button>
  </div>
</template>
```

### AIè¯­éŸ³åŠ©æ‰‹ï¼Ÿ

```javascript
// ç»“åˆAIçš„æ™ºèƒ½è¯­éŸ³åŠ©æ‰‹
class AIVoiceAssistant extends VoiceAssistant {
  constructor(apiKey) {
    super();
    this.apiKey = apiKey;
  }

  async processCommand(text) {
    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: [
          {
            role: 'system',
            content: 'You are a helpful voice assistant. Provide concise, friendly responses.'
          },
          {
            role: 'user',
            content: text
          }
        ],
        max_tokens: 200
      })
    });

    const data = await response.json();
    return data.choices[0].message.content;
  }

  async onResult(transcript) {
    if (transcript.endsWith('ã€‚') || transcript.endsWith('.')) {
      const response = await this.processCommand(transcript);

      // æ˜¾ç¤ºå“åº”
      this.onResponse(response);

      // æœ—è¯»å“åº”
      this.speak(response);
    }
  }

  onResponse(response) {
    console.log('AI response:', response);
  }
}

// ä½¿ç”¨
<script setup>
import { ref, onMounted } from 'vue';
import { AIVoiceAssistant } from '@/utils/aiVoice';

const aiAssistant = new AIVoiceAssistant(import.meta.env.VITE_OPENAI_API_KEY);

const userInput = ref('');
const aiResponse = ref('');

onMounted(() => {
  aiAssistant.init();
  aiAssistant.onResponse = (response) => {
    aiResponse.value = response;
  };
});

function startAssistant() {
  aiAssistant.start();
}
</script>

<template>
  <div>
    <button @click="startAssistant">å¯åŠ¨è¯­éŸ³åŠ©æ‰‹</button>

    <div v-if="aiResponse">
      <p>AI: {{ aiResponse }}</p>
    </div>
  </div>
</template>
```

## AI APIé›†æˆä¸ä¼˜åŒ–

### AI APIå®‰å…¨ä¸é™æµï¼Ÿï¼ˆé˜¿é‡Œé«˜é¢‘ï¼‰

```javascript
// AI APIè°ƒç”¨çš„å®‰å…¨ä¸é™æµç­–ç•¥
class AIService {
  constructor(config) {
    this.apiKey = config.apiKey
    this.endpoint = config.endpoint
    this.rateLimiter = new RateLimiter(config.maxRequests, config.perMilliseconds)
    this.retryConfig = {
      maxRetries: 3,
      baseDelay: 1000,
      maxDelay: 10000
    }
  }

  // å¸¦é‡è¯•çš„APIè°ƒç”¨
  async callAPI(prompt, options = {}) {
    await this.rateLimiter.acquire()

    let lastError
    for (let attempt = 0; attempt < this.retryConfig.maxRetries; attempt++) {
      try {
        const response = await fetch(this.endpoint, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${this.apiKey}`
          },
          body: JSON.stringify({
            model: options.model || 'gpt-3.5-turbo',
            messages: [{ role: 'user', content: prompt }],
            temperature: options.temperature || 0.7,
            max_tokens: options.maxTokens || 1000
          })
        })

        if (!response.ok) {
          const error = new Error(`API Error: ${response.status}`)
          error.status = response.status
          throw error
        }

        const data = await response.json()
        return data.choices[0].message.content

      } catch (error) {
        lastError = error

        // 429 Too Many Requests æˆ– 5xx é”™è¯¯æ‰é‡è¯•
        if (error.status === 429 || (error.status >= 500 && attempt < this.retryConfig.maxRetries - 1)) {
          const delay = Math.min(
            this.retryConfig.baseDelay * Math.pow(2, attempt),
            this.retryConfig.maxDelay
          )
          await this.sleep(delay)
          continue
        }

        throw error
      }
    }

    throw lastError
  }

  // æŒ‡æ•°é€€é¿
  sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms))
  }

  // Tokenè®¡æ•°ä¸é¢„ç®—æ§åˆ¶
  estimateTokens(text) {
    // ç²—ç•¥ä¼°ç®—ï¼š1 token â‰ˆ 4 characters
    return Math.ceil(text.length / 4)
  }

  async callWithBudget(prompt, maxTokens = 100000) {
    const estimatedTokens = this.estimateTokens(prompt)

    if (estimatedTokens > maxTokens) {
      throw new Error(`Prompt too long: ${estimatedTokens} tokens`)
    }

    return this.callAPI(prompt)
  }
}

// é™æµå™¨å®ç°
class RateLimiter {
  constructor(maxRequests, perMilliseconds) {
    this.maxRequests = maxRequests
    this.perMilliseconds = perMilliseconds
    this.requests = []
  }

  async acquire() {
    const now = Date.now()

    // æ¸…ç†è¿‡æœŸè¯·æ±‚
    this.requests = this.requests.filter(
      time => now - time < this.perMilliseconds
    )

    // æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
    if (this.requests.length >= this.maxRequests) {
      const oldestRequest = this.requests[0]
      const waitTime = this.perMilliseconds - (now - oldestRequest)

      if (waitTime > 0) {
        await new Promise(resolve => setTimeout(resolve, waitTime))
      }
    }

    this.requests.push(Date.now())
  }
}
```

### Promptå·¥ç¨‹åœ¨å‰ç«¯çš„åº”ç”¨ï¼Ÿï¼ˆå­—èŠ‚å¿…é—®ï¼‰

```javascript
// Promptå·¥ç¨‹æœ€ä½³å®è·µ
class PromptEngineer {
  constructor() {
    this.templates = new Map()
  }

  // 1. æ¨¡æ¿åŒ–Prompt
  defineTemplate(name, template) {
    this.templates.set(name, template)
  }

  renderTemplate(name, variables) {
    const template = this.templates.get(name)
    if (!template) {
      throw new Error(`Template not found: ${name}`)
    }

    return template.replace(/\{\{(\w+)\}\}/g, (_, key) => {
      return variables[key] || `{{${key}}}`
    })
  }

  // 2. Few-shot Learning
  buildFewShotPrompt(examples, newInput) {
    const prompt = examples.map(ex => {
      return `è¾“å…¥ï¼š${ex.input}\nè¾“å‡ºï¼š${ex.output}\n`
    }).join('\n')

    return `${prompt}è¾“å…¥ï¼š${newInput}\nè¾“å‡ºï¼š`
  }

  // 3. Chain of Thought
  buildCoTPrompt(problem) {
    return `
è¯·ä¸€æ­¥æ­¥æ€è€ƒä»¥ä¸‹é—®é¢˜ï¼Œæœ€åç»™å‡ºç­”æ¡ˆï¼š

é—®é¢˜ï¼š${problem}

æ€è€ƒè¿‡ç¨‹ï¼š
1. é¦–å…ˆï¼Œè®©æˆ‘ç†è§£é—®é¢˜...
2. ç„¶åï¼Œæˆ‘éœ€è¦...
3. æ¥ä¸‹æ¥...
4. æœ€åï¼Œç­”æ¡ˆæ˜¯...

è¯·æŒ‰ç…§ä¸Šè¿°æ ¼å¼è¾“å‡ºä½ çš„æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆã€‚
    `.trim()
  }

  // 4. Prompt Chaining
  async chainPrompts(chainData) {
    let currentContext = {}

    for (const step of chainData) {
      const prompt = this.renderTemplate(step.template, {
        ...step.variables,
        ...currentContext
      })

      const result = await aiService.callAPI(prompt)
      currentContext[step.outputKey] = result
    }

    return currentContext
  }
}
```

### AIæµå¼è¾“å‡ºå¤„ç†ï¼Ÿï¼ˆè…¾è®¯é«˜é¢‘ï¼‰

```javascript
// SSEï¼ˆServer-Sent Eventsï¼‰æµå¼è¾“å‡ºå¤„ç†
class StreamingAIChat {
  constructor(apiKey) {
    this.apiKey = apiKey
    this.controller = null
  }

  // æµå¼ç”Ÿæˆæ–‡æœ¬
  async *streamChat(messages, onChunk) {
    this.controller = new AbortController()

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${this.apiKey}`
      },
      body: JSON.stringify({
        model: 'gpt-3.5-turbo',
        messages: messages,
        stream: true
      }),
      signal: this.controller.signal
    })

    const reader = response.body.getReader()
    const decoder = new TextDecoder()
    let buffer = ''

    while (true) {
      const { done, value } = await reader.read()

      if (done) break

      buffer += decoder.decode(value, { stream: true })
      const lines = buffer.split('\n')
      buffer = lines.pop() || ''

      for (const line of lines) {
        const trimmed = line.trim()
        if (trimmed === 'data: [DONE]') return

        if (trimmed.startsWith('data: ')) {
          try {
            const data = JSON.parse(trimmed.slice(6))
            const content = data.choices[0]?.delta?.content

            if (content) {
              yield content
              onChunk?.(content)
            }
          } catch (e) {
            console.error('Error parsing SSE data:', e)
          }
        }
      }
    }
  }

  // å–æ¶ˆæµå¼è¯·æ±‚
  abort() {
    this.controller?.abort()
  }
}
```

### AIä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥ï¼Ÿï¼ˆé˜¿é‡ŒçœŸé¢˜ï¼‰

```javascript
// AIå¯¹è¯çš„ä¸Šä¸‹æ–‡ç®¡ç†
class ContextManager {
  constructor(options = {}) {
    this.maxTokens = options.maxTokens || 4000
    this.contextWindow = options.contextWindow || 10
    this.systemPrompt = options.systemPrompt || ''
    this.conversations = new Map()
  }

  // å¼€å§‹æ–°å¯¹è¯
  startConversation(conversationId) {
    this.conversations.set(conversationId, {
      messages: [],
      summary: null,
      tokenCount: 0
    })

    if (this.systemPrompt) {
      this.addMessage(conversationId, {
        role: 'system',
        content: this.systemPrompt
      })
    }
  }

  // æ·»åŠ æ¶ˆæ¯
  addMessage(conversationId, message) {
    const conversation = this.conversations.get(conversationId)
    if (!conversation) {
      throw new Error(`Conversation not found: ${conversationId}`)
    }

    const tokens = this.estimateTokens(message.content)
    conversation.tokenCount += tokens

    conversation.messages.push({
      ...message,
      timestamp: Date.now(),
      tokens
    })

    // æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©ä¸Šä¸‹æ–‡
    this.maybeCompressContext(conversationId)
  }

  // ä¼°ç®—Tokenæ•°é‡
  estimateTokens(text) {
    return Math.ceil(text.length / 4)
  }

  // å‹ç¼©ä¸Šä¸‹æ–‡
  async maybeCompressContext(conversationId) {
    const conversation = this.conversations.get(conversationId)

    // å¦‚æœTokenæ•°é‡è¶…é™ï¼Œè¿›è¡Œå‹ç¼©
    if (conversation.tokenCount > this.maxTokens) {
      await this.summarizeConversation(conversationId)
    }

    // ä¿ç•™æœ€è¿‘Nè½®å¯¹è¯
    while (conversation.messages.length > this.contextWindow * 2 + 1) {
      const removed = conversation.messages.shift()
      conversation.tokenCount -= removed.tokens
    }
  }

  // æ€»ç»“å¯¹è¯å†å²
  async summarizeConversation(conversationId) {
    const conversation = this.conversations.get(conversationId)

    const messagesToSummarize = conversation.messages
      .filter(m => m.role !== 'system')
      .slice(0, -this.contextWindow * 2)

    if (messagesToSummarize.length === 0) return

    const summaryPrompt = `è¯·æ€»ç»“ä»¥ä¸‹å¯¹è¯å†å²ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š
${messagesToSummarize.map(m => `${m.role}: ${m.content}`).join('\n')}`

    const summary = await aiService.callAPI(summaryPrompt)

    conversation.summary = summary
    conversation.messages = conversation.messages.slice(-this.contextWindow * 2)

    conversation.tokenCount = conversation.messages.reduce(
      (sum, m) => sum + m.tokens,
      this.estimateTokens(summary)
    )
  }

  // è·å–ä¸Šä¸‹æ–‡æ¶ˆæ¯
  getContext(conversationId) {
    const conversation = this.conversations.get(conversationId)
    if (!conversation) return []

    const messages = []

    if (conversation.summary) {
      messages.push({
        role: 'system',
        content: `ä¹‹å‰çš„å¯¹è¯æ€»ç»“ï¼š${conversation.summary}`
      })
    }

    messages.push(...conversation.messages)

    return messages
  }

  // æ¸…é™¤å¯¹è¯
  clearConversation(conversationId) {
    this.conversations.delete(conversationId)
  }
}
```

### AIèŠå¤©ç•Œé¢æœ€ä½³å®è·µï¼Ÿï¼ˆå­—èŠ‚å¿…é—®ï¼‰

```vue
<!-- AICHat.vue - ä¸“ä¸šçš„AIèŠå¤©ç•Œé¢ -->
<template>
  <div class="ai-chat-container">
    <!-- æ¶ˆæ¯åˆ—è¡¨ -->
    <div ref="messagesContainer" class="messages-container">
      <div
        v-for="(message, index) in displayMessages"
        :key="index"
        :class="['message', message.role, { streaming: message.isStreaming }]"
      >
        <!-- å¤´åƒ -->
        <div class="avatar">
          <img v-if="message.role === 'user'" :src="userAvatar" />
          <span v-else class="ai-icon">ğŸ¤–</span>
        </div>

        <!-- æ¶ˆæ¯å†…å®¹ -->
        <div class="message-content">
          <!-- Markdownæ¸²æŸ“ -->
          <div v-html="renderMarkdown(message.content)"></div>

          <!-- æµå¼è¾“å‡ºæ—¶çš„å…‰æ ‡ -->
          <span v-if="message.isStreaming" class="typing-cursor"></span>

          <!-- æ¶ˆæ¯æ“ä½œ -->
          <div class="message-actions">
            <button @click="copyMessage(message.content)" title="å¤åˆ¶">ğŸ“‹</button>
            <button v-if="message.role === 'assistant'" @click="regenerateMessage(index)" title="é‡æ–°ç”Ÿæˆ">ğŸ”„</button>
          </div>
        </div>

        <!-- æ—¶é—´æˆ³ -->
        <div class="message-time">{{ formatTime(message.timestamp) }}</div>
      </div>

      <!-- åŠ è½½ä¸­æŒ‡ç¤ºå™¨ -->
      <div v-if="isLoading" class="message assistant">
        <div class="avatar"><span class="ai-icon">ğŸ¤–</span></div>
        <div class="message-content">
          <div class="typing-indicator">
            <span></span><span></span><span></span>
          </div>
        </div>
      </div>
    </div>

    <!-- è¾“å…¥åŒºåŸŸ -->
    <div class="input-container">
      <textarea
        v-model="userInput"
        @keydown="handleKeyDown"
        placeholder="è¾“å…¥æ¶ˆæ¯... (Shift+Enteræ¢è¡Œï¼ŒEnterå‘é€)"
        rows="1"
        :disabled="isGenerating"
      ></textarea>

      <button @click="sendMessage" :disabled="!canSend" :class="{ generating: isGenerating }">
        {{ isGenerating ? 'â¸ï¸' : 'å‘é€' }}
      </button>
    </div>
  </div>
</template>

<script setup>
import { ref, computed, nextTick } from 'vue'
import { marked } from 'marked'

const messages = ref([])
const userInput = ref('')
const isGenerating = ref(false)
const isLoading = ref(false)

const displayMessages = computed(() => {
  return messages.value.map(m => ({
    ...m,
    isStreaming: m.role === 'assistant' && isGenerating.value
  }))
})

const canSend = computed(() => {
  return userInput.value.trim().length > 0 && !isGenerating.value
})

function renderMarkdown(content) {
  return marked(content)
}

function formatTime(timestamp) {
  return new Date(timestamp).toLocaleTimeString()
}

function handleKeyDown(e) {
  if (e.key === 'Enter' && !e.shiftKey) {
    e.preventDefault()
    sendMessage()
  }
}
</script>

<style scoped>
.message {
  display: flex;
  margin-bottom: 20px;
}

.message.user {
  justify-content: flex-end;
}

.message.assistant {
  justify-content: flex-start;
}

.message-content {
  max-width: 70%;
  padding: 12px 16px;
  border-radius: 12px;
}

.message.user .message-content {
  background: #007AFF;
  color: white;
}

.message.assistant .message-content {
  background: white;
  color: #333;
}

.typing-cursor::after {
  content: 'â–‹';
  animation: blink 1s infinite;
}

@keyframes blink {
  0%, 100% { opacity: 1; }
  50% { opacity: 0; }
}
</style>
```

### AIåº”ç”¨æ€§èƒ½ä¼˜åŒ–ï¼Ÿï¼ˆé˜¿é‡Œ2025çœŸé¢˜ï¼‰

```javascript
// AIåº”ç”¨çš„æ€§èƒ½ä¼˜åŒ–ç­–ç•¥
class OptimizedAIService {
  constructor(config) {
    this.cache = new Map()
    this.queue = new PromiseQueue(config.maxConcurrent || 3)
  }

  // 1. è¯·æ±‚ç¼“å­˜
  async cachedCall(key, fetcher, ttl = 60000) {
    const cached = this.cache.get(key)

    if (cached && Date.now() - cached.timestamp < ttl) {
      return cached.data
    }

    const data = await fetcher()

    this.cache.set(key, {
      data,
      timestamp: Date.now()
    })

    return data
  }

  // 2. è¯·æ±‚é˜Ÿåˆ—ï¼ˆæ§åˆ¶å¹¶å‘ï¼‰
  async queuedRequest(fn) {
    return this.queue.add(fn)
  }

  // 3. æ‰¹é‡è¯·æ±‚åˆå¹¶
  async batchCall(requests) {
    const batchPrompt = requests.map((req, idx) => `[${idx}] ${req.prompt}`).join('\n\n')
    const response = await this.callAPI(batchPrompt)
    return this.parseBatchResponse(response, requests.length)
  }
}

// Promiseé˜Ÿåˆ—å®ç°
class PromiseQueue {
  constructor(maxConcurrent) {
    this.maxConcurrent = maxConcurrent
    this.running = 0
    this.queue = []
  }

  async add(fn) {
    while (this.running >= this.maxConcurrent) {
      await new Promise(resolve => this.queue.push(resolve))
    }

    this.running++

    try {
      return await fn()
    } finally {
      this.running--
      const next = this.queue.shift()
      if (next) next()
    }
  }
}
```

---

**å°å¾å¸¦ä½ é£ç³»åˆ—æ•™ç¨‹**

**æœ€åæ›´æ–°ï¼š2026 å¹´ 2 æœˆ**
**ç‰ˆæœ¬ï¼šv1.0**
**ä½œè€…ï¼šå°å¾**
**é‚®ç®±ï¼šesimonx@163.com**
