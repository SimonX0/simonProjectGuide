---
title: AIæ¨¡å‹åŸºç¡€é¢è¯•é¢˜
---

# AIæ¨¡å‹åŸºç¡€é¢è¯•é¢˜

## æ¨¡å‹æ¶æ„

### Transformerçš„æ ¸å¿ƒåŸç†æ˜¯ä»€ä¹ˆï¼Ÿ

Transformeræ˜¯åŸºäºè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„æ¶æ„ï¼Œä¸»è¦åŒ…å«ï¼š

**æ ¸å¿ƒç»„ä»¶**ï¼š

1. **è‡ªæ³¨æ„åŠ›æœºåˆ¶**ï¼šè®¡ç®—åºåˆ—ä¸­æ¯ä¸ªä½ç½®ä¸å…¶ä»–æ‰€æœ‰ä½ç½®çš„ç›¸å…³æ€§
2. **å¤šå¤´æ³¨æ„åŠ›**ï¼šå¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›è¡¨ç¤º
3. **ä½ç½®ç¼–ç **ï¼šæ³¨å…¥åºåˆ—ä½ç½®ä¿¡æ¯
4. **å‰é¦ˆç½‘ç»œ**ï¼šéçº¿æ€§å˜æ¢

```python
# è‡ªæ³¨æ„åŠ›è®¡ç®—
import torch
import torch.nn.functional as F

def scaled_dot_product_attention(query, key, value, mask=None):
    """ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
    d_k = query.size(-1)

    # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)

    # åº”ç”¨maskï¼ˆå¯é€‰ï¼‰
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)

    # Softmaxå½’ä¸€åŒ–
    attention_weights = F.softmax(scores, dim=-1)

    # åŠ æƒæ±‚å’Œ
    output = torch.matmul(attention_weights, value)

    return output, attention_weights
```

### ä»€ä¹ˆæ˜¯GPTå’ŒBERTçš„åŒºåˆ«ï¼Ÿ

| ç‰¹æ€§ | GPT | BERT |
|------|-----|------|
| æ¶æ„ | Decoder-only | Encoder-only |
| æ³¨æ„åŠ› | å› æœæ³¨æ„åŠ›ï¼ˆå•å‘ï¼‰ | åŒå‘æ³¨æ„åŠ› |
| è®­ç»ƒæ–¹å¼ | è‡ªå›å½’ï¼ˆé¢„æµ‹ä¸‹ä¸€ä¸ªtokenï¼‰ | æ©ç è¯­è¨€æ¨¡å‹ |
| é€‚ç”¨ä»»åŠ¡ | æ–‡æœ¬ç”Ÿæˆ | æ–‡æœ¬ç†è§£ |
| ä»£è¡¨æ¨¡å‹ | GPT-3, GPT-4 | BERT, RoBERTa |

```python
# GPTé£æ ¼ - å› æœæ³¨æ„åŠ›
def causal_attention_mask(seq_len):
    """åˆ›å»ºå› æœmaskï¼Œåªèƒ½çœ‹åˆ°ä¹‹å‰çš„ä¿¡æ¯"""
    mask = torch.tril(torch.ones(seq_len, seq_len))
    return mask

# BERTé£æ ¼ - åŒå‘æ³¨æ„åŠ›
def bidirectional_attention(seq_len):
    """å¯ä»¥çœ‹åˆ°æ‰€æœ‰ä½ç½®çš„ä¿¡æ¯"""
    return torch.ones(seq_len, seq_len)
```

### ä»€ä¹ˆæ˜¯ä½ç½®ç¼–ç ï¼Ÿ

ä½ç½®ç¼–ç è®©æ¨¡å‹çŸ¥é“tokenåœ¨åºåˆ—ä¸­çš„ä½ç½®ã€‚

```python
import math
import torch

def positional_encoding(max_seq_len, d_model):
    """æ­£å¼¦ä½ç½®ç¼–ç """
    position = torch.arange(max_seq_len).unsqueeze(1)
    div_term = torch.exp(
        torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)
    )

    pe = torch.zeros(max_seq_len, d_model)
    pe[:, 0::2] = torch.sin(position * div_term)
    pe[:, 1::2] = torch.cos(position * div_term)

    return pe
```

**ä¸ºä»€ä¹ˆç”¨æ­£å¼¦å‡½æ•°**ï¼š
- èƒ½å¤Ÿå¤„ç†è®­ç»ƒæ—¶æœªè§è¿‡çš„åºåˆ—é•¿åº¦
- ä¸åŒé¢‘ç‡ç¼–ç ä¸åŒå°ºåº¦ä½ç½®å…³ç³»
- ç›¸å¯¹ä½ç½®ä¿¡æ¯å¯å­¦ä¹ 

## æ¨¡å‹é€‰æ‹©

### å¦‚ä½•é€‰æ‹©åˆé€‚çš„LLMï¼Ÿ

**é€‰æ‹©æ ‡å‡†**ï¼š

1. **ä»»åŠ¡ç±»å‹**ï¼š
   - æ–‡æœ¬ç”Ÿæˆï¼šGPTç³»åˆ—ã€LLaMA
   - æ–‡æœ¬ç†è§£ï¼šBERTã€RoBERTa
   - å¤šæ¨¡æ€ï¼šGPT-4Vã€CLIP

2. **èµ„æºé™åˆ¶**ï¼š
   - äº‘ç«¯å¤§æ¨¡å‹ï¼šGPT-4ã€Claude
   - æœ¬åœ°éƒ¨ç½²ï¼šLLaMAã€Mistral
   - è¾¹ç¼˜è®¾å¤‡ï¼šDistilBERTã€TinyLlama

3. **æ€§èƒ½è¦æ±‚**ï¼š
   - é«˜è´¨é‡ï¼šGPT-4ã€Claude 3
   - å¹³è¡¡ï¼šGPT-3.5ã€LLaMA 2 70B
   - é€Ÿåº¦ä¼˜å…ˆï¼š7Bå‚æ•°æ¨¡å‹

4. **æˆæœ¬è€ƒè™‘**ï¼š
   - å…è´¹å¼€æºï¼šLLaMAã€Mistral
   - APIè°ƒç”¨ï¼šæŒ‰tokenè®¡è´¹
   - è‡ªæ‰˜ç®¡ï¼šç¡¬ä»¶æˆæœ¬

### ä¸»è¦å¼€æºæ¨¡å‹æœ‰å“ªäº›ï¼Ÿ

**7Bçº§åˆ«**ï¼š
```python
# è½»é‡çº§ï¼Œé€‚åˆæœ¬åœ°éƒ¨ç½²
models = {
    "LLaMA 2 7B": {
        "ä¼˜ç‚¹": ["æ€§èƒ½ä¼˜ç§€", "ç¤¾åŒºæ´»è·ƒ", "æ˜“äºéƒ¨ç½²"],
        "åœºæ™¯": "èŠå¤©ã€æ–‡æœ¬ç”Ÿæˆ"
    },
    "Mistral 7B": {
        "ä¼˜ç‚¹": ["æ€§èƒ½è¶…è¶ŠLLaMA 2 7B", "æ»‘åŠ¨çª—å£æ³¨æ„åŠ›"],
        "åœºæ™¯": "é€šç”¨ä»»åŠ¡"
    },
    "Qwen 7B": {
        "ä¼˜ç‚¹": ["ä¸­æ–‡ä¼˜ç§€", "æ•°å­¦èƒ½åŠ›å¼º"],
        "åœºæ™¯": "ä¸­æ–‡åº”ç”¨"
    }
}
```

**70Bçº§åˆ«**ï¼š
```python
# é«˜æ€§èƒ½ï¼Œéœ€è¦å¼ºå¤§ç¡¬ä»¶
models = {
    "LLaMA 2 70B": {
        "ä¼˜ç‚¹": ["æ¥è¿‘GPT-3.5æ€§èƒ½", "å¼€æº"],
        "ç¡¬ä»¶": "éœ€è¦å¤šGPU"
    },
    "Falcon 180B": {
        "ä¼˜ç‚¹": ["å½“æ—¶æœ€å¼ºå¼€æºæ¨¡å‹"],
        "ç¡¬ä»¶": "éœ€è¦é«˜ç«¯ç¡¬ä»¶"
    }
}
```

### é‡åŒ–æŠ€æœ¯æ˜¯ä»€ä¹ˆï¼Ÿ

é‡åŒ–å‡å°‘æ¨¡å‹å‚æ•°çš„ç²¾åº¦ä»¥é™ä½å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚

```python
import torch
from transformers import BitsAndBytesConfig

# 8ä½é‡åŒ–
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)

# 4ä½é‡åŒ–
quantization_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",  # NormalFloat 4
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

# åŠ è½½é‡åŒ–æ¨¡å‹
from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b",
    quantization_config=quantization_config_4bit,
    device_map="auto"
)
```

**é‡åŒ–æ•ˆæœ**ï¼š
- FP16 â†’ 8bitï¼šå†…å­˜å‡å°‘50%
- FP16 â†’ 4bitï¼šå†…å­˜å‡å°‘75%
- ç²¾åº¦æŸå¤±ï¼šé€šå¸¸åœ¨1-2%ä»¥å†…

## Fine-tuning

### ä»€ä¹ˆæ˜¯Fine-tuningï¼Ÿ

Fine-tuningæ˜¯åœ¨é¢„è®­ç»ƒæ¨¡å‹åŸºç¡€ä¸Šï¼Œç”¨ç‰¹å®šä»»åŠ¡æ•°æ®ç»§ç»­è®­ç»ƒã€‚

```python
from transformers import AutoModelForCausalLM, TrainingArguments, Trainer
from datasets import load_dataset

# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b")

# 2. å‡†å¤‡æ•°æ®é›†
dataset = load_dataset("csv", data_files="training_data.csv")

# 3. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    learning_rate=2e-5,
    warmup_steps=100,
    logging_steps=10,
    save_steps=100
)

# 4. è®­ç»ƒ
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
)

trainer.train()
```

### ä»€ä¹ˆæ˜¯LoRAï¼Ÿ

LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ã€‚

```python
from peft import LoraConfig, get_peft_model, TaskType

# LoRAé…ç½®
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,  # ä»»åŠ¡ç±»å‹
    inference_mode=False,
    r=8,  # LoRAç§©ï¼Œè¶Šå°å‚æ•°è¶Šå°‘
    lora_alpha=32,  # LoRAç¼©æ”¾å‚æ•°
    lora_dropout=0.1,
    target_modules=["q_proj", "v_proj"]  # è¦åº”ç”¨LoRAçš„æ¨¡å—
)

# åº”ç”¨LoRA
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# è¾“å‡ºç±»ä¼¼ï¼š
# trainable params: 4194304 || all params: 673841152 || trainable%: 0.62%
```

**LoRAä¼˜åŠ¿**ï¼š
- åªè®­ç»ƒ0.5-2%çš„å‚æ•°
- å¤§å¹…å‡å°‘æ˜¾å­˜éœ€æ±‚
- å¯ä»¥ä¸åŸºç¡€æ¨¡å‹åˆ†ç¦»å­˜å‚¨

### ä»€ä¹ˆæ˜¯QLoRAï¼Ÿ

QLoRAç»“åˆé‡åŒ–å’ŒLoRAï¼Œè¿›ä¸€æ­¥é™ä½å¾®è°ƒæˆæœ¬ã€‚

```python
from peft import LoraConfig, get_peft_model
from transformers import BitsAndBytesConfig

# 4ä½é‡åŒ–é…ç½®
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True
)

# åŠ è½½é‡åŒ–æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-70b",
    quantization_config=bnb_config,
    device_map="auto"
)

# åº”ç”¨LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

model = get_peft_model(model, lora_config)
```

**QLoRAæ•ˆæœ**ï¼š
- åœ¨å•å¼ 24GBæ˜¾å¡ä¸Šå¾®è°ƒ70Bæ¨¡å‹
- æ€§èƒ½æ¥è¿‘å…¨é‡å¾®è°ƒ

## æç¤ºå·¥ç¨‹

### ä»€ä¹ˆæ˜¯Prompt Templateï¼Ÿ

```python
from langchain.prompts import (
    PromptTemplate,
    ChatPromptTemplate,
    FewShotPromptTemplate
)

# åŸºç¡€æç¤ºæ¨¡æ¿
template = "è¯·è§£é‡Šä¸€ä¸‹{topic}ï¼Œç”¨{style}çš„é£æ ¼ã€‚"
prompt = PromptTemplate(
    template=template,
    input_variables=["topic", "style"]
)

# èŠå¤©æç¤ºæ¨¡æ¿
chat_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}ã€‚"),
    ("human", "{user_input}"),
])

# Few-shotæ¨¡æ¿
examples = [
    {"input": "å¼€å¿ƒ", "output": "ğŸ˜Š"},
    {"input": "éš¾è¿‡", "output": "ğŸ˜¢"}
]

few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=PromptTemplate(
        input_variables=["input", "output"],
        template="è¾“å…¥: {input}\nè¾“å‡º: {output}"
    ),
    prefix="ä»¥ä¸‹æ˜¯ä¸€äº›æƒ…æ„Ÿæ˜ å°„çš„ä¾‹å­ï¼š",
    suffix="è¾“å…¥: {input}\nè¾“å‡º:",
    input_variables=["input"]
)
```

### ä»€ä¹ˆæ˜¯System Prompt vs User Promptï¼Ÿ

```python
messages = [
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªPythonç¼–ç¨‹ä¸“å®¶ï¼Œæ“…é•¿è§£é‡Šä»£ç ã€‚"
    },
    {
        "role": "user",
        "content": "è§£é‡Šä¸€ä¸‹è£…é¥°å™¨æ˜¯ä»€ä¹ˆ"
    },
    {
        "role": "assistant",
        "content": "è£…é¥°å™¨æ˜¯..."
    },
    {
        "role": "user",
        "content": "ç»™æˆ‘ä¸€ä¸ªä¾‹å­"
    }
]
```

- **System Prompt**ï¼šè®¾å®šè§’è‰²å’Œè§„åˆ™
- **User Prompt**ï¼šç”¨æˆ·çš„å…·ä½“é—®é¢˜
- **Assistant Prompt**ï¼šå†å²å›å¤ï¼ˆå¤šè½®å¯¹è¯ï¼‰

### å¦‚ä½•ä¼˜åŒ–Promptï¼Ÿ

**ä¼˜åŒ–æŠ€å·§**ï¼š

1. **æ˜ç¡®æŒ‡ä»¤**ï¼š
```python
# âŒ æ¨¡ç³Š
prompt = "å†™ä¸ªå‡½æ•°"

# âœ… æ˜ç¡®
prompt = """
è¯·ç”¨Pythonå†™ä¸€ä¸ªå‡½æ•°ï¼Œå®ç°å¿«é€Ÿæ’åºç®—æ³•ã€‚
è¦æ±‚ï¼š
1. åŒ…å«è¯¦ç»†æ³¨é‡Š
2. å¤„ç†è¾¹ç•Œæƒ…å†µ
3. åŒ…å«æµ‹è¯•ç”¨ä¾‹
"""
```

2. **æä¾›ç¤ºä¾‹**ï¼š
```python
prompt = """
ä»»åŠ¡ï¼šå°†è‡ªç„¶è¯­è¨€è½¬æ¢ä¸ºSQL

ç¤ºä¾‹1ï¼š
è¾“å…¥ï¼šæŸ¥è¯¢æ‰€æœ‰å¹´é¾„å¤§äº30çš„ç”¨æˆ·
è¾“å‡ºï¼šSELECT * FROM users WHERE age > 30

ç¤ºä¾‹2ï¼š
è¾“å…¥ï¼šç»Ÿè®¡æ¯ä¸ªéƒ¨é—¨çš„å‘˜å·¥æ•°
è¾“å‡ºï¼šSELECT department, COUNT(*) FROM employees GROUP BY department

è¾“å…¥ï¼šæŸ¥è¯¢é”€å”®é¢å‰10çš„äº§å“
è¾“å‡ºï¼š
"""
```

3. **æ€ç»´é“¾**ï¼š
```python
prompt = """
é—®é¢˜ï¼šä¸€ä¸ªå†œåœºæœ‰é¸¡å’Œå…”å…±100åªï¼Œè…¿å…±320æ¡ï¼Œé¸¡å…”å„å¤šå°‘ï¼Ÿ

è®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ€è€ƒï¼š
1. è®¾é¸¡æœ‰xåªï¼Œå…”æœ‰yåª
2. æ ¹æ®é¢˜æ„ï¼šx + y = 100
3. è…¿æ•°æ–¹ç¨‹ï¼š2x + 4y = 320
4. è§£æ–¹ç¨‹ç»„...

ç­”æ¡ˆï¼š
"""
```

4. **è§’è‰²è®¾å®š**ï¼š
```python
prompt = """
ä½ æ˜¯ä¸€ä½æœ‰10å¹´ç»éªŒçš„ç®—æ³•å·¥ç¨‹å¸ˆã€‚
è¯·ä»ä»¥ä¸‹è§’åº¦åˆ†æè¿™æ®µä»£ç çš„æ—¶é—´å¤æ‚åº¦ï¼š
1. ç®—æ³•é€»è¾‘
2. å¾ªç¯åµŒå¥—
3. æ•°æ®ç»“æ„å½±å“
"""
```

## è¯„ä¼°æŒ‡æ ‡

### å¦‚ä½•è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼Ÿ

**å¸¸ç”¨æŒ‡æ ‡**ï¼š

```python
# BLEUåˆ†æ•°ï¼ˆæœºå™¨ç¿»è¯‘ï¼‰
from nltk.translate.bleu_score import sentence_bleu

reference = [["the", "cat", "is", "on", "the", "mat"]]
candidate = ["the", "cat", "is", "on", "mat"]

score = sentence_bleu(reference, candidate)
print(f"BLEU: {score}")

# ROUGEåˆ†æ•°ï¼ˆæ–‡æœ¬æ‘˜è¦ï¼‰
from rouge import Rouge

rouge = Rouge()
scores = rouge.get_scores(
    "the cat is on the mat",
    "the cat sat on the mat"
)

print(f"ROUGE-1: {scores[0]['rouge-1']['f']}")

# Perplexityï¼ˆå›°æƒ‘åº¦ï¼‰
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("gpt2")
tokenizer = AutoTokenizer.from_pretrained("gpt2")

text = "The quick brown fox"
inputs = tokenizer(text, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs, labels=inputs["input_ids"])
    perplexity = torch.exp(outputs.loss)

print(f"Perplexity: {perplexity.item()}")
```

### å¦‚ä½•è¯„ä¼°RAGç³»ç»Ÿï¼Ÿ

```python
from ragas import evaluate
from datasets import Dataset

# å‡†å¤‡è¯„ä¼°æ•°æ®
eval_data = {
    "question": [
        "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "Pythonä¸­å¦‚ä½•å¤„ç†å¼‚å¸¸ï¼Ÿ"
    ],
    "answer": [
        "æœºå™¨å­¦ä¹ æ˜¯...",
        "ä½¿ç”¨try-exceptè¯­å¥..."
    ],
    "contexts": [
        ["æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„åˆ†æ”¯..."],
        ["Pythonå¼‚å¸¸å¤„ç†ä½¿ç”¨try-except..."]
    ],
    "ground_truth": [
        "æœºå™¨å­¦ä¹ ä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ",
        "try-exceptè¯­å¥ç”¨äºæ•è·å’Œå¤„ç†å¼‚å¸¸"
    ]
}

dataset = Dataset.from_dict(eval_data)

# è¯„ä¼°
result = evaluate(dataset)

print(result)
# è¾“å‡ºï¼š
# {'faithfulness': 0.85, 'answer_relevancy': 0.82}
```

**å…³é”®æŒ‡æ ‡**ï¼š
- **Faithfulness**ï¼šç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢çš„ä¸Šä¸‹æ–‡
- **Answer Relevancy**ï¼šç­”æ¡ˆæ˜¯å¦ä¸é—®é¢˜ç›¸å…³
- **Context Precision**ï¼šæ£€ç´¢çš„ä¸Šä¸‹æ–‡æ˜¯å¦ç›¸å…³

## æ¨¡å‹éƒ¨ç½²

### å¦‚ä½•ä½¿ç”¨vLLMåŠ é€Ÿæ¨ç†ï¼Ÿ

```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="meta-llama/Llama-2-7b",
    tensor_parallel_size=2,  # GPUæ•°é‡
    gpu_memory_utilization=0.9,
    max_model_len=4096
)

# ç”Ÿæˆå‚æ•°
sampling_params = SamplingParams(
    temperature=0.8,
    top_p=0.95,
    max_tokens=1000
)

# æ‰¹é‡ç”Ÿæˆ
prompts = [
    "å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—",
    "è§£é‡Šä»€ä¹ˆæ˜¯é‡å­è®¡ç®—"
]

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(output.outputs[0].text)
```

### å¦‚ä½•ä½¿ç”¨Text Generation Inference (TGI)ï¼Ÿ

```bash
# å¯åŠ¨TGIæœåŠ¡
model=meta-llama/Llama-2-7b
volume=$PWD/data # share a volume with the Docker container to avoid downloading weights every run

docker run --gpus all --shm-size 1g -p 8080:80 \
  -v $volume:/data \
  ghcr.io/huggingface/text-generation-inference:latest \
  --model-id $model
```

```python
# Pythonå®¢æˆ·ç«¯
import requests

API_URL = "http://localhost:8080/generate"

def query(payload):
    response = requests.post(API_URL, json=payload)
    return response.json()

output = query({
    "inputs": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
    "parameters": {
        "max_new_tokens": 200,
        "temperature": 0.7
    }
})

print(output[0]["generated_text"])
```

---

**å°å¾å¸¦ä½ é£ç³»åˆ—æ•™ç¨‹**

**æœ€åæ›´æ–°ï¼š2026 å¹´ 2 æœˆ**
**ç‰ˆæœ¬ï¼šv1.0**
**ä½œè€…ï¼šå°å¾**
**é‚®ç®±ï¼šesimonx@163.com**
