# ç¬¬4ç« ï¼šRAGæ£€ç´¢å¢å¼ºç”Ÿæˆ

## æœ¬ç« å¯¼è¯»

**RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰** æ˜¯ç›®å‰æœ€çƒ­é—¨çš„AIåº”ç”¨æŠ€æœ¯ä¹‹ä¸€ã€‚å®ƒç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆï¼Œè®©AIèƒ½å¤ŸåŸºäºä½ çš„ç§æœ‰æ•°æ®å›ç­”é—®é¢˜ï¼Œè§£å†³LLMçŸ¥è¯†è¿‡æ—¶å’Œå¹»è§‰é—®é¢˜ã€‚

**å­¦ä¹ ç›®æ ‡**ï¼š
- ç†è§£RAGçš„åŸç†å’Œåº”ç”¨åœºæ™¯
- æŒæ¡æ–‡æ¡£åŠ è½½å’Œåˆ†å‰²æŠ€æœ¯
- å­¦ä¹ Embeddingså’Œå‘é‡æ•°æ®åº“
- æ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿ

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š70åˆ†é’Ÿ

---

## 4.1 ä»€ä¹ˆæ˜¯RAGï¼Ÿ

### 4.1.1 RAGçš„å®šä¹‰

**RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰** æ˜¯ä¸€ç§æŠ€æœ¯æ¡†æ¶ï¼Œé€šè¿‡å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶åå°†æ£€ç´¢åˆ°çš„å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™LLMï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„å›ç­”ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RAG å·¥ä½œæµç¨‹                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  ç”¨æˆ·é—®é¢˜                                       â”‚
â”‚    â†“                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ 1. æ£€ç´¢é˜¶æ®µ  â”‚ â†’ ä»çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚    â†“                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ 2. å¢å¼ºé˜¶æ®µ  â”‚ â†’ å°†æ£€ç´¢å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚    â†“                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ 3. ç”Ÿæˆé˜¶æ®µ  â”‚ â†’ LLMåŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚    â†“                                            â”‚
â”‚  å‡†ç¡®çš„å›ç­” + å¼•ç”¨æ¥æº                           â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.1.2 ä¸ºä»€ä¹ˆéœ€è¦RAGï¼Ÿ

| é—®é¢˜ | çº¯LLMçš„å±€é™ | RAGçš„è§£å†³æ–¹æ¡ˆ |
|------|-------------|--------------|
| **çŸ¥è¯†è¿‡æ—¶** | è®­ç»ƒæˆªæ­¢æ—¥æœŸåçš„äº‹ä»¶ä¸çŸ¥é“ | å®æ—¶æ›´æ–°æ–‡æ¡£åº“ |
| **å¹»è§‰é—®é¢˜** | å¯èƒ½ç¼–é€ é”™è¯¯ä¿¡æ¯ | åŸºäºçœŸå®æ–‡æ¡£å›ç­” |
| **ç§æœ‰æ•°æ®** | æ— æ³•è®¿é—®ä¼ä¸šå†…éƒ¨æ•°æ® | é›†æˆç§æœ‰çŸ¥è¯†åº“ |
| **å¯è¿½æº¯æ€§** | æ— æ³•éªŒè¯ç­”æ¡ˆæ¥æº | æä¾›å¼•ç”¨æ¥æº |
| **ä¸“ä¸šæ€§** | é€šç”¨çŸ¥è¯†ï¼Œä¸å¤Ÿä¸“ä¸š | é¢†åŸŸä¸“ç”¨çŸ¥è¯†åº“ |

### 4.1.3 RAG vs å¾®è°ƒï¼ˆFine-tuningï¼‰

```
RAG vs Fine-tuning å¯¹æ¯”ï¼š

RAGï¼š
  âœ… å¿«é€Ÿå®æ–½ï¼ˆå‡ å°æ—¶åˆ°å‡ å¤©ï¼‰
  âœ… æ— éœ€è®­ç»ƒï¼Œæˆæœ¬ä½
  âœ… æ˜“äºæ›´æ–°çŸ¥è¯†
  âœ… å¯è¿½æº¯ç­”æ¡ˆæ¥æº
  âœ… é€‚åˆçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡

Fine-tuningï¼š
  âœ… æ”¹å˜æ¨¡å‹çš„è¯´è¯æ–¹å¼
  âœ… å­¦ä¹ ç‰¹å®šæ ¼å¼
  âœ… è®­ç»ƒæˆæœ¬é«˜
  âŒ çŸ¥è¯†æ›´æ–°å›°éš¾
  âŒ éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®

ğŸ’¡ æœ€ä½³å®è·µï¼šRAG + Fine-tuning ç»“åˆä½¿ç”¨
```

### 4.1.4 RAGçš„åº”ç”¨åœºæ™¯

**1. ä¼ä¸šçŸ¥è¯†åº“é—®ç­”**
```python
# ç¤ºä¾‹ï¼šå†…éƒ¨æ–‡æ¡£é—®ç­”
Q: "å…¬å¸çš„æŠ¥é”€æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ"
A: "æ ¹æ®ã€Šå‘˜å·¥æ‰‹å†Œã€‹ç¬¬5ç« ï¼š
    1. å¡«å†™æŠ¥é”€å•
    2. éƒ¨é—¨ç»ç†å®¡æ‰¹
    3. è´¢åŠ¡å®¡æ ¸
    4. 3-5ä¸ªå·¥ä½œæ—¥åˆ°è´¦
    æ¥æºï¼šå‘˜å·¥æ‰‹å†Œ_v3.0.pdf, ç¬¬23é¡µ"
```

**2. æ™ºèƒ½å®¢æœç³»ç»Ÿ**
```python
# ç¤ºä¾‹ï¼šäº§å“æ”¯æŒ
Q: "å¦‚ä½•é‡ç½®è·¯ç”±å™¨ï¼Ÿ"
A: "æ ¹æ®äº§å“æ–‡æ¡£ï¼š
    æ­¥éª¤1ï¼šé•¿æŒ‰Reseté”®10ç§’
    æ­¥éª¤2ï¼šç­‰å¾…æŒ‡ç¤ºç¯é—ªçƒ
    æ­¥éª¤3ï¼šé‡æ–°é…ç½®ç½‘ç»œ
    å¦‚æœ‰é—®é¢˜è¯·è”ç³»å®¢æœ..."
```

**3. æ³•å¾‹æ–‡æ¡£åˆ†æ**
```python
# ç¤ºä¾‹ï¼šåˆåŒå®¡æŸ¥
Q: "è¿™ä»½åˆåŒæœ‰å“ªäº›é£é™©ç‚¹ï¼Ÿ"
A: "æ ¹æ®åˆåŒå®¡æŸ¥æŒ‡å—ï¼š
    å‘ç°3ä¸ªæ½œåœ¨é£é™©ï¼š
    1. ç¬¬5æ¡ï¼šè¿çº¦è´£ä»»ä¸æ˜ç¡®
    2. ç¬¬12æ¡ï¼šç®¡è¾–æ¡æ¬¾å¯èƒ½ä¸åˆ©
    3. ..."
```

**4. æŠ€æœ¯æ–‡æ¡£åŠ©æ‰‹**
```python
# ç¤ºä¾‹ï¼šä»£ç æ–‡æ¡£é—®ç­”
Q: "å¦‚ä½•åœ¨Djangoä¸­å®ç°JWTè®¤è¯ï¼Ÿ"
A: "æ ¹æ®é¡¹ç›®æ–‡æ¡£ï¼š
    ä½¿ç”¨django-rest-framework-simplejwtï¼š
    [ä»£ç ç¤ºä¾‹]
    ç›¸å…³æ–‡æ¡£ï¼šapi_auth.md"
```

---

## 4.2 RAGç³»ç»Ÿæ¶æ„

### 4.2.1 å®Œæ•´çš„RAGæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            RAGç³»ç»Ÿå®Œæ•´æ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  å‡†å¤‡é˜¶æ®µï¼ˆä¸€æ¬¡æ€§ï¼‰                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ æ–‡æ¡£åŠ è½½ â”‚ â†’   â”‚ æ–‡æœ¬åˆ†å‰² â”‚ â†’  â”‚ å‘é‡åŒ–   â”‚    â”‚
â”‚  â”‚ Loader  â”‚      â”‚Splitter â”‚    â”‚Embeddingsâ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â†“                      â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                    â”‚ å‘é‡æ•°æ®åº“   â”‚                â”‚
â”‚                    â”‚Vector Store â”‚                â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                    â”‚
â”‚  è¿è¡Œé˜¶æ®µï¼ˆæ¯æ¬¡æŸ¥è¯¢ï¼‰                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ç”¨æˆ·é—®é¢˜ â”‚ â†’  â”‚ é—®é¢˜å‘é‡åŒ–â”‚ â†’  â”‚ ç›¸ä¼¼åº¦æ£€ç´¢â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                   â†“                â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                          â”‚ Top-Kæ–‡æ¡£   â”‚           â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                   â†“                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚          Prompt + æ£€ç´¢å†…å®¹ + é—®é¢˜         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ LLMç”Ÿæˆ  â”‚ â†’ å›ç­” + å¼•ç”¨                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2.2 æ ¸å¿ƒç»„ä»¶è¯´æ˜

```python
# 1. Document Loaders - æ–‡æ¡£åŠ è½½å™¨
from langchain.document_loaders import (
    PyPDFLoader,           # PDFæ–‡æ¡£
    TextLoader,            # æ–‡æœ¬æ–‡æ¡£
    DirectoryLoader,       # ç›®å½•åŠ è½½
    CSVLoader,             # CSVæ–‡ä»¶
    UnstructuredMarkdownLoader,  # Markdown
    WebBaseLoader          # ç½‘é¡µ
)

# 2. Text Splitters - æ–‡æœ¬åˆ†å‰²å™¨
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,  # é€’å½’åˆ†å‰²
    CharacterTextSplitter,           # å­—ç¬¦åˆ†å‰²
    TokenTextSplitter,               # Tokenåˆ†å‰²
    MarkdownTextSplitter             # Markdownåˆ†å‰²
)

# 3. Embeddings - å‘é‡åŒ–
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import (
    HuggingFaceEmbeddings,  # å¼€æºæ¨¡å‹
    CohereEmbeddings,
    BedrockEmbeddings
)

# 4. Vector Stores - å‘é‡æ•°æ®åº“
from langchain.vectorstores import (
    Chroma,           # è½»é‡çº§ï¼Œæ¨è
    FAISS,            # Facebookå‡ºå“
    Pinecone,         # äº‘æœåŠ¡
    Weaviate          # å¼€æº
)
```

---

## 4.3 æ–‡æ¡£åŠ è½½ï¼ˆDocument Loadingï¼‰

### 4.3.1 æ”¯æŒçš„æ–‡æ¡£ç±»å‹

LangChainæ”¯æŒ100+ç§æ–‡æ¡£æ ¼å¼ï¼

| æ ¼å¼ | Loader | è¯´æ˜ |
|------|--------|------|
| PDF | `PyPDFLoader` | æœ€å¸¸ç”¨ |
| Word | `Docx2txtLoader` | Wordæ–‡æ¡£ |
| TXT | `TextLoader` | çº¯æ–‡æœ¬ |
| Markdown | `UnstructuredMarkdownLoader` | MDæ–‡ä»¶ |
| HTML | `Bs4HTMLLoader` | ç½‘é¡µ |
| CSV | `CSVLoader` | è¡¨æ ¼æ•°æ® |
| JSON | `JSONLoader` | JSONæ•°æ® |
| ä»£ç  | `PythonLoader` | æºä»£ç  |

### 4.3.2 åŠ è½½å•ä¸ªæ–‡æ¡£

```python
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import TextLoader

# åŠ è½½PDF
pdf_loader = PyPDFLoader("docs/manual.pdf")
pdf_docs = pdf_loader.load()

print(f"åŠ è½½äº† {len(pdf_docs)} é¡µ")
print(f"ç¬¬ä¸€é¡µå†…å®¹ï¼š\n{pdf_docs[0].page_content[:500]}")
print(f"å…ƒæ•°æ®ï¼š{pdf_docs[0].metadata}")
# {'source': 'docs/manual.pdf', 'page': 0}

# åŠ è½½TXT
txt_loader = TextLoader("docs/guide.txt", encoding='utf-8')
txt_docs = txt_loader.load()
```

### 4.3.3 åŠ è½½ç›®å½•

```python
from langchain.document_loaders import DirectoryLoader

# åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰PDF
loader = DirectoryLoader(
    'docs/',                    # ç›®å½•è·¯å¾„
    glob="**/*.pdf",            # æ–‡ä»¶æ¨¡å¼
    loader_cls=PyPDFLoader,     # ä½¿ç”¨çš„åŠ è½½å™¨
    show_progress=True,         # æ˜¾ç¤ºè¿›åº¦
    use_multithreading=True     # å¤šçº¿ç¨‹åŠ è½½
)

docs = loader.load()
print(f"æ€»å…±åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£å—")
```

### 4.3.4 åŠ è½½ç½‘é¡µ

```python
from langchain.document_loaders import WebBaseLoader
from bs4 import SoupStrainer

# åªæå–ç‰¹å®šå†…å®¹
bs4_strainer = SoupStrainer(class_=("post-title", "post-header", "post-content"))

loader = WebBaseLoader(
    "https://python.langchain.com/docs/get_started/introduction",
    bs_kwargs={"parse_only": bs4_strainer}
)

docs = loader.load()
print(docs[0].page_content[:500])
```

### 4.3.5 è‡ªå®šä¹‰Loader

```python
from langchain.document_loaders.base import BaseLoader
from typing import List
from langchain.schema import Document

class MyCustomLoader(BaseLoader):
    """è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨"""

    def __init__(self, source: str):
        self.source = source

    def load(self) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        with open(self.source, 'r', encoding='utf-8') as f:
            content = f.read()

        # è‡ªå®šä¹‰å¤„ç†é€»è¾‘
        metadata = {"source": self.source}

        return [Document(page_content=content, metadata=metadata)]

# ä½¿ç”¨
loader = MyCustomLoader("data/custom.txt")
docs = loader.load()
```

---

## 4.4 æ–‡æœ¬åˆ†å‰²ï¼ˆText Splittingï¼‰

### 4.4.1 ä¸ºä»€ä¹ˆéœ€è¦åˆ†å‰²ï¼Ÿ

```
é—®é¢˜ï¼šLLMæœ‰ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶

ç¤ºä¾‹ï¼šGPT-3.5-turbo
  - æœ€å¤§è¾“å…¥ï¼š4096 tokens
  - 1 token â‰ˆ 0.75 ä¸ªè‹±æ–‡å•è¯
  - 1 token â‰ˆ 1.5-2 ä¸ªæ±‰å­—

å¦‚æœä¸åˆ†å‰²ï¼š
  âŒ æ•´æœ¬ä¹¦æ— æ³•ä¸€æ¬¡æ€§è¾“å…¥
  âŒ æ£€ç´¢ä¸ç²¾å‡†ï¼ˆæ‰¾åˆ°å†—ä½™å†…å®¹ï¼‰
  âŒ æˆæœ¬é«˜ï¼ˆå¤„ç†å¤§é‡æ— å…³å†…å®¹ï¼‰

åˆ†å‰²åï¼š
  âœ… æ¯ä¸ªå—éƒ½æ˜¯ç‹¬ç«‹è¯­ä¹‰å•å…ƒ
  âœ… æ£€ç´¢æ›´ç²¾å‡†
  âœ… é™ä½Tokenæ¶ˆè€—
```

### 4.4.2 åˆ†å‰²ç­–ç•¥

```python
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    TokenTextSplitter
)

# 1. RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰
# é€’å½’åœ°æŒ‰ä¸åŒåˆ†éš”ç¬¦åˆ†å‰²
recursive_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,              # æ¯å—æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=200,            # å—ä¹‹é—´é‡å å­—ç¬¦æ•°
    length_function=len,          # é•¿åº¦è®¡ç®—å‡½æ•°
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
)

# åˆ†å‰²é€»è¾‘ï¼š
# 1. å…ˆæŒ‰æ®µè½åˆ†éš”ï¼ˆ\n\nï¼‰
# 2. å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼ŒæŒ‰å¥å­åˆ†éš”
# 3. å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼ŒæŒ‰è¯åˆ†éš”
# 4. æœ€åæŒ‰å­—ç¬¦åˆ†å‰²

splits = recursive_splitter.split_documents(docs)
print(f"åˆ†å‰²æˆ {len(splits)} ä¸ªå—")

# 2. CharacterTextSplitter
# æŒ‰å›ºå®šå­—ç¬¦æ•°åˆ†å‰²
character_splitter = CharacterTextSplitter(
    separator="\n\n",      # åˆ†éš”ç¬¦
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)

# 3. TokenTextSplitter
# æŒ‰Tokenæ•°é‡åˆ†å‰²ï¼ˆæ›´ç²¾å‡†ï¼‰
from langchain.text_splitter import TokenTextSplitter

token_splitter = TokenTextSplitter(
    chunk_size=500,        # 500 tokens
    chunk_overlap=50,
    encoding_name="cl100k_base"  # GPT-3.5/4çš„ç¼–ç 
)

splits = token_splitter.split_documents(docs)
```

### 4.4.3 ä¸åŒæ–‡æ¡£ç±»å‹çš„åˆ†å‰²

```python
# Markdownæ–‡æ¡£
from langchain.text_splitter import MarkdownTextSplitter

markdown_splitter = MarkdownTextSplitter(
    chunk_size=1000,
    chunk_overlap=0
)

# æŒ‰Markdownç»“æ„åˆ†å‰²ï¼ˆ#ã€##ç­‰ï¼‰
splits = markdown_splitter.split_text(markdown_content)

# ä»£ç æ–‡æ¡£
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    Language
)

python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=1000,
    chunk_overlap=100
)

# æŒ‰å‡½æ•°ã€ç±»ç­‰ä»£ç ç»“æ„åˆ†å‰²
code_splits = python_splitter.split_documents(code_docs)

# JavaScript
js_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.JS
)
```

### 4.4.4 åˆ†å‰²æ•ˆæœè¯„ä¼°

```python
def evaluate_splits(splits):
    """è¯„ä¼°åˆ†å‰²æ•ˆæœ"""
    for i, split in enumerate(splits[:5]):  # æŸ¥çœ‹å‰5ä¸ª
        print(f"\n--- å— {i+1} ---")
        print(f"é•¿åº¦ï¼š{len(split.page_content)} å­—ç¬¦")
        print(f"æ¥æºï¼š{split.metadata.get('source', 'Unknown')}")
        print(f"å†…å®¹é¢„è§ˆï¼š{split.page_content[:200]}...")

    # ç»Ÿè®¡
    lengths = [len(s.page_content) for s in splits]
    print(f"\nç»Ÿè®¡ï¼š")
    print(f"æ€»å—æ•°ï¼š{len(splits)}")
    print(f"å¹³å‡é•¿åº¦ï¼š{sum(lengths)/len(lengths):.0f}")
    print(f"æœ€å°é•¿åº¦ï¼š{min(lengths)}")
    print(f"æœ€å¤§é•¿åº¦ï¼š{max(lengths)}")

# ä½¿ç”¨
evaluate_splits(splits)
```

---

## 4.5 Embeddingså’Œå‘é‡æ•°æ®åº“

### 4.5.1 ä»€ä¹ˆæ˜¯Embeddingsï¼Ÿ

**Embeddingsï¼ˆåµŒå…¥ï¼‰** æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼ˆæ•°å­—æ•°ç»„ï¼‰çš„æŠ€æœ¯ï¼Œç›¸ä¼¼çš„æ–‡æœ¬ä¼šæœ‰ç›¸ä¼¼çš„å‘é‡ã€‚

```python
# ç¤ºä¾‹ï¼šæ–‡æœ¬å‘é‡åŒ–
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(openai_api_key="your-key")

# è½¬æ¢ä¸ºå‘é‡
text = "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€"
vector = embeddings.embed_query(text)

print(f"å‘é‡ç»´åº¦ï¼š{len(vector)}")  # 1536 (OpenAI embeddings)
print(f"å‰10ä¸ªå€¼ï¼š{vector[:10]}")
# [0.0023, -0.0156, 0.0089, ...]
```

**è¯­ä¹‰ç›¸ä¼¼åº¦ç¤ºä¾‹**ï¼š

```python
# è®¡ç®—ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

texts = [
    "Pythonæ˜¯ç¼–ç¨‹è¯­è¨€",
    "Javaæ˜¯ç¼–ç¨‹è¯­è¨€",
    "ä»Šå¤©å¤©æ°”ä¸é”™",
    "æœºå™¨å­¦ä¹ æ˜¯AIçš„åˆ†æ”¯"
]

vectors = [embeddings.embed_query(t) for t in texts]

# è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
similarities = cosine_similarity(vectors)

# ç»“æœï¼šPythonå’ŒJavaç›¸ä¼¼åº¦é«˜ï¼Œä¸å¤©æ°”ç›¸ä¼¼åº¦ä½
# [[1.00, 0.89, 0.23, 0.45],
#  [0.89, 1.00, 0.21, 0.43],
#  [0.23, 0.21, 1.00, 0.19],
#  [0.45, 0.43, 0.19, 1.00]]
```

### 4.5.2 ä¸»æµEmbeddingæ¨¡å‹

| æ¨¡å‹ | ç»´åº¦ | ç‰¹ç‚¹ | ä»·æ ¼ |
|------|------|------|------|
| **OpenAI text-embedding-3** | 1536 | ç»¼åˆæ€§èƒ½æœ€ä½³ | $0.0001/1K tokens |
| **Cohere embed-v3** | 1024 | å¤šè¯­è¨€æ”¯æŒå¥½ | å…è´¹/ä»˜è´¹ |
| **HuggingFace mteb** | 768 | å¼€æºå…è´¹ | å®Œå…¨å…è´¹ |
| **BGE-large-zh** | 1024 | ä¸­æ–‡ä¼˜åŒ– | å…è´¹ |

```python
# ä½¿ç”¨OpenAI Embeddings
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",  # æœ€ç»æµ
    openai_api_key="your-key"
)

# ä½¿ç”¨å¼€æºæ¨¡å‹ï¼ˆå…è´¹ï¼‰
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5",  # ä¸­æ–‡æ¨¡å‹
    model_kwargs={'device': 'cpu'},  # æˆ– 'cuda' å¦‚æœæœ‰GPU
    encode_kwargs={'normalize_embeddings': True}
)

# ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="shibing624/text2vec-base-chinese"
)
```

### 4.5.3 å‘é‡æ•°æ®åº“é€‰æ‹©

```
å‘é‡æ•°æ®åº“å¯¹æ¯”ï¼š

Chroma:
  âœ… è½»é‡çº§ï¼Œæ˜“éƒ¨ç½²
  âœ… æ— éœ€é¢å¤–æœåŠ¡
  âœ… é€‚åˆä¸­å°è§„æ¨¡æ•°æ®
  âœ… å®Œå…¨å…è´¹
  ğŸ’¡ æ¨èç”¨äºå­¦ä¹ å’ŒåŸå‹

FAISS:
  âœ… Facebookå‡ºå“ï¼Œæ€§èƒ½ä¼˜ç§€
  âœ… æ”¯æŒGPUåŠ é€Ÿ
  âœ… å†…å­˜ç´¢å¼•ï¼Œé€Ÿåº¦å¿«
  âŒ ä¸æ”¯æŒæŒä¹…åŒ–ï¼ˆéœ€æ‰‹åŠ¨ä¿å­˜ï¼‰
  ğŸ’¡ æ¨èç”¨äºå¤§è§„æ¨¡æ•°æ®

Pinecone:
  âœ… äº‘æœåŠ¡ï¼Œå®Œå…¨æ‰˜ç®¡
  âœ… è‡ªåŠ¨æ‰©å±•
  âœ… ä¼ä¸šçº§æ”¯æŒ
  âŒ éœ€è¦ä»˜è´¹
  ğŸ’¡ æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ

Weaviate:
  âœ… åŠŸèƒ½ä¸°å¯Œ
  âœ… æ”¯æŒå¤šç§æ•°æ®ç±»å‹
  âŒ é…ç½®ç›¸å¯¹å¤æ‚
  ğŸ’¡ æ¨èç”¨äºå¤æ‚åœºæ™¯
```

### 4.5.4 ä½¿ç”¨Chromaå‘é‡æ•°æ®åº“

```python
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. å‡†å¤‡æ–‡æ¡£
loader = PyPDFLoader("docs/manual.pdf")
docs = loader.load()

# 2. åˆ†å‰²
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(docs)

# 3. åˆ›å»ºå‘é‡æ•°æ®åº“
embeddings = OpenAIEmbeddings()

# æ–¹å¼1ï¼šåˆ›å»ºä¸´æ—¶æ•°æ®åº“ï¼ˆå†…å­˜ï¼‰
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)

# æ–¹å¼2ï¼šæŒä¹…åŒ–åˆ°ç£ç›˜
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./chroma_db"  # ä¿å­˜è·¯å¾„
)

# æ–¹å¼3ï¼šåŠ è½½å·²å­˜åœ¨çš„æ•°æ®åº“
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# 4. ç›¸ä¼¼åº¦æœç´¢
query = "å¦‚ä½•å®‰è£…Pythonï¼Ÿ"
results = vectorstore.similarity_search(query, k=3)  # è¿”å›Top 3

for i, result in enumerate(results):
    print(f"\n--- ç»“æœ {i+1} ---")
    print(f"å†…å®¹ï¼š{result.page_content[:200]}...")
    print(f"å…ƒæ•°æ®ï¼š{result.metadata}")

# 5. å¸¦åˆ†æ•°çš„æœç´¢
results_with_scores = vectorstore.similarity_search_with_score(query, k=3)

for doc, score in results_with_scores:
    print(f"\nç›¸ä¼¼åº¦åˆ†æ•°ï¼š{score:.4f}")
    print(f"å†…å®¹ï¼š{doc.page_content[:200]}...")
```

### 4.5.5 ä½¿ç”¨FAISS

```python
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# åˆ›å»ºFAISSç´¢å¼•
embeddings = OpenAIEmbeddings()
faiss_index = FAISS.from_documents(splits, embeddings)

# ä¿å­˜åˆ°ç£ç›˜
faiss_index.save_local("faiss_index")

# åŠ è½½
faiss_index = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)

# æœç´¢
results = faiss_index.similarity_search(query, k=3)

# æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ï¼ˆMMRï¼‰
# å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
results = faiss_index.max_marginal_relevance_search(
    query,
    k=3,
    fetch_k=10  # ä»å‰10ä¸ªä¸­é€‰æ‹©3ä¸ª
)
```

---

## 4.6 æ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿ

### 4.6.1 åŸºç¡€RAGé“¾

```python
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. åˆå§‹åŒ–ç»„ä»¶
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
embeddings = OpenAIEmbeddings()

# 2. åŠ è½½å‘é‡æ•°æ®åº“
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # è¿”å›Top 3
)

# 3. åˆ›å»ºæç¤ºè¯æ¨¡æ¿
template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œæ˜ç¡®å‘ŠçŸ¥
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´
4. æ ‡æ³¨ä¿¡æ¯æ¥æº

å›ç­”ï¼š"""

prompt = ChatPromptTemplate.from_template(template)

# 4. åˆ›å»ºRAGé“¾
def format_docs(docs):
    """æ ¼å¼åŒ–æ–‡æ¡£"""
    return "\n\n".join([
        f"ã€æ–‡æ¡£{i+1}ã€‘{doc.page_content}"
        for i, doc in enumerate(docs)
    ])

rag_chain = (
    {
        "context": retriever | format_docs,  # æ£€ç´¢å¹¶æ ¼å¼åŒ–
        "question": RunnablePassthrough()    # ä¼ é€’é—®é¢˜
    }
    | prompt
    | llm
    | StrOutputParser()
)

# 5. ä½¿ç”¨
query = "Pythonä¸­çš„è£…é¥°å™¨æ˜¯ä»€ä¹ˆï¼Ÿ"
answer = rag_chain.invoke(query)
print(answer)
```

### 4.6.2 å¸¦æ¥æºå¼•ç”¨çš„RAG

```python
from langchain_core.prompts import PromptTemplate

# æç¤ºè¯æ¨¡æ¿
template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
## å›ç­”
[ä½ çš„å›ç­”]

## ä¿¡æ¯æ¥æº
- æ–‡æ¡£1ï¼š[æ¥æºä¿¡æ¯]
- æ–‡æ¡£2ï¼š[æ¥æºä¿¡æ¯]
"""

prompt = PromptTemplate.from_template(template)

# ä¿®æ”¹format_docsä¿ç•™å…ƒæ•°æ®
def format_docs_with_source(docs):
    """æ ¼å¼åŒ–æ–‡æ¡£å¹¶ä¿ç•™æ¥æº"""
    return "\n\n".join([
        f"æ¥æºï¼š{doc.metadata.get('source', 'Unknown')}"
        f"ï¼ˆç¬¬{doc.metadata.get('page', 0)}é¡µï¼‰\n"
        f"å†…å®¹ï¼š{doc.page_content}"
        for doc in docs
    ])

# åˆ›å»ºé“¾
rag_chain = (
    {
        "context": retriever | format_docs_with_source,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)
```

### 4.6.3 æµå¼è¾“å‡ºRAG

```python
# æµå¼è¾“å‡º
for chunk in rag_chain.stream("è§£é‡ŠPythonçš„GIL"):
    print(chunk, end="", flush=True)
```

### 4.6.4 å®Œæ•´çš„RAGåº”ç”¨

```python
class RAGSystem:
    """å®Œæ•´çš„RAGç³»ç»Ÿ"""

    def __init__(self, db_path="./chroma_db"):
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            persist_directory=db_path,
            embedding_function=self.embeddings
        )
        self.retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 3}
        )

        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0
        )

        self.chain = self._create_chain()

    def _create_chain(self):
        """åˆ›å»ºRAGé“¾"""
        template = """ä½ æ˜¯ä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºä¸Šä¸‹æ–‡å‡†ç¡®å›ç­”
2. å¦‚æ— ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
3. å¼•ç”¨ä¿¡æ¯æ¥æº

å›ç­”ï¼š"""

        prompt = ChatPromptTemplate.from_template(template)

        def format_docs(docs):
            return "\n\n".join([
                f"ã€æ¥æºï¼š{doc.metadata.get('source', 'Unknown')}ã€‘\n{doc.page_content}"
                for doc in docs
            ])

        return (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

    def ask(self, question: str) -> dict:
        """æé—®"""
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.vectorstore.similarity_search(question, k=3)

        # 2. ç”Ÿæˆå›ç­”
        answer = self.chain.invoke(question)

        # 3. è¿”å›ç»“æœå’Œæ¥æº
        return {
            "question": question,
            "answer": answer,
            "sources": [
                {
                    "source": doc.metadata.get('source'),
                    "page": doc.metadata.get('page'),
                    "content": doc.page_content[:200] + "..."
                }
                for doc in docs
            ]
        }

    def chat(self):
        """äº¤äº’å¼å¯¹è¯"""
        print("ğŸ¤– RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼ˆè¾“å…¥'quit'é€€å‡ºï¼‰\n")

        while True:
            question = input("\nä½ ï¼š").strip()

            if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("å†è§ï¼ğŸ‘‹")
                break

            if not question:
                continue

            try:
                result = self.ask(question)

                print(f"\nğŸ“ å›ç­”ï¼š\n{result['answer']}\n")

                print("ğŸ“š ä¿¡æ¯æ¥æºï¼š")
                for i, source in enumerate(result['sources'], 1):
                    print(f"\n{i}. {source['source']}")
                    print(f"   å†…å®¹ï¼š{source['content']}")

            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼š{e}")

# ä½¿ç”¨
if __name__ == "__main__":
    rag = RAGSystem()
    rag.chat()
```

---

## 4.7 RAGä¼˜åŒ–æŠ€å·§

### 4.7.1 æ£€ç´¢ä¼˜åŒ–

```python
# 1. è°ƒæ•´æ£€ç´¢æ•°é‡
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 5,        # å¢åŠ æ£€ç´¢æ•°é‡
        "score_threshold": 0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
    }
)

# 2. ä½¿ç”¨MMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20}
)

# 3. å¤šæŸ¥è¯¢æ£€ç´¢
from langchain.retrievers import MultiQueryRetriever

retriever = MultiQueryRetriver.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=ChatOpenAI(temperature=0)
)

# è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“ï¼Œæé«˜å¬å›ç‡

# 4. ä¸Šä¸‹æ–‡å‹ç¼©
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(ChatOpenAI())
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)
```

### 4.7.2 æ··åˆæœç´¢

```python
# ç»“åˆå…³é”®è¯æœç´¢å’Œè¯­ä¹‰æœç´¢
from langchain.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

# BM25å…³é”®è¯æ£€ç´¢
bm25_retriever = BM25Retriever.from_documents(splits)
bm25_retriever.k = 3

# è¯­ä¹‰æ£€ç´¢
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# ç»„åˆæ£€ç´¢
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.4, 0.6]  # æƒé‡
)
```

### 4.7.3 é‡æ’åºï¼ˆRerankingï¼‰

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerankCompressor

# ä½¿ç”¨Cohere Rerank APIé‡æ–°æ’åº
compressor = CohereRerankCompressor(
    cohere_api_key="your-key",
    top_n_queries=3  # ä¿ç•™å‰3ä¸ªæœ€ç›¸å…³çš„
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever(search_kwargs={"k": 10})
)
# å…ˆæ£€ç´¢10ä¸ªï¼Œç„¶åé‡æ’åºï¼Œä¿ç•™æœ€ç›¸å…³çš„3ä¸ª
```

---

## 4.8 æœ¬ç« å°ç»“

### 4.8.1 æ ¸å¿ƒæ¦‚å¿µ

âœ… **RAGæµç¨‹**ï¼š
1. æ–‡æ¡£åŠ è½½å’Œåˆ†å‰²
2. å‘é‡åŒ–
3. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
4. æ£€ç´¢ç›¸å…³å†…å®¹
5. ç»“åˆé—®é¢˜ç”Ÿæˆå›ç­”

âœ… **å…³é”®ç»„ä»¶**ï¼š
- Document Loadersï¼šåŠ è½½å„ç§æ–‡æ¡£
- Text Splittersï¼šæ™ºèƒ½åˆ†å‰²
- Embeddingsï¼šæ–‡æœ¬å‘é‡åŒ–
- Vector Storesï¼šå‘é‡å­˜å‚¨å’Œæ£€ç´¢

âœ… **ä¼˜åŒ–æŠ€å·§**ï¼š
- MMRï¼šæé«˜å¤šæ ·æ€§
- å¤šæŸ¥è¯¢æ£€ç´¢ï¼šæé«˜å¬å›ç‡
- æ··åˆæœç´¢ï¼šç»“åˆå…³é”®è¯å’Œè¯­ä¹‰
- é‡æ’åºï¼šæå‡ç²¾åº¦

---

## 4.9 ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šæ„å»ºæ–‡æ¡£é—®ç­”ç³»ç»Ÿ

åŸºäºä½ çš„é¡¹ç›®æ–‡æ¡£ï¼Œæ„å»ºä¸€ä¸ªRAGé—®ç­”ç³»ç»Ÿã€‚

### ç»ƒä¹ 2ï¼šä¼˜åŒ–æ£€ç´¢æ•ˆæœ

å®ç°å¤šæŸ¥è¯¢æ£€ç´¢ï¼Œæ¯”è¾ƒæ£€ç´¢æ•ˆæœã€‚

### ç»ƒä¹ 3ï¼šæ·»åŠ æ¥æºè¿½è¸ª

è®©å›ç­”æ˜¾ç¤ºå¼•ç”¨çš„æ–‡æ¡£å’Œé¡µç ã€‚

---

**ä¸‹ä¸€ç« ï¼š[AI Agentæ™ºèƒ½ä½“ â†’](chapter-05)**
