# RAGæ£€ç´¢å¢å¼º

## æœ¬ç« å¯¼è¯»

**RAGï¼ˆRetrieval-Augmented Generationï¼Œæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰** æ˜¯ç›®å‰æœ€çƒ­é—¨çš„AIåº”ç”¨æŠ€æœ¯ä¹‹ä¸€ã€‚å®ƒç»“åˆäº†ä¿¡æ¯æ£€ç´¢å’Œæ–‡æœ¬ç”Ÿæˆï¼Œè®©AIèƒ½å¤ŸåŸºäºä½ çš„ç§æœ‰æ•°æ®å›ç­”é—®é¢˜ï¼Œè§£å†³LLMçŸ¥è¯†è¿‡æ—¶å’Œå¹»è§‰é—®é¢˜ã€‚

**å­¦ä¹ ç›®æ ‡**ï¼š
- ç†è§£RAGçš„åŸç†å’Œåº”ç”¨åœºæ™¯
- æŒæ¡æ–‡æ¡£åŠ è½½å’Œåˆ†å‰²æŠ€æœ¯
- å­¦ä¹ Embeddingså’Œå‘é‡æ•°æ®åº“
- æ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿ

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š70åˆ†é’Ÿ

---

## ä»€ä¹ˆæ˜¯RAGï¼Ÿ

### RAGçš„å®šä¹‰

**RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰** æ˜¯ä¸€ç§æŠ€æœ¯æ¡†æ¶ï¼Œé€šè¿‡å…ˆä»çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼Œç„¶åå°†æ£€ç´¢åˆ°çš„å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡æä¾›ç»™LLMï¼Œä»è€Œç”Ÿæˆæ›´å‡†ç¡®çš„å›ç­”ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              RAG å·¥ä½œæµç¨‹                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  ç”¨æˆ·é—®é¢˜                                       â”‚
â”‚    â†“                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ 1. æ£€ç´¢é˜¶æ®µ  â”‚ â†’ ä»çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ç›¸å…³æ–‡æ¡£       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚    â†“                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ 2. å¢å¼ºé˜¶æ®µ  â”‚ â†’ å°†æ£€ç´¢å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚    â†“                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚
â”‚  â”‚ 3. ç”Ÿæˆé˜¶æ®µ  â”‚ â†’ LLMåŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå›ç­”        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚
â”‚    â†“                                            â”‚
â”‚  å‡†ç¡®çš„å›ç­” + å¼•ç”¨æ¥æº                           â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸ºä»€ä¹ˆéœ€è¦RAGï¼Ÿ

| é—®é¢˜ | çº¯LLMçš„å±€é™ | RAGçš„è§£å†³æ–¹æ¡ˆ |
|------|-------------|--------------|
| **çŸ¥è¯†è¿‡æ—¶** | è®­ç»ƒæˆªæ­¢æ—¥æœŸåçš„äº‹ä»¶ä¸çŸ¥é“ | å®æ—¶æ›´æ–°æ–‡æ¡£åº“ |
| **å¹»è§‰é—®é¢˜** | å¯èƒ½ç¼–é€ é”™è¯¯ä¿¡æ¯ | åŸºäºçœŸå®æ–‡æ¡£å›ç­” |
| **ç§æœ‰æ•°æ®** | æ— æ³•è®¿é—®ä¼ä¸šå†…éƒ¨æ•°æ® | é›†æˆç§æœ‰çŸ¥è¯†åº“ |
| **å¯è¿½æº¯æ€§** | æ— æ³•éªŒè¯ç­”æ¡ˆæ¥æº | æä¾›å¼•ç”¨æ¥æº |
| **ä¸“ä¸šæ€§** | é€šç”¨çŸ¥è¯†ï¼Œä¸å¤Ÿä¸“ä¸š | é¢†åŸŸä¸“ç”¨çŸ¥è¯†åº“ |

### RAG vs å¾®è°ƒï¼ˆFine-tuningï¼‰

```
RAG vs Fine-tuning å¯¹æ¯”ï¼š

RAGï¼š
  âœ… å¿«é€Ÿå®æ–½ï¼ˆå‡ å°æ—¶åˆ°å‡ å¤©ï¼‰
  âœ… æ— éœ€è®­ç»ƒï¼Œæˆæœ¬ä½
  âœ… æ˜“äºæ›´æ–°çŸ¥è¯†
  âœ… å¯è¿½æº¯ç­”æ¡ˆæ¥æº
  âœ… é€‚åˆçŸ¥è¯†å¯†é›†å‹ä»»åŠ¡

Fine-tuningï¼š
  âœ… æ”¹å˜æ¨¡å‹çš„è¯´è¯æ–¹å¼
  âœ… å­¦ä¹ ç‰¹å®šæ ¼å¼
  âœ… è®­ç»ƒæˆæœ¬é«˜
  âŒ çŸ¥è¯†æ›´æ–°å›°éš¾
  âŒ éœ€è¦å¤§é‡è®­ç»ƒæ•°æ®

ğŸ’¡ æœ€ä½³å®è·µï¼šRAG + Fine-tuning ç»“åˆä½¿ç”¨
```

### RAGçš„åº”ç”¨åœºæ™¯

**1. ä¼ä¸šçŸ¥è¯†åº“é—®ç­”**
```python
# ç¤ºä¾‹ï¼šå†…éƒ¨æ–‡æ¡£é—®ç­”
Q: "å…¬å¸çš„æŠ¥é”€æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ"
A: "æ ¹æ®ã€Šå‘˜å·¥æ‰‹å†Œã€‹ç¬¬5ç« ï¼š
    1. å¡«å†™æŠ¥é”€å•
    2. éƒ¨é—¨ç»ç†å®¡æ‰¹
    3. è´¢åŠ¡å®¡æ ¸
    4. 3-5ä¸ªå·¥ä½œæ—¥åˆ°è´¦
    æ¥æºï¼šå‘˜å·¥æ‰‹å†Œ_v3.0.pdf, ç¬¬23é¡µ"
```

**2. æ™ºèƒ½å®¢æœç³»ç»Ÿ**
```python
# ç¤ºä¾‹ï¼šäº§å“æ”¯æŒ
Q: "å¦‚ä½•é‡ç½®è·¯ç”±å™¨ï¼Ÿ"
A: "æ ¹æ®äº§å“æ–‡æ¡£ï¼š
    æ­¥éª¤1ï¼šé•¿æŒ‰Reseté”®10ç§’
    æ­¥éª¤2ï¼šç­‰å¾…æŒ‡ç¤ºç¯é—ªçƒ
    æ­¥éª¤3ï¼šé‡æ–°é…ç½®ç½‘ç»œ
    å¦‚æœ‰é—®é¢˜è¯·è”ç³»å®¢æœ..."
```

**3. æ³•å¾‹æ–‡æ¡£åˆ†æ**
```python
# ç¤ºä¾‹ï¼šåˆåŒå®¡æŸ¥
Q: "è¿™ä»½åˆåŒæœ‰å“ªäº›é£é™©ç‚¹ï¼Ÿ"
A: "æ ¹æ®åˆåŒå®¡æŸ¥æŒ‡å—ï¼š
    å‘ç°3ä¸ªæ½œåœ¨é£é™©ï¼š
    1. ç¬¬5æ¡ï¼šè¿çº¦è´£ä»»ä¸æ˜ç¡®
    2. ç¬¬12æ¡ï¼šç®¡è¾–æ¡æ¬¾å¯èƒ½ä¸åˆ©
    3. ..."
```

**4. æŠ€æœ¯æ–‡æ¡£åŠ©æ‰‹**
```python
# ç¤ºä¾‹ï¼šä»£ç æ–‡æ¡£é—®ç­”
Q: "å¦‚ä½•åœ¨Djangoä¸­å®ç°JWTè®¤è¯ï¼Ÿ"
A: "æ ¹æ®é¡¹ç›®æ–‡æ¡£ï¼š
    ä½¿ç”¨django-rest-framework-simplejwtï¼š
    [ä»£ç ç¤ºä¾‹]
    ç›¸å…³æ–‡æ¡£ï¼šapi_auth.md"
```

---

## RAGç³»ç»Ÿæ¶æ„

### å®Œæ•´çš„RAGæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            RAGç³»ç»Ÿå®Œæ•´æ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                    â”‚
â”‚  å‡†å¤‡é˜¶æ®µï¼ˆä¸€æ¬¡æ€§ï¼‰                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ æ–‡æ¡£åŠ è½½ â”‚ â†’   â”‚ æ–‡æœ¬åˆ†å‰² â”‚ â†’  â”‚ å‘é‡åŒ–   â”‚    â”‚
â”‚  â”‚ Loader  â”‚      â”‚Splitter â”‚    â”‚Embeddingsâ”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                            â†“                      â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚                    â”‚ å‘é‡æ•°æ®åº“   â”‚                â”‚
â”‚                    â”‚Vector Store â”‚                â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                    â”‚
â”‚  è¿è¡Œé˜¶æ®µï¼ˆæ¯æ¬¡æŸ¥è¯¢ï¼‰                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ ç”¨æˆ·é—®é¢˜ â”‚ â†’  â”‚ é—®é¢˜å‘é‡åŒ–â”‚ â†’  â”‚ ç›¸ä¼¼åº¦æ£€ç´¢â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                   â†“                â”‚
â”‚                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚                          â”‚ Top-Kæ–‡æ¡£   â”‚           â”‚
â”‚                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                   â†“                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚          Prompt + æ£€ç´¢å†…å®¹ + é—®é¢˜         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                    â†“                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                    â”‚
â”‚  â”‚ LLMç”Ÿæˆ  â”‚ â†’ å›ç­” + å¼•ç”¨                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                    â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒç»„ä»¶è¯´æ˜

```python
# 1. Document Loaders - æ–‡æ¡£åŠ è½½å™¨
from langchain.document_loaders import (
    PyPDFLoader,           # PDFæ–‡æ¡£
    TextLoader,            # æ–‡æœ¬æ–‡æ¡£
    DirectoryLoader,       # ç›®å½•åŠ è½½
    CSVLoader,             # CSVæ–‡ä»¶
    UnstructuredMarkdownLoader,  # Markdown
    WebBaseLoader          # ç½‘é¡µ
)

# 2. Text Splitters - æ–‡æœ¬åˆ†å‰²å™¨
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,  # é€’å½’åˆ†å‰²
    CharacterTextSplitter,           # å­—ç¬¦åˆ†å‰²
    TokenTextSplitter,               # Tokenåˆ†å‰²
    MarkdownTextSplitter             # Markdownåˆ†å‰²
)

# 3. Embeddings - å‘é‡åŒ–
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import (
    HuggingFaceEmbeddings,  # å¼€æºæ¨¡å‹
    CohereEmbeddings,
    BedrockEmbeddings
)

# 4. Vector Stores - å‘é‡æ•°æ®åº“
from langchain.vectorstores import (
    Chroma,           # è½»é‡çº§ï¼Œæ¨è
    FAISS,            # Facebookå‡ºå“
    Pinecone,         # äº‘æœåŠ¡
    Weaviate          # å¼€æº
)
```

---

## æ–‡æ¡£åŠ è½½ï¼ˆDocument Loadingï¼‰

### æ”¯æŒçš„æ–‡æ¡£ç±»å‹

LangChainæ”¯æŒ100+ç§æ–‡æ¡£æ ¼å¼ï¼

| æ ¼å¼ | Loader | è¯´æ˜ |
|------|--------|------|
| PDF | `PyPDFLoader` | æœ€å¸¸ç”¨ |
| Word | `Docx2txtLoader` | Wordæ–‡æ¡£ |
| TXT | `TextLoader` | çº¯æ–‡æœ¬ |
| Markdown | `UnstructuredMarkdownLoader` | MDæ–‡ä»¶ |
| HTML | `Bs4HTMLLoader` | ç½‘é¡µ |
| CSV | `CSVLoader` | è¡¨æ ¼æ•°æ® |
| JSON | `JSONLoader` | JSONæ•°æ® |
| ä»£ç  | `PythonLoader` | æºä»£ç  |

### åŠ è½½å•ä¸ªæ–‡æ¡£

```python
from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import TextLoader

# åŠ è½½PDF
pdf_loader = PyPDFLoader("docs/manual.pdf")
pdf_docs = pdf_loader.load()

print(f"åŠ è½½äº† {len(pdf_docs)} é¡µ")
print(f"ç¬¬ä¸€é¡µå†…å®¹ï¼š\n{pdf_docs[0].page_content[:500]}")
print(f"å…ƒæ•°æ®ï¼š{pdf_docs[0].metadata}")
# {'source': 'docs/manual.pdf', 'page': 0}

# åŠ è½½TXT
txt_loader = TextLoader("docs/guide.txt", encoding='utf-8')
txt_docs = txt_loader.load()
```

### åŠ è½½ç›®å½•

```python
from langchain.document_loaders import DirectoryLoader

# åŠ è½½ç›®å½•ä¸‹æ‰€æœ‰PDF
loader = DirectoryLoader(
    'docs/',                    # ç›®å½•è·¯å¾„
    glob="**/*.pdf",            # æ–‡ä»¶æ¨¡å¼
    loader_cls=PyPDFLoader,     # ä½¿ç”¨çš„åŠ è½½å™¨
    show_progress=True,         # æ˜¾ç¤ºè¿›åº¦
    use_multithreading=True     # å¤šçº¿ç¨‹åŠ è½½
)

docs = loader.load()
print(f"æ€»å…±åŠ è½½äº† {len(docs)} ä¸ªæ–‡æ¡£å—")
```

### åŠ è½½ç½‘é¡µ

```python
from langchain.document_loaders import WebBaseLoader
from bs4 import SoupStrainer

# åªæå–ç‰¹å®šå†…å®¹
bs4_strainer = SoupStrainer(class_=("post-title", "post-header", "post-content"))

loader = WebBaseLoader(
    "https://python.langchain.com/docs/get_started/introduction",
    bs_kwargs={"parse_only": bs4_strainer}
)

docs = loader.load()
print(docs[0].page_content[:500])
```

### è‡ªå®šä¹‰Loader

```python
from langchain.document_loaders.base import BaseLoader
from typing import List
from langchain.schema import Document

class MyCustomLoader(BaseLoader):
    """è‡ªå®šä¹‰æ–‡æ¡£åŠ è½½å™¨"""

    def __init__(self, source: str):
        self.source = source

    def load(self) -> List[Document]:
        """åŠ è½½æ–‡æ¡£"""
        with open(self.source, 'r', encoding='utf-8') as f:
            content = f.read()

        # è‡ªå®šä¹‰å¤„ç†é€»è¾‘
        metadata = {"source": self.source}

        return [Document(page_content=content, metadata=metadata)]

# ä½¿ç”¨
loader = MyCustomLoader("data/custom.txt")
docs = loader.load()
```

---

## æ–‡æœ¬åˆ†å‰²ï¼ˆText Splittingï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦åˆ†å‰²ï¼Ÿ

```
é—®é¢˜ï¼šLLMæœ‰ä¸Šä¸‹æ–‡é•¿åº¦é™åˆ¶

ç¤ºä¾‹ï¼šGPT-3.5-turbo
  - æœ€å¤§è¾“å…¥ï¼š4096 tokens
  - 1 token â‰ˆ 0.75 ä¸ªè‹±æ–‡å•è¯
  - 1 token â‰ˆ 1.5-2 ä¸ªæ±‰å­—

å¦‚æœä¸åˆ†å‰²ï¼š
  âŒ æ•´æœ¬ä¹¦æ— æ³•ä¸€æ¬¡æ€§è¾“å…¥
  âŒ æ£€ç´¢ä¸ç²¾å‡†ï¼ˆæ‰¾åˆ°å†—ä½™å†…å®¹ï¼‰
  âŒ æˆæœ¬é«˜ï¼ˆå¤„ç†å¤§é‡æ— å…³å†…å®¹ï¼‰

åˆ†å‰²åï¼š
  âœ… æ¯ä¸ªå—éƒ½æ˜¯ç‹¬ç«‹è¯­ä¹‰å•å…ƒ
  âœ… æ£€ç´¢æ›´ç²¾å‡†
  âœ… é™ä½Tokenæ¶ˆè€—
```

### åˆ†å‰²ç­–ç•¥

```python
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    CharacterTextSplitter,
    TokenTextSplitter
)

# 1. RecursiveCharacterTextSplitterï¼ˆæ¨èï¼‰
# é€’å½’åœ°æŒ‰ä¸åŒåˆ†éš”ç¬¦åˆ†å‰²
recursive_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,              # æ¯å—æœ€å¤§å­—ç¬¦æ•°
    chunk_overlap=200,            # å—ä¹‹é—´é‡å å­—ç¬¦æ•°
    length_function=len,          # é•¿åº¦è®¡ç®—å‡½æ•°
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
)

# åˆ†å‰²é€»è¾‘ï¼š
# 1. å…ˆæŒ‰æ®µè½åˆ†éš”ï¼ˆ\n\nï¼‰
# 2. å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼ŒæŒ‰å¥å­åˆ†éš”
# 3. å¦‚æœè¿˜æ˜¯å¤ªå¤§ï¼ŒæŒ‰è¯åˆ†éš”
# 4. æœ€åæŒ‰å­—ç¬¦åˆ†å‰²

splits = recursive_splitter.split_documents(docs)
print(f"åˆ†å‰²æˆ {len(splits)} ä¸ªå—")

# 2. CharacterTextSplitter
# æŒ‰å›ºå®šå­—ç¬¦æ•°åˆ†å‰²
character_splitter = CharacterTextSplitter(
    separator="\n\n",      # åˆ†éš”ç¬¦
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len
)

# 3. TokenTextSplitter
# æŒ‰Tokenæ•°é‡åˆ†å‰²ï¼ˆæ›´ç²¾å‡†ï¼‰
from langchain.text_splitter import TokenTextSplitter

token_splitter = TokenTextSplitter(
    chunk_size=500,        # 500 tokens
    chunk_overlap=50,
    encoding_name="cl100k_base"  # GPT-3.5/4çš„ç¼–ç 
)

splits = token_splitter.split_documents(docs)
```

### ä¸åŒæ–‡æ¡£ç±»å‹çš„åˆ†å‰²

```python
# Markdownæ–‡æ¡£
from langchain.text_splitter import MarkdownTextSplitter

markdown_splitter = MarkdownTextSplitter(
    chunk_size=1000,
    chunk_overlap=0
)

# æŒ‰Markdownç»“æ„åˆ†å‰²ï¼ˆ#ã€##ç­‰ï¼‰
splits = markdown_splitter.split_text(markdown_content)

# ä»£ç æ–‡æ¡£
from langchain.text_splitter import (
    RecursiveCharacterTextSplitter,
    Language
)

python_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=1000,
    chunk_overlap=100
)

# æŒ‰å‡½æ•°ã€ç±»ç­‰ä»£ç ç»“æ„åˆ†å‰²
code_splits = python_splitter.split_documents(code_docs)

# JavaScript
js_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.JS
)
```

### åˆ†å‰²æ•ˆæœè¯„ä¼°

```python
def evaluate_splits(splits):
    """è¯„ä¼°åˆ†å‰²æ•ˆæœ"""
    for i, split in enumerate(splits[:5]):  # æŸ¥çœ‹å‰5ä¸ª
        print(f"\n--- å— {i+1} ---")
        print(f"é•¿åº¦ï¼š{len(split.page_content)} å­—ç¬¦")
        print(f"æ¥æºï¼š{split.metadata.get('source', 'Unknown')}")
        print(f"å†…å®¹é¢„è§ˆï¼š{split.page_content[:200]}...")

    # ç»Ÿè®¡
    lengths = [len(s.page_content) for s in splits]
    print(f"\nç»Ÿè®¡ï¼š")
    print(f"æ€»å—æ•°ï¼š{len(splits)}")
    print(f"å¹³å‡é•¿åº¦ï¼š{sum(lengths)/len(lengths):.0f}")
    print(f"æœ€å°é•¿åº¦ï¼š{min(lengths)}")
    print(f"æœ€å¤§é•¿åº¦ï¼š{max(lengths)}")

# ä½¿ç”¨
evaluate_splits(splits)
```

---

## Embeddingså’Œå‘é‡æ•°æ®åº“

### ä»€ä¹ˆæ˜¯Embeddingsï¼Ÿ

**Embeddingsï¼ˆåµŒå…¥ï¼‰** æ˜¯å°†æ–‡æœ¬è½¬æ¢ä¸ºé«˜ç»´å‘é‡ï¼ˆæ•°å­—æ•°ç»„ï¼‰çš„æŠ€æœ¯ï¼Œç›¸ä¼¼çš„æ–‡æœ¬ä¼šæœ‰ç›¸ä¼¼çš„å‘é‡ã€‚

```python
# ç¤ºä¾‹ï¼šæ–‡æœ¬å‘é‡åŒ–
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(openai_api_key="your-key")

# è½¬æ¢ä¸ºå‘é‡
text = "Pythonæ˜¯ä¸€ç§ç¼–ç¨‹è¯­è¨€"
vector = embeddings.embed_query(text)

print(f"å‘é‡ç»´åº¦ï¼š{len(vector)}")  # 1536 (OpenAI embeddings)
print(f"å‰10ä¸ªå€¼ï¼š{vector[:10]}")
# [0.0023, -0.0156, 0.0089, ...]
```

**è¯­ä¹‰ç›¸ä¼¼åº¦ç¤ºä¾‹**ï¼š

```python
# è®¡ç®—ç›¸ä¼¼åº¦
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

texts = [
    "Pythonæ˜¯ç¼–ç¨‹è¯­è¨€",
    "Javaæ˜¯ç¼–ç¨‹è¯­è¨€",
    "ä»Šå¤©å¤©æ°”ä¸é”™",
    "æœºå™¨å­¦ä¹ æ˜¯AIçš„åˆ†æ”¯"
]

vectors = [embeddings.embed_query(t) for t in texts]

# è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
similarities = cosine_similarity(vectors)

# ç»“æœï¼šPythonå’ŒJavaç›¸ä¼¼åº¦é«˜ï¼Œä¸å¤©æ°”ç›¸ä¼¼åº¦ä½
# [[1.00, 0.89, 0.23, 0.45],
#  [0.89, 1.00, 0.21, 0.43],
#  [0.23, 0.21, 1.00, 0.19],
#  [0.45, 0.43, 0.19, 1.00]]
```

### ä¸»æµEmbeddingæ¨¡å‹

| æ¨¡å‹ | ç»´åº¦ | ç‰¹ç‚¹ | ä»·æ ¼ |
|------|------|------|------|
| **OpenAI text-embedding-3** | 1536 | ç»¼åˆæ€§èƒ½æœ€ä½³ | $0.0001/1K tokens |
| **Cohere embed-v3** | 1024 | å¤šè¯­è¨€æ”¯æŒå¥½ | å…è´¹/ä»˜è´¹ |
| **HuggingFace mteb** | 768 | å¼€æºå…è´¹ | å®Œå…¨å…è´¹ |
| **BGE-large-zh** | 1024 | ä¸­æ–‡ä¼˜åŒ– | å…è´¹ |

```python
# ä½¿ç”¨OpenAI Embeddings
from langchain_openai import OpenAIEmbeddings

embeddings = OpenAIEmbeddings(
    model="text-embedding-3-small",  # æœ€ç»æµ
    openai_api_key="your-key"
)

# ä½¿ç”¨å¼€æºæ¨¡å‹ï¼ˆå…è´¹ï¼‰
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5",  # ä¸­æ–‡æ¨¡å‹
    model_kwargs={'device': 'cpu'},  # æˆ– 'cuda' å¦‚æœæœ‰GPU
    encode_kwargs={'normalize_embeddings': True}
)

# ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="shibing624/text2vec-base-chinese"
)
```

### å‘é‡æ•°æ®åº“é€‰æ‹©

```
å‘é‡æ•°æ®åº“å¯¹æ¯”ï¼š

Chroma:
  âœ… è½»é‡çº§ï¼Œæ˜“éƒ¨ç½²
  âœ… æ— éœ€é¢å¤–æœåŠ¡
  âœ… é€‚åˆä¸­å°è§„æ¨¡æ•°æ®
  âœ… å®Œå…¨å…è´¹
  ğŸ’¡ æ¨èç”¨äºå­¦ä¹ å’ŒåŸå‹

FAISS:
  âœ… Facebookå‡ºå“ï¼Œæ€§èƒ½ä¼˜ç§€
  âœ… æ”¯æŒGPUåŠ é€Ÿ
  âœ… å†…å­˜ç´¢å¼•ï¼Œé€Ÿåº¦å¿«
  âŒ ä¸æ”¯æŒæŒä¹…åŒ–ï¼ˆéœ€æ‰‹åŠ¨ä¿å­˜ï¼‰
  ğŸ’¡ æ¨èç”¨äºå¤§è§„æ¨¡æ•°æ®

Pinecone:
  âœ… äº‘æœåŠ¡ï¼Œå®Œå…¨æ‰˜ç®¡
  âœ… è‡ªåŠ¨æ‰©å±•
  âœ… ä¼ä¸šçº§æ”¯æŒ
  âŒ éœ€è¦ä»˜è´¹
  ğŸ’¡ æ¨èç”¨äºç”Ÿäº§ç¯å¢ƒ

Weaviate:
  âœ… åŠŸèƒ½ä¸°å¯Œ
  âœ… æ”¯æŒå¤šç§æ•°æ®ç±»å‹
  âŒ é…ç½®ç›¸å¯¹å¤æ‚
  ğŸ’¡ æ¨èç”¨äºå¤æ‚åœºæ™¯
```

### ä½¿ç”¨Chromaå‘é‡æ•°æ®åº“

```python
from langchain.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 1. å‡†å¤‡æ–‡æ¡£
loader = PyPDFLoader("docs/manual.pdf")
docs = loader.load()

# 2. åˆ†å‰²
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
splits = text_splitter.split_documents(docs)

# 3. åˆ›å»ºå‘é‡æ•°æ®åº“
embeddings = OpenAIEmbeddings()

# æ–¹å¼1ï¼šåˆ›å»ºä¸´æ—¶æ•°æ®åº“ï¼ˆå†…å­˜ï¼‰
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)

# æ–¹å¼2ï¼šæŒä¹…åŒ–åˆ°ç£ç›˜
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./chroma_db"  # ä¿å­˜è·¯å¾„
)

# æ–¹å¼3ï¼šåŠ è½½å·²å­˜åœ¨çš„æ•°æ®åº“
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# 4. ç›¸ä¼¼åº¦æœç´¢
query = "å¦‚ä½•å®‰è£…Pythonï¼Ÿ"
results = vectorstore.similarity_search(query, k=3)  # è¿”å›Top 3

for i, result in enumerate(results):
    print(f"\n--- ç»“æœ {i+1} ---")
    print(f"å†…å®¹ï¼š{result.page_content[:200]}...")
    print(f"å…ƒæ•°æ®ï¼š{result.metadata}")

# 5. å¸¦åˆ†æ•°çš„æœç´¢
results_with_scores = vectorstore.similarity_search_with_score(query, k=3)

for doc, score in results_with_scores:
    print(f"\nç›¸ä¼¼åº¦åˆ†æ•°ï¼š{score:.4f}")
    print(f"å†…å®¹ï¼š{doc.page_content[:200]}...")
```

### ä½¿ç”¨FAISS

```python
from langchain.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# åˆ›å»ºFAISSç´¢å¼•
embeddings = OpenAIEmbeddings()
faiss_index = FAISS.from_documents(splits, embeddings)

# ä¿å­˜åˆ°ç£ç›˜
faiss_index.save_local("faiss_index")

# åŠ è½½
faiss_index = FAISS.load_local(
    "faiss_index",
    embeddings,
    allow_dangerous_deserialization=True
)

# æœç´¢
results = faiss_index.similarity_search(query, k=3)

# æœ€å¤§è¾¹é™…ç›¸å…³æ€§æœç´¢ï¼ˆMMRï¼‰
# å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
results = faiss_index.max_marginal_relevance_search(
    query,
    k=3,
    fetch_k=10  # ä»å‰10ä¸ªä¸­é€‰æ‹©3ä¸ª
)
```

---

## æ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿ

### åŸºç¡€RAGé“¾

```python
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# 1. åˆå§‹åŒ–ç»„ä»¶
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
embeddings = OpenAIEmbeddings()

# 2. åŠ è½½å‘é‡æ•°æ®åº“
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # è¿”å›Top 3
)

# 3. åˆ›å»ºæç¤ºè¯æ¨¡æ¿
template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œæ˜ç¡®å‘ŠçŸ¥
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´
4. æ ‡æ³¨ä¿¡æ¯æ¥æº

å›ç­”ï¼š"""

prompt = ChatPromptTemplate.from_template(template)

# 4. åˆ›å»ºRAGé“¾
def format_docs(docs):
    """æ ¼å¼åŒ–æ–‡æ¡£"""
    return "\n\n".join([
        f"ã€æ–‡æ¡£{i+1}ã€‘{doc.page_content}"
        for i, doc in enumerate(docs)
    ])

rag_chain = (
    {
        "context": retriever | format_docs,  # æ£€ç´¢å¹¶æ ¼å¼åŒ–
        "question": RunnablePassthrough()    # ä¼ é€’é—®é¢˜
    }
    | prompt
    | llm
    | StrOutputParser()
)

# 5. ä½¿ç”¨
query = "Pythonä¸­çš„è£…é¥°å™¨æ˜¯ä»€ä¹ˆï¼Ÿ"
answer = rag_chain.invoke(query)
print(answer)
```

### å¸¦æ¥æºå¼•ç”¨çš„RAG

```python
from langchain_core.prompts import PromptTemplate

# æç¤ºè¯æ¨¡æ¿
template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼å›ç­”ï¼š
## å›ç­”
[ä½ çš„å›ç­”]

## ä¿¡æ¯æ¥æº
- æ–‡æ¡£1ï¼š[æ¥æºä¿¡æ¯]
- æ–‡æ¡£2ï¼š[æ¥æºä¿¡æ¯]
"""

prompt = PromptTemplate.from_template(template)

# ä¿®æ”¹format_docsä¿ç•™å…ƒæ•°æ®
def format_docs_with_source(docs):
    """æ ¼å¼åŒ–æ–‡æ¡£å¹¶ä¿ç•™æ¥æº"""
    return "\n\n".join([
        f"æ¥æºï¼š{doc.metadata.get('source', 'Unknown')}"
        f"ï¼ˆç¬¬{doc.metadata.get('page', 0)}é¡µï¼‰\n"
        f"å†…å®¹ï¼š{doc.page_content}"
        for doc in docs
    ])

# åˆ›å»ºé“¾
rag_chain = (
    {
        "context": retriever | format_docs_with_source,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)
```

### æµå¼è¾“å‡ºRAG

```python
# æµå¼è¾“å‡º
for chunk in rag_chain.stream("è§£é‡ŠPythonçš„GIL"):
    print(chunk, end="", flush=True)
```

### å®Œæ•´çš„RAGåº”ç”¨

```python
class RAGSystem:
    """å®Œæ•´çš„RAGç³»ç»Ÿ"""

    def __init__(self, db_path="./chroma_db"):
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            persist_directory=db_path,
            embedding_function=self.embeddings
        )
        self.retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 3}
        )

        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0
        )

        self.chain = self._create_chain()

    def _create_chain(self):
        """åˆ›å»ºRAGé“¾"""
        template = """ä½ æ˜¯ä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºä¸Šä¸‹æ–‡å‡†ç¡®å›ç­”
2. å¦‚æ— ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
3. å¼•ç”¨ä¿¡æ¯æ¥æº

å›ç­”ï¼š"""

        prompt = ChatPromptTemplate.from_template(template)

        def format_docs(docs):
            return "\n\n".join([
                f"ã€æ¥æºï¼š{doc.metadata.get('source', 'Unknown')}ã€‘\n{doc.page_content}"
                for doc in docs
            ])

        return (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

    def ask(self, question: str) -> dict:
        """æé—®"""
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.vectorstore.similarity_search(question, k=3)

        # 2. ç”Ÿæˆå›ç­”
        answer = self.chain.invoke(question)

        # 3. è¿”å›ç»“æœå’Œæ¥æº
        return {
            "question": question,
            "answer": answer,
            "sources": [
                {
                    "source": doc.metadata.get('source'),
                    "page": doc.metadata.get('page'),
                    "content": doc.page_content[:200] + "..."
                }
                for doc in docs
            ]
        }

    def chat(self):
        """äº¤äº’å¼å¯¹è¯"""
        print("ğŸ¤– RAGæ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼ˆè¾“å…¥'quit'é€€å‡ºï¼‰\n")

        while True:
            question = input("\nä½ ï¼š").strip()

            if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("å†è§ï¼ğŸ‘‹")
                break

            if not question:
                continue

            try:
                result = self.ask(question)

                print(f"\nğŸ“ å›ç­”ï¼š\n{result['answer']}\n")

                print("ğŸ“š ä¿¡æ¯æ¥æºï¼š")
                for i, source in enumerate(result['sources'], 1):
                    print(f"\n{i}. {source['source']}")
                    print(f"   å†…å®¹ï¼š{source['content']}")

            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼š{e}")

# ä½¿ç”¨
if __name__ == "__main__":
    rag = RAGSystem()
    rag.chat()
```

---

## RAGä¼˜åŒ–æŠ€å·§

### æ£€ç´¢ä¼˜åŒ–

```python
# 1. è°ƒæ•´æ£€ç´¢æ•°é‡
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 5,        # å¢åŠ æ£€ç´¢æ•°é‡
        "score_threshold": 0.7  # ç›¸ä¼¼åº¦é˜ˆå€¼
    }
)

# 2. ä½¿ç”¨MMRï¼ˆæœ€å¤§è¾¹é™…ç›¸å…³æ€§ï¼‰
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={"k": 5, "fetch_k": 20}
)

# 3. å¤šæŸ¥è¯¢æ£€ç´¢
from langchain.retrievers import MultiQueryRetriever

retriever = MultiQueryRetriver.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=ChatOpenAI(temperature=0)
)

# è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“ï¼Œæé«˜å¬å›ç‡

# 4. ä¸Šä¸‹æ–‡å‹ç¼©
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(ChatOpenAI())
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever()
)
```

### æ··åˆæœç´¢

```python
# ç»“åˆå…³é”®è¯æœç´¢å’Œè¯­ä¹‰æœç´¢
from langchain.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever

# BM25å…³é”®è¯æ£€ç´¢
bm25_retriever = BM25Retriever.from_documents(splits)
bm25_retriever.k = 3

# è¯­ä¹‰æ£€ç´¢
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# ç»„åˆæ£€ç´¢
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.4, 0.6]  # æƒé‡
)
```

### é‡æ’åºï¼ˆRerankingï¼‰

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerankCompressor

# ä½¿ç”¨Cohere Rerank APIé‡æ–°æ’åº
compressor = CohereRerankCompressor(
    cohere_api_key="your-key",
    top_n_queries=3  # ä¿ç•™å‰3ä¸ªæœ€ç›¸å…³çš„
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever(search_kwargs={"k": 10})
)
# å…ˆæ£€ç´¢10ä¸ªï¼Œç„¶åé‡æ’åºï¼Œä¿ç•™æœ€ç›¸å…³çš„3ä¸ª
```

---

## æœ¬ç« å°ç»“

### æ ¸å¿ƒæ¦‚å¿µ

âœ… **RAGæµç¨‹**ï¼š
1. æ–‡æ¡£åŠ è½½å’Œåˆ†å‰²
2. å‘é‡åŒ–
3. å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“
4. æ£€ç´¢ç›¸å…³å†…å®¹
5. ç»“åˆé—®é¢˜ç”Ÿæˆå›ç­”

âœ… **å…³é”®ç»„ä»¶**ï¼š
- Document Loadersï¼šåŠ è½½å„ç§æ–‡æ¡£
- Text Splittersï¼šæ™ºèƒ½åˆ†å‰²
- Embeddingsï¼šæ–‡æœ¬å‘é‡åŒ–
- Vector Storesï¼šå‘é‡å­˜å‚¨å’Œæ£€ç´¢

âœ… **ä¼˜åŒ–æŠ€å·§**ï¼š
- MMRï¼šæé«˜å¤šæ ·æ€§
- å¤šæŸ¥è¯¢æ£€ç´¢ï¼šæé«˜å¬å›ç‡
- æ··åˆæœç´¢ï¼šç»“åˆå…³é”®è¯å’Œè¯­ä¹‰
- é‡æ’åºï¼šæå‡ç²¾åº¦

---

## ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šæ„å»ºæ–‡æ¡£é—®ç­”ç³»ç»Ÿ

åŸºäºä½ çš„é¡¹ç›®æ–‡æ¡£ï¼Œæ„å»ºä¸€ä¸ªRAGé—®ç­”ç³»ç»Ÿã€‚

### ç»ƒä¹ 2ï¼šä¼˜åŒ–æ£€ç´¢æ•ˆæœ

å®ç°å¤šæŸ¥è¯¢æ£€ç´¢ï¼Œæ¯”è¾ƒæ£€ç´¢æ•ˆæœã€‚

### ç»ƒä¹ 3ï¼šæ·»åŠ æ¥æºè¿½è¸ª

è®©å›ç­”æ˜¾ç¤ºå¼•ç”¨çš„æ–‡æ¡£å’Œé¡µç ã€‚

---

## å¸¸è§é—®é¢˜ FAQ

### Q1: RAGå’Œä¼ ç»Ÿçš„æœç´¢æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä»€ä¹ˆæ—¶å€™ç”¨RAGï¼Ÿ

**A:**

```python
# ä¼ ç»Ÿæœç´¢ vs RAG å¯¹æ¯”ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç»´åº¦     â”‚   ä¼ ç»Ÿæœç´¢       â”‚      RAG         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ åŒ¹é…æ–¹å¼   â”‚ å…³é”®è¯åŒ¹é…       â”‚ è¯­ä¹‰ç†è§£         â”‚
â”‚ è¾“å‡ºæ–¹å¼   â”‚ è¿”å›æ–‡æ¡£åˆ—è¡¨     â”‚ ç”Ÿæˆè‡ªç„¶è¯­è¨€ç­”æ¡ˆ â”‚
â”‚ ç†è§£èƒ½åŠ›   â”‚ å­—é¢åŒ¹é…         â”‚ æ·±åº¦ç†è§£         â”‚
â”‚ å‡†ç¡®æ€§     â”‚ å¯èƒ½è¯¯åŒ¹é…       â”‚ ä¸Šä¸‹æ–‡ç›¸å…³       â”‚
â”‚ ç”¨æˆ·ä½“éªŒ   â”‚ éœ€è¦ç”¨æˆ·é˜…è¯»     â”‚ ç›´æ¥å¾—åˆ°ç­”æ¡ˆ     â”‚
â”‚ æˆæœ¬       â”‚ ä½               â”‚ é«˜ï¼ˆéœ€è¦LLMï¼‰    â”‚
â”‚ å»¶è¿Ÿ       â”‚ æ¯«ç§’çº§           â”‚ ç§’çº§             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# ä½¿ç”¨åœºæ™¯ï¼š

âœ… ä½¿ç”¨ä¼ ç»Ÿæœç´¢ï¼š
- ç”¨æˆ·éœ€è¦æµè§ˆåŸå§‹æ–‡æ¡£
- ä¿¡æ¯æŸ¥æ‰¾ç±»ä»»åŠ¡ï¼ˆæ‰¾æ–‡æ¡£ã€æ‰¾ä»£ç ï¼‰
- éœ€è¦æå¿«å“åº”
- æˆæœ¬æ•æ„Ÿ
- ç®€å•çš„å…³é”®è¯æŸ¥è¯¢

ç¤ºä¾‹ï¼š
# çŸ¥è¯†åº“æœç´¢
def search_documents(query):
    results = database.search(query)
    return [
        {
            "title": doc.title,
            "snippet": doc.content[:200],
            "relevance": doc.score
        }
        for doc in results
    ]

âœ… ä½¿ç”¨RAGï¼š
- éœ€è¦ç†è§£å’Œç»¼åˆä¿¡æ¯
- éœ€è¦è‡ªç„¶è¯­è¨€ç­”æ¡ˆ
- é—®é¢˜å¤æ‚ã€éœ€è¦æ¨ç†
- é‡è§†ç”¨æˆ·ä½“éªŒ
- æœ‰AIé¢„ç®—

ç¤ºä¾‹ï¼š
# RAGé—®ç­”
def rag_answer(query):
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    docs = vectorstore.similarity_search(query, k=3)
    # 2. ç”Ÿæˆç­”æ¡ˆ
    context = "\n".join([doc.page_content for doc in docs])
    answer = llm.generate(f"åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ï¼š{context}\né—®é¢˜ï¼š{query}")
    return answer

# æ··åˆæ–¹æ¡ˆï¼ˆæ¨èï¼‰ï¼š
def hybrid_system(query):
    # 1. ä¼ ç»Ÿæœç´¢è·å–å€™é€‰
    candidates = keyword_search(query, top_k=20)
    # 2. RAGç”Ÿæˆç­”æ¡ˆ
    answer = rag_generate(query, candidates)
    # 3. åŒæ—¶è¿”å›ç›¸å…³æ–‡æ¡£é“¾æ¥
    return {
        "answer": answer,
        "related_docs": candidates[:5]
    }

# å†³ç­–æµç¨‹ï¼š
éœ€è¦ç­”æ¡ˆè¿˜æ˜¯æ–‡æ¡£ï¼Ÿ
â†’ ç­”æ¡ˆ â†’ ç”¨RAG
â†’ æ–‡æ¡£ â†’ ç”¨ä¼ ç»Ÿæœç´¢
â†’ éƒ½è¦ â†’ ç”¨æ··åˆæ–¹æ¡ˆ
```

### Q2: å¦‚ä½•é€‰æ‹©åˆé€‚çš„chunk_sizeå’Œchunk_overlapï¼Ÿ

**A:**

```python
# Chunk Size å’Œ Overlap çš„é€‰æ‹©ç­–ç•¥ï¼š

âœ… Chunk Sizeï¼ˆå—å¤§å°ï¼‰é€‰æ‹©ï¼š

# å°å—ï¼ˆ300-500å­—ç¬¦ï¼‰
ä¼˜ç‚¹ï¼š
âœ… æ£€ç´¢æ›´ç²¾å‡†ï¼ˆç²’åº¦ç»†ï¼‰
âœ… é€‚åˆå›ç­”å…·ä½“é—®é¢˜
âœ… Tokenæ¶ˆè€—å°‘

ç¼ºç‚¹ï¼š
âŒ å¯èƒ½ä¸¢å¤±ä¸Šä¸‹æ–‡
âŒ éœ€è¦æ£€ç´¢æ›´å¤šå—
âŒ æ‹¼æ¥åå¯èƒ½ä¸è¿è´¯

é€‚ç”¨åœºæ™¯ï¼š
- FAQå‹é—®é¢˜
- ç²¾ç¡®ä¿¡æ¯æŸ¥è¯¢
- ä»£ç æ–‡æ¡£

# ä¸­ç­‰å—ï¼ˆ500-1000å­—ç¬¦ï¼‰- æ¨è
ä¼˜ç‚¹ï¼š
âœ… å¹³è¡¡ç²¾å‡†åº¦å’Œä¸Šä¸‹æ–‡
âœ… é€‚åˆå¤§å¤šæ•°åœºæ™¯
âœ… æ£€ç´¢æ•ˆç‡é«˜

ç¼ºç‚¹ï¼š
âš ï¸ éœ€è¦æ ¹æ®å…·ä½“ä»»åŠ¡è°ƒæ•´

é€‚ç”¨åœºæ™¯ï¼š
- é€šç”¨é—®ç­”
- æŠ€æœ¯æ–‡æ¡£
- ä¼ä¸šçŸ¥è¯†åº“

# å¤§å—ï¼ˆ1000-2000å­—ç¬¦ï¼‰
ä¼˜ç‚¹ï¼š
âœ… ä¿ç•™å®Œæ•´ä¸Šä¸‹æ–‡
âœ… é€‚åˆéœ€è¦ä¸Šä¸‹æ–‡çš„é—®é¢˜
âœ… å‡å°‘æ£€ç´¢æ¬¡æ•°

ç¼ºç‚¹ï¼š
âŒ æ£€ç´¢ä¸å¤Ÿç²¾å‡†
âŒ å¯èƒ½åŒ…å«æ— å…³ä¿¡æ¯
âŒ Tokenæ¶ˆè€—å¤§

é€‚ç”¨åœºæ™¯ï¼š
- éœ€è¦ä¸Šä¸‹æ–‡çš„é—®é¢˜
- æ€»ç»“ç±»ä»»åŠ¡
- é•¿æ–‡æ¡£åˆ†æ

âœ… Chunk Overlapï¼ˆé‡å ï¼‰é€‰æ‹©ï¼š

# æ— é‡å ï¼ˆ0ï¼‰
- å—ä¹‹é—´ç‹¬ç«‹
- å¯èƒ½ä¸¢å¤±è¾¹ç•Œä¿¡æ¯
- ä¸æ¨è

# å°é‡å ï¼ˆ50-100å­—ç¬¦ï¼‰- ä¿å®ˆ
- ä¿ç•™éƒ¨åˆ†ä¸Šä¸‹æ–‡
- å‡å°‘å†—ä½™
- é€‚åˆè¾ƒå°çš„å—

# ä¸­ç­‰é‡å ï¼ˆ100-200å­—ç¬¦ï¼‰- æ¨è
- è‰¯å¥½çš„ä¸Šä¸‹æ–‡è¿ç»­æ€§
- å¹³è¡¡å†—ä½™å’Œä¿¡æ¯å®Œæ•´æ€§
- å¤§å¤šæ•°åœºæ™¯çš„æœ€ä½³é€‰æ‹©

# å¤§é‡å ï¼ˆ200-300å­—ç¬¦ï¼‰
- ç¡®ä¿ä¸ä¼šä¸¢å¤±ä¿¡æ¯
- å¢åŠ æ£€ç´¢å†—ä½™
- é€‚åˆé‡è¦æ–‡æ¡£

# å®è·µå»ºè®®ï¼š

from langchain.text_splitter import RecursiveCharacterTextSplitter

# åœºæ™¯1ï¼šFAQæ–‡æ¡£ï¼ˆå°å—ï¼‰
faq_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50
)

# åœºæ™¯2ï¼šæŠ€æœ¯æ–‡æ¡£ï¼ˆä¸­ç­‰å—ï¼‰
tech_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150
)

# åœºæ™¯3ï¼šæ³•å¾‹æ–‡æ¡£ï¼ˆå¤§å—ï¼Œä¿ç•™å®Œæ•´æ¡æ¬¾ï¼‰
legal_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,
    chunk_overlap=200
)

# åœºæ™¯4ï¼šä»£ç æ–‡æ¡£ï¼ˆæŒ‰å‡½æ•°åˆ†å‰²ï¼‰
code_splitter = RecursiveCharacterTextSplitter.from_language(
    language=Language.PYTHON,
    chunk_size=1000,
    chunk_overlap=100
)

# ä¼˜åŒ–æ–¹æ³•ï¼š
# 1. å®éªŒæµ‹è¯•
def test_chunk_sizes(text, sizes):
    for size in sizes:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=size,
            chunk_overlap=size//5
        )
        chunks = splitter.split_text(text)
        print(f"Size {size}: {len(chunks)} chunks")

# 2. è¯„ä¼°æ£€ç´¢è´¨é‡
# - æŸ¥è¯¢åæ£€æŸ¥è¿”å›çš„å—æ˜¯å¦ç›¸å…³
# - æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å‡†ç¡®
# - è°ƒæ•´å‚æ•°å¯¹æ¯”æ•ˆæœ

# 3. æ ¹æ®LLMä¸Šä¸‹æ–‡çª—å£è°ƒæ•´
# GPT-3.5: 4K tokens â†’ chunk_size 500-1000
# GPT-4: 8K/32K tokens â†’ chunk_size 1000-2000
# Claude-2: 100K tokens â†’ chunk_size 2000-4000
```

### Q3: å¦‚ä½•è¯„ä¼°RAGç³»ç»Ÿçš„æ•ˆæœï¼Ÿ

**A:**

```python
# RAGç³»ç»Ÿè¯„ä¼°æ¡†æ¶ï¼š

âœ… è¯„ä¼°ç»´åº¦ï¼š

# 1. æ£€ç´¢è´¨é‡ï¼ˆRetrieval Qualityï¼‰

def evaluate_retrieval(ground_truth, retrieved_docs):
    """è¯„ä¼°æ£€ç´¢è´¨é‡"""

    # Precision@K: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å æ¯”
    precision = len(relevant_docs) / len(retrieved_docs)

    # Recall@K: æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£å æ‰€æœ‰ç›¸å…³æ–‡æ¡£çš„æ¯”ä¾‹
    recall = len(retrieved_relevant_docs) / len(all_relevant_docs)

    # MRR (Mean Reciprocal Rank): ç¬¬ä¸€ä¸ªç›¸å…³æ–‡æ¡£çš„æ’å
    mrr = 1 / rank_of_first_relevant

    # NDCG (Normalized Discounted Cumulative Gain): è€ƒè™‘æ’åºè´¨é‡
    ndcg = calculate_ndcg(retrieved_docs, relevance_scores)

    return {
        "precision@k": precision,
        "recall@k": recall,
        "mrr": mrr,
        "ndcg": ndcg
    }

# 2. ç”Ÿæˆè´¨é‡ï¼ˆGeneration Qualityï¼‰

def evaluate_generation(generated_answer, ground_truth):
    """è¯„ä¼°ç”Ÿæˆç­”æ¡ˆè´¨é‡"""

    # Faithfulness (å¿ å®åº¦): ç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢å†…å®¹
    faithfulness = check_if_answer_based_on_context(
        generated_answer,
        retrieved_contexts
    )

    # Answer Relevance (ç­”æ¡ˆç›¸å…³æ€§)
    relevance = calculate_similarity(
        generated_answer,
        ground_truth_answer
    )

    # Context Precision (ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦)
    context_precision = check_if_contexts_relevant_to_query(
        retrieved_contexts,
        query
    )

    return {
        "faithfulness": faithfulness,
        "relevance": relevance,
        "context_precision": context_precision
    }

# 3. ç«¯åˆ°ç«¯è¯„ä¼°

def evaluate_rag_end_to_end(queries, ground_truths):
    """ç«¯åˆ°ç«¯è¯„ä¼°"""

    results = []
    for query, ground_truth in zip(queries, ground_truths):
        # æ£€ç´¢
        docs = retriever.get_relevant_documents(query)

        # ç”Ÿæˆ
        answer = rag_chain.invoke(query)

        # è¯„ä¼°
        result = {
            "query": query,
            "retrieved_docs": docs,
            "answer": answer,
            "ground_truth": ground_truth,

            # æ£€ç´¢æŒ‡æ ‡
            "precision": calculate_precision(docs, ground_truth["relevant_docs"]),
            "recall": calculate_recall(docs, ground_truth["all_relevant_docs"]),

            # ç”ŸæˆæŒ‡æ ‡
            "faithfulness": check_faithfulness(answer, docs),
            "relevance": calculate_relevance(answer, ground_truth["answer"])
        }

        results.append(result)

    # æ±‡æ€»ç»Ÿè®¡
    avg_metrics = {
        "avg_precision": np.mean([r["precision"] for r in results]),
        "avg_recall": np.mean([r["recall"] for r in results]),
        "avg_faithfulness": np.mean([r["faithfulness"] for r in results]),
        "avg_relevance": np.mean([r["relevance"] for r in results])
    }

    return results, avg_metrics

# 4. ç”¨æˆ·åé¦ˆè¯„ä¼°

class RAGEvaluatorWithFeedback:
    """å¸¦ç”¨æˆ·åé¦ˆçš„è¯„ä¼°"""

    def log_query(self, query, answer, retrieved_docs):
        """è®°å½•æŸ¥è¯¢"""
        return {
            "query": query,
            "answer": answer,
            "docs": retrieved_docs,
            "timestamp": datetime.now(),
            "user_feedback": None  # å¾…ç”¨æˆ·åé¦ˆ
        }

    def add_feedback(self, query_id, feedback):
        """æ·»åŠ ç”¨æˆ·åé¦ˆ"""
        # feedback: {
        #   "thumbs_up": true/false,
        #   "rating": 1-5,
        #   "comment": "..."
        # }
        pass

    def calculate_user_satisfaction(self):
        """è®¡ç®—ç”¨æˆ·æ»¡æ„åº¦"""
        return {
            "thumbs_up_rate": thumbs_up / total_queries,
            "avg_rating": sum(ratings) / len(ratings)
        }

# 5. A/Bæµ‹è¯•

def ab_test_rag(system_a, system_b, test_queries):
    """A/Bæµ‹è¯•ä¸¤ä¸ªRAGç³»ç»Ÿ"""

    results_a = [system_a.query(q) for q in test_queries]
    results_b = [system_b.query(q) for q in test_queries]

    # äººå·¥è¯„ä¼°æˆ–è‡ªåŠ¨è¯„ä¼°
    comparison = compare_results(results_a, results_b)

    return {
        "system_a_avg_score": np.mean([r["score"] for r in results_a]),
        "system_b_avg_score": np.mean([r["score"] for r in results_b]),
        "winner": "A" if comparison > 0 else "B"
    }

# è¯„ä¼°å·¥å…·æ¨èï¼š
# - RAGAS: è‡ªåŠ¨åŒ–RAGè¯„ä¼°æ¡†æ¶
# - TruLens: RAGå¯è§‚æµ‹æ€§å’Œè¯„ä¼°
# - DeepEval: LLMåº”ç”¨è¯„ä¼°æ¡†æ¶
# - è‡ªå®šä¹‰è¯„ä¼°è„šæœ¬

# è¯„ä¼°æœ€ä½³å®è·µï¼š
âœ… å‡†å¤‡å¤šæ ·åŒ–çš„æµ‹è¯•é›†ï¼ˆè¦†ç›–å„ç§åœºæ™¯ï¼‰
âœ… åŒ…å«ç®€å•å’Œå¤æ‚é—®é¢˜
âœ… å‡†å¤‡ground truthï¼ˆæ ‡å‡†ç­”æ¡ˆï¼‰
âœ… å®šæœŸè¯„ä¼°å’Œè¿­ä»£
âœ… æ”¶é›†çœŸå®ç”¨æˆ·åé¦ˆ
âœ… ç›‘æ§ç”Ÿäº§ç¯å¢ƒè¡¨ç°
```

### Q4: å‘é‡æ•°æ®åº“å¦‚ä½•é€‰æ‹©ï¼ŸChromaã€FAISSã€Pineconeæ€ä¹ˆé€‰ï¼Ÿ

**A:**

```python
# å‘é‡æ•°æ®åº“é€‰å‹æŒ‡å—ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç‰¹æ€§      â”‚  Chroma  â”‚   FAISS  â”‚ Pinecone â”‚ Weaviate â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ éƒ¨ç½²æ–¹å¼    â”‚ æœ¬åœ°     â”‚ æœ¬åœ°     â”‚ äº‘æœåŠ¡   â”‚ æœ¬åœ°/äº‘  â”‚
â”‚ å­¦ä¹ æ›²çº¿    â”‚ ä½       â”‚ ä¸­       â”‚ ä½       â”‚ é«˜       â”‚
â”‚ æ€§èƒ½        â”‚ ä¸­       â”‚ é«˜       â”‚ é«˜       â”‚ é«˜       â”‚
â”‚ æ‰©å±•æ€§      â”‚ ä½       â”‚ ä¸­       â”‚ é«˜       â”‚ é«˜       â”‚
â”‚ æŒä¹…åŒ–      â”‚ âœ…       â”‚ âŒ       â”‚ âœ…       â”‚ âœ…       â”‚
â”‚ æˆæœ¬        â”‚ å…è´¹     â”‚ å…è´¹     â”‚ ä»˜è´¹     â”‚ å…è´¹/ä»˜è´¹â”‚
â”‚ æ¨èåœºæ™¯    â”‚ å­¦ä¹ /åŸå‹â”‚ å¤§è§„æ¨¡   â”‚ ç”Ÿäº§     â”‚ é«˜çº§     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

# è¯¦ç»†å¯¹æ¯”ï¼š

âœ… Chroma
# æœ€ä½³é€‰æ‹©ï¼šå­¦ä¹ ã€åŸå‹å¼€å‘ã€ä¸­å°è§„æ¨¡é¡¹ç›®

from langchain.vectorstores import Chroma

# ä¼˜ç‚¹ï¼š
âœ… å®‰è£…ç®€å•ï¼ˆpip install chromadbï¼‰
âœ… æ— éœ€é¢å¤–æœåŠ¡
âœ… è‡ªåŠ¨æŒä¹…åŒ–åˆ°ç£ç›˜
âœ… APIå‹å¥½
âœ… å®Œå…¨å…è´¹
âœ… æ”¯æŒå…ƒæ•°æ®è¿‡æ»¤

# ç¼ºç‚¹ï¼š
âŒ æ€§èƒ½ä¸å¦‚FAISSï¼ˆå¤§è§„æ¨¡æ•°æ®ï¼‰
âŒ ä¸æ”¯æŒåˆ†å¸ƒå¼
âŒ åŠŸèƒ½ç›¸å¯¹åŸºç¡€

# é€‚ç”¨åœºæ™¯ï¼š
- æ•°æ®é‡ < 100ä¸‡æ–‡æ¡£
- å­¦ä¹ RAGæŠ€æœ¯
- å¿«é€ŸåŸå‹éªŒè¯
- ä¸ªäººé¡¹ç›®
- ä¸­å°ä¼ä¸šåº”ç”¨

# ä½¿ç”¨ç¤ºä¾‹ï¼š
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory="./db"  # æŒä¹…åŒ–
)

âœ… FAISS
# æœ€ä½³é€‰æ‹©ï¼šå¤§è§„æ¨¡æ•°æ®ã€æ€§èƒ½è¦æ±‚é«˜

from langchain.vectorstores import FAISS

# ä¼˜ç‚¹ï¼š
âœ… Facebookå‡ºå“ï¼Œæ€§èƒ½ä¼˜ç§€
âœ… æ”¯æŒGPUåŠ é€Ÿ
âœ… å†…å­˜ç´¢å¼•ï¼Œé€Ÿåº¦æå¿«
âœ… å¤šç§ç´¢å¼•ç®—æ³•ï¼ˆIVFã€PQç­‰ï¼‰
âœ… å®Œå…¨å…è´¹
âœ… æ”¯æŒæ•°åäº¿çº§å‘é‡

# ç¼ºç‚¹ï¼š
âŒ ä¸è‡ªåŠ¨æŒä¹…åŒ–ï¼ˆéœ€æ‰‹åŠ¨ä¿å­˜ï¼‰
âŒ ä¸æ”¯æŒå®æ—¶æ›´æ–°ï¼ˆéœ€é‡å»ºç´¢å¼•ï¼‰
âŒ å…ƒæ•°æ®è¿‡æ»¤åŠŸèƒ½å¼±
âŒ éœ€è¦æ›´å¤šé…ç½®

# é€‚ç”¨åœºæ™¯ï¼š
- æ•°æ®é‡ > 100ä¸‡æ–‡æ¡£
- å¯¹æ£€ç´¢é€Ÿåº¦è¦æ±‚æé«˜
- æ‰¹é‡å¤„ç†ï¼Œä¸éœ€è¦é¢‘ç¹æ›´æ–°
- æœ‰GPUèµ„æº
- å¤§å‹æœç´¢å¼•æ“

# ä½¿ç”¨ç¤ºä¾‹ï¼š
faiss_index = FAISS.from_documents(splits, embeddings)
faiss_index.save_local("faiss_index")  # æ‰‹åŠ¨ä¿å­˜

âœ… Pinecone
# æœ€ä½³é€‰æ‹©ï¼šç”Ÿäº§ç¯å¢ƒã€ä¼ä¸šçº§åº”ç”¨

import pinecone
from langchain.vectorstores import Pinecone

# ä¼˜ç‚¹ï¼š
âœ… å®Œå…¨æ‰˜ç®¡ï¼Œæ— éœ€è¿ç»´
âœ… è‡ªåŠ¨æ‰©å±•
âœ… é«˜å¯ç”¨ï¼ˆ99.9% SLAï¼‰
âœ… å®æ—¶æ›´æ–°
âœ… å¼ºå¤§çš„å…ƒæ•°æ®è¿‡æ»¤
âœ… ä¼ä¸šçº§æ”¯æŒ

# ç¼ºç‚¹ï¼š
âŒ éœ€è¦ä»˜è´¹ï¼ˆæœ‰å…è´¹é¢åº¦ï¼‰
âŒ æ•°æ®åœ¨äº‘ç«¯ï¼ˆæ•°æ®å®‰å…¨ï¼‰
âŒ ä¾èµ–ç½‘ç»œ

# é€‚ç”¨åœºæ™¯ï¼š
- ç”Ÿäº§ç¯å¢ƒ
- ä¼ä¸šçº§åº”ç”¨
- éœ€è¦é«˜å¯ç”¨æ€§
- å›¢é˜Ÿåä½œ
- é¢„ç®—å……è¶³

# ä½¿ç”¨ç¤ºä¾‹ï¼š
pinecone.init(
    api_key="your-key",
    environment="us-east1-aws"
)
vectorstore = Pinecone.from_documents(
    splits,
    embeddings,
    index_name="my-rag-system"
)

âœ… Weaviate
# æœ€ä½³é€‰æ‹©ï¼šé«˜çº§åŠŸèƒ½ã€å¤æ‚æ•°æ®ç±»å‹

from langchain.vectorstores import Weaviate

# ä¼˜ç‚¹ï¼š
âœ… åŠŸèƒ½ä¸°å¯Œ
âœ… æ”¯æŒå¤šç§æ•°æ®ç±»å‹
âœ… GraphQL API
âœ… æ¨¡å—åŒ–æ¶æ„
âœ… æ”¯æŒæœ¬åœ°å’Œäº‘ç«¯

# ç¼ºç‚¹ï¼š
âŒ é…ç½®å¤æ‚
âŒ å­¦ä¹ æ›²çº¿é™¡å³­
âŒ æ–‡æ¡£ç›¸å¯¹è¾ƒå°‘

# é€‚ç”¨åœºæ™¯ï¼š
- éœ€è¦é«˜çº§åŠŸèƒ½
- å¤æ‚æ•°æ®ç±»å‹
- å›¾åƒã€å¤šæ¨¡æ€æœç´¢
- éœ€è¦GraphQL

# å†³ç­–æ ‘ï¼š

æ•°æ®é‡å¤šå¤§ï¼Ÿ
â†’ < 10ä¸‡ â†’ Chromaï¼ˆæœ€ç®€å•ï¼‰
â†’ 10ä¸‡-100ä¸‡ â†’ Chroma æˆ– FAISS
â†’ > 100ä¸‡ â†’ FAISS æˆ– Pinecone

éœ€è¦é¢‘ç¹æ›´æ–°å—ï¼Ÿ
â†’ æ˜¯ â†’ Chroma æˆ– Pinecone
â†’ å¦ï¼ˆæ‰¹é‡ï¼‰â†’ FAISS

æ˜¯ç”Ÿäº§ç¯å¢ƒå—ï¼Ÿ
â†’ æ˜¯ï¼Œé¢„ç®—å……è¶³ â†’ Pinecone
â†’ æ˜¯ï¼Œé¢„ç®—æœ‰é™ â†’ FAISSï¼ˆè‡ªå»ºï¼‰
â†’ å¦ï¼ˆå­¦ä¹ /åŸå‹ï¼‰â†’ Chroma

å›¢é˜ŸæŠ€æœ¯èƒ½åŠ›ï¼Ÿ
â†’ åˆå­¦è€… â†’ Chroma æˆ– Pinecone
â†’ æœ‰ç»éªŒ â†’ FAISS æˆ– Weaviate

# æ¨èç»„åˆï¼š
# å¼€å‘é˜¶æ®µï¼šChromaï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰
# æµ‹è¯•é˜¶æ®µï¼šFAISSï¼ˆæ€§èƒ½æµ‹è¯•ï¼‰
# ç”Ÿäº§é˜¶æ®µï¼šPineconeï¼ˆç¨³å®šå¯é ï¼‰
```

### Q5: å¦‚ä½•å¤„ç†ä¸­æ–‡RAGï¼Ÿæœ‰ä»€ä¹ˆç‰¹åˆ«è¦æ³¨æ„çš„ï¼Ÿ

**A:**

```python
# ä¸­æ–‡RAGçš„ç‰¹æ®Šå¤„ç†ï¼š

# 1. ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„Embeddingæ¨¡å‹

# âŒ ä¸æ¨èï¼šè‹±æ–‡æ¨¡å‹
from langchain_openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")  # è‹±æ–‡ä¸ºä¸»

# âœ… æ¨èï¼šä¸­æ–‡ä¼˜åŒ–æ¨¡å‹
from langchain_community.embeddings import HuggingFaceEmbeddings

# BGEç³»åˆ—ï¼ˆä¸­æ–‡æœ€ä½³ä¹‹ä¸€ï¼‰
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5",  # BAAIçš„ä¸­æ–‡æ¨¡å‹
    model_kwargs={'device': 'cuda'},  # ä½¿ç”¨GPU
    encode_kwargs={'normalize_embeddings': True}  # å½’ä¸€åŒ–
)

# text2vecç³»åˆ—
embeddings = HuggingFaceEmbeddings(
    model_name="shibing624/text2vec-base-chinese",
    model_kwargs={'device': 'cuda'}
)

# m3e-baseï¼ˆè½»é‡çº§ï¼‰
embeddings = HuggingFaceEmbeddings(
    model_name="moka-ai/m3e-base"
)

# 2. ä¸­æ–‡æ–‡æœ¬åˆ†å‰²

# âœ… æ¨èï¼šé’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–
from langchain.text_splitter import RecursiveCharacterTextSplitter

# ä¸­æ–‡åˆ†å‰²å™¨ï¼ˆæŒ‰å¥å­åˆ†å‰²ï¼‰
chinese_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,              # ä¸­æ–‡å­—ç¬¦å¯ä»¥ç¨å¤§
    chunk_overlap=100,           # 20%é‡å 
    separators=[
        "\n\n",    # æ®µè½
        "\n",      # è¡Œ
        "ã€‚",      # ä¸­æ–‡å¥å·
        "ï¼",      # ä¸­æ–‡æ„Ÿå¹å·
        "ï¼Ÿ",      # ä¸­æ–‡é—®å·
        "ï¼›",      # ä¸­æ–‡åˆ†å·
        "ï¼Œ",      # ä¸­æ–‡é€—å·
        " ",       # ç©ºæ ¼
        ""         # å­—ç¬¦çº§
    ],
    length_function=len  # ä¸­æ–‡å­—ç¬¦è®¡æ•°
)

# 3. ä¸­æ–‡åˆ†è¯ï¼ˆå¯é€‰ï¼‰

import jieba

def chinese_tokenize(text):
    """ä¸­æ–‡åˆ†è¯"""
    return list(jieba.cut(text))

# ç”¨äºæŸäº›éœ€è¦åˆ†è¯çš„åœºæ™¯
tokens = chinese_tokenize("ä¸­æ–‡åˆ†è¯æµ‹è¯•")

# 4. å¤„ç†ä¸­æ–‡ç¼–ç 

# âœ… å§‹ç»ˆæŒ‡å®šç¼–ç 
from langchain.document_loaders import TextLoader

loader = TextLoader(
    "ä¸­æ–‡æ–‡æ¡£.txt",
    encoding='utf-8'  # æˆ– 'gbk', 'gb2312'
)

# 5. ä¸­æ–‡Promptä¼˜åŒ–

# âœ… ä¸­æ–‡RAGä¸“ç”¨Prompt
prompt_template = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡AIåŠ©æ‰‹ã€‚

è¯·åŸºäºä»¥ä¸‹ä¸­æ–‡ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. ä½¿ç”¨æµåˆ©çš„ä¸­æ–‡
2. å‡†ç¡®ç†è§£ä¸­æ–‡è¯­ä¹‰
3. ä¿æŒä¸“ä¸šå’Œç¤¼è²Œ
4. å¼•ç”¨ä¿¡æ¯æ¥æº

å›ç­”ï¼š"""

# 6. å¤„ç†ä¸­æ–‡æ ‡ç‚¹ç¬¦å·

import re

def clean_chinese_text(text):
    """æ¸…ç†ä¸­æ–‡æ–‡æœ¬"""
    # ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
    text = text.replace("ï¼Œ", ",").replace("ã€‚", ".")
    # å»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    # å»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿã€ï¼›ï¼š""''ï¼ˆï¼‰ã€Šã€‹\s]', '', text)
    return text

# 7. ä¸­æ–‡ç›¸å…³æ€§è¯„åˆ†

def chinese_relevance_score(query, document):
    """ä¸­æ–‡ç›¸å…³æ€§è¯„åˆ†"""
    # ä½¿ç”¨ä¸­æ–‡æ¨¡å‹è®¡ç®—ç›¸ä¼¼åº¦
    query_embedding = embeddings.embed_query(query)
    doc_embedding = embeddings.embed_query(document)

    # ä½™å¼¦ç›¸ä¼¼åº¦
    from sklearn.metrics.pairwise import cosine_similarity
    similarity = cosine_similarity(
        [query_embedding],
        [doc_embedding]
    )[0][0]

    return similarity

# 8. ä¸­æ–‡å…ƒæ•°æ®å¤„ç†

# æ·»åŠ ä¸­æ–‡å…ƒæ•°æ®
document.metadata = {
    "source": "ä¸­æ–‡æ–‡æ¡£.pdf",
    "title": "äº§å“è¯´æ˜ä¹¦",
    "category": "æŠ€æœ¯æ–‡æ¡£",
    "language": "zh-CN"  # æ ‡è®°ä¸ºä¸­æ–‡
}

# å…ƒæ•°æ®è¿‡æ»¤ï¼ˆä¸­æ–‡ï¼‰
results = vectorstore.similarity_search(
    "äº§å“åŠŸèƒ½",
    filter={"language": "zh-CN"}  # åªæ£€ç´¢ä¸­æ–‡æ–‡æ¡£
)

# ä¸­æ–‡RAGæœ€ä½³å®è·µï¼š
âœ… ä½¿ç”¨ä¸­æ–‡ä¼˜åŒ–çš„Embeddingæ¨¡å‹
âœ… æŒ‰ä¸­æ–‡æ ‡ç‚¹ç¬¦å·åˆ†å‰²æ–‡æœ¬
âœ… æ³¨æ„æ–‡ä»¶ç¼–ç ï¼ˆUTF-8ï¼‰
âœ… ä½¿ç”¨ä¸­æ–‡å‹å¥½çš„Prompt
âœ… å¤„ç†ä¸­æ–‡ç‰¹æœ‰çš„æ ‡ç‚¹å’Œæ ¼å¼
âœ… æ·»åŠ è¯­è¨€å…ƒæ•°æ®æ ‡ç­¾
âœ… æµ‹è¯•ä¸­æ–‡è¯­ä¹‰ç†è§£å‡†ç¡®åº¦

# ä¸­æ–‡Embeddingæ¨¡å‹æ¨èï¼ˆæŒ‰æ•ˆæœæ’åºï¼‰ï¼š
# 1. BAAI/bge-large-zh-v1.5 â­â­â­â­â­
# 2. BAAI/bge-base-zh-v1.5 â­â­â­â­
# 3. shibing624/text2vec-base-chinese â­â­â­â­
# 4. moka-ai/m3e-large â­â­â­â­
# 5. shibing624/text2vec-large-chinese â­â­â­
```

### Q6: RAGç³»ç»Ÿå¦‚ä½•æ›´æ–°çŸ¥è¯†ï¼Ÿæ·»åŠ æ–°æ–‡æ¡£éœ€è¦é‡å»ºç´¢å¼•å—ï¼Ÿ

**A:**

```python
# RAGçŸ¥è¯†åº“æ›´æ–°ç­–ç•¥ï¼š

# 1. å¢é‡æ·»åŠ ï¼ˆæ¨èï¼‰

# Chromaæ”¯æŒå¢é‡æ·»åŠ 
from langchain.vectorstores import Chroma

# åŠ è½½ç°æœ‰æ•°æ®åº“
vectorstore = Chroma(
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# æ·»åŠ æ–°æ–‡æ¡£
new_docs = load_new_documents("new_docs/")
vectorstore.add_documents(new_docs)

# è‡ªåŠ¨ä¿å­˜åˆ°æŒä¹…åŒ–å­˜å‚¨
# æ— éœ€é‡å»ºæ•´ä¸ªç´¢å¼•ï¼

# 2. æ‰¹é‡æ›´æ–°

def batch_update_vectorstore(vectorstore, new_docs_path, batch_size=100):
    """æ‰¹é‡æ›´æ–°å‘é‡åº“"""

    # åŠ è½½æ–°æ–‡æ¡£
    new_docs = load_documents(new_docs_path)

    # åˆ†æ‰¹æ·»åŠ 
    for i in range(0, len(new_docs), batch_size):
        batch = new_docs[i:i+batch_size]
        vectorstore.add_documents(batch)
        print(f"å·²æ·»åŠ  {i+len(batch)}/{len(new_docs)} ä¸ªæ–‡æ¡£")

# 3. æ›´æ–°å·²æœ‰æ–‡æ¡£

# åˆ é™¤æ—§ç‰ˆæœ¬
vectorstore.delete(ids=["doc_id_1", "doc_id_2"])

# æ·»åŠ æ–°ç‰ˆæœ¬
updated_docs = load_updated_documents()
vectorstore.add_documents(updated_docs)

# 4. FAISSçš„æ›´æ–°ï¼ˆéœ€è¦ç‰¹æ®Šå¤„ç†ï¼‰

from langchain.vectorstores import FAISS

# FAISSä¸æ”¯æŒç›´æ¥æ·»åŠ ï¼Œéœ€è¦ï¼š
# æ–¹æ³•1ï¼šåˆå¹¶ç´¢å¼•
old_index = FAISS.load_local("faiss_index", embeddings)
new_index = FAISS.from_documents(new_docs, embeddings)
merged_index = old_index.merge_from(new_index)
merged_index.save_local("faiss_index")

# æ–¹æ³•2ï¼šå®šæœŸé‡å»º
def rebuild_faiss_index(all_docs_path):
    """é‡å»ºFAISSç´¢å¼•"""
    # åŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼ˆåŒ…æ‹¬æ–°çš„ï¼‰
    all_docs = load_documents(all_docs_path)

    # é‡å»ºç´¢å¼•
    new_index = FAISS.from_documents(all_docs, embeddings)
    new_index.save_local("faiss_index")

# 5. å…ƒæ•°æ®è¿‡æ»¤æ›´æ–°

# é€šè¿‡å…ƒæ•°æ®æ ‡è®°æ–‡æ¡£çŠ¶æ€
document.metadata = {
    "source": "document.pdf",
    "created_at": "2024-01-15",
    "updated_at": "2024-01-20",
    "version": "v2.0",
    "status": "active"  # active, archived, deleted
}

# æŸ¥è¯¢æ—¶è¿‡æ»¤
results = vectorstore.similarity_search(
    query,
    filter={
        "status": "active",
        "version": "v2.0"
    }
)

# 6. è‡ªåŠ¨åŒ–æ›´æ–°ç³»ç»Ÿ

class RAGKnowledgeUpdater:
    """RAGçŸ¥è¯†åº“æ›´æ–°ç³»ç»Ÿ"""

    def __init__(self, vectorstore_path):
        self.vectorstore = Chroma(
            persist_directory=vectorstore_path,
            embedding_function=embeddings
        )
        self.doc_tracker = DocumentTracker()  # æ–‡æ¡£è¿½è¸ªå™¨

    def check_updates(self, docs_path):
        """æ£€æŸ¥æ–‡æ¡£æ›´æ–°"""
        # æ‰«ææ–‡æ¡£ç›®å½•
        current_files = scan_directory(docs_path)

        # å¯¹æ¯”å·²è®°å½•çš„æ–‡ä»¶
        new_files, modified_files, deleted_files = \
            self.doc_tracker.compare(current_files)

        return {
            "new": new_files,
            "modified": modified_files,
            "deleted": deleted_files
        }

    def update_knowledge_base(self, docs_path):
        """æ›´æ–°çŸ¥è¯†åº“"""

        # 1. æ£€æŸ¥æ›´æ–°
        updates = self.check_updates(docs_path)

        # 2. å¤„ç†æ–°æ–‡æ¡£
        if updates["new"]:
            new_docs = load_documents(updates["new"])
            self.vectorstore.add_documents(new_docs)
            print(f"âœ… æ·»åŠ äº† {len(new_docs)} ä¸ªæ–°æ–‡æ¡£")

        # 3. æ›´æ–°å·²ä¿®æ”¹æ–‡æ¡£
        if updates["modified"]:
            for file_path in updates["modified"]:
                # åˆ é™¤æ—§ç‰ˆæœ¬
                self.vectorstore.delete(
                    ids=[self.doc_tracker.get_doc_id(file_path)]
                )
                # æ·»åŠ æ–°ç‰ˆæœ¬
                new_doc = load_document(file_path)
                self.vectorstore.add_documents([new_doc])
            print(f"âœ… æ›´æ–°äº† {len(updates['modified'])} ä¸ªæ–‡æ¡£")

        # 4. å¤„ç†å·²åˆ é™¤æ–‡æ¡£
        if updates["deleted"]:
            doc_ids = [
                self.doc_tracker.get_doc_id(f)
                for f in updates["deleted"]
            ]
            self.vectorstore.delete(ids=doc_ids)
            print(f"âœ… åˆ é™¤äº† {len(updates['deleted'])} ä¸ªæ–‡æ¡£")

        # 5. æ›´æ–°è¿½è¸ªè®°å½•
        self.doc_tracker.update_records(
            new=updates["new"],
            modified=updates["modified"],
            deleted=updates["deleted"]
        )

    def schedule_auto_update(self, docs_path, interval_hours=24):
        """å®šæ—¶è‡ªåŠ¨æ›´æ–°"""
        import schedule
        import time

        def job():
            print(f"\n{datetime.now()}: å¼€å§‹æ£€æŸ¥æ›´æ–°...")
            self.update_knowledge_base(docs_path)

        schedule.every(interval_hours).hours.do(job)

        while True:
            schedule.run_pending()
            time.sleep(60)

# 7. ç‰ˆæœ¬æ§åˆ¶

class VersionedVectorStore:
    """å¸¦ç‰ˆæœ¬æ§åˆ¶çš„å‘é‡åº“"""

    def __init__(self, base_path):
        self.base_path = base_path
        self.current_version = self._get_latest_version()

    def create_new_version(self, docs):
        """åˆ›å»ºæ–°ç‰ˆæœ¬"""
        new_version = self.current_version + 1
        version_path = f"{self.base_path}/v{new_version}"

        # åˆ›å»ºæ–°ç‰ˆæœ¬
        vectorstore = Chroma.from_documents(
            docs,
            embeddings,
            persist_directory=version_path
        )

        self.current_version = new_version
        return vectorstore

    def rollback_to_version(self, version):
        """å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬"""
        version_path = f"{self.base_path}/v{version}"
        return Chroma(
            persist_directory=version_path,
            embedding_function=embeddings
        )

# æ›´æ–°æœ€ä½³å®è·µï¼š
âœ… ä½¿ç”¨æ”¯æŒå¢é‡æ›´æ–°çš„å‘é‡åº“ï¼ˆChroma, Pineconeï¼‰
âœ… å®šæœŸæ£€æŸ¥æ–‡æ¡£å˜åŒ–
âœ… è‡ªåŠ¨åŒ–æ›´æ–°æµç¨‹
âœ… å®ç°ç‰ˆæœ¬æ§åˆ¶
âœ… åˆ é™¤è¿‡æœŸæ–‡æ¡£
âœ… è®°å½•æ›´æ–°æ—¥å¿—
âŒ ä¸è¦æ¯æ¬¡éƒ½é‡å»ºæ•´ä¸ªç´¢å¼•ï¼ˆé™¤éå¿…è¦ï¼‰
```

### Q7: RAGç³»ç»Ÿå›ç­”ä¸å‡†ç¡®æ€ä¹ˆåŠï¼Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿ

**A:**

```python
# RAGç³»ç»Ÿä¼˜åŒ–ç­–ç•¥ï¼š

# é—®é¢˜1ï¼šæ£€ç´¢ä¸ç²¾å‡†
# è§£å†³æ–¹æ¡ˆï¼š

# 1. è°ƒæ•´æ£€ç´¢å‚æ•°
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "k": 5,                  # å¢åŠ æ£€ç´¢æ•°é‡
        "score_threshold": 0.7   # è®¾ç½®ç›¸ä¼¼åº¦é˜ˆå€¼
    }
)

# 2. ä½¿ç”¨å¤šç§æ£€ç´¢æ–¹å¼
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import BM25Retriever

# å…³é”®è¯æ£€ç´¢
bm25_retriever = BM25Retriever.from_documents(splits)
bm25_retriever.k = 3

# è¯­ä¹‰æ£€ç´¢
vector_retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

# ç»„åˆæ£€ç´¢
ensemble_retriever = EnsembleRetriever(
    retrievers=[bm25_retriever, vector_retriever],
    weights=[0.4, 0.6]  # è°ƒæ•´æƒé‡
)

# 3. å¤šæŸ¥è¯¢æ£€ç´¢
from langchain.retrievers import MultiQueryRetriever

multi_query_retriever = MultiQueryRetriever.from_llm(
    retriever=vector_retriever,
    llm=ChatOpenAI(temperature=0)
)
# è‡ªåŠ¨ç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“

# é—®é¢˜2ï¼šç­”æ¡ˆè´¨é‡ä¸é«˜
# è§£å†³æ–¹æ¡ˆï¼š

# 1. ä¼˜åŒ–Prompt
prompt = ChatPromptTemplate.from_template("""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚è¯·ä»”ç»†é˜…è¯»ä»¥ä¸‹ä¸Šä¸‹æ–‡ï¼Œå‡†ç¡®å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯å›ç­”
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜"æ ¹æ®ç°æœ‰ä¿¡æ¯æ— æ³•å›ç­”"
3. å›ç­”è¦å‡†ç¡®ã€å®Œæ•´ã€æœ‰æ¡ç†
4. å¼•ç”¨å…·ä½“çš„ä¿¡æ¯æ¥æº
5. ä¸è¦ç¼–é€ ä»»ä½•ä¿¡æ¯

å›ç­”ï¼š
""")

# 2. é‡æ’åºï¼ˆRerankingï¼‰
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CohereRerankCompressor

compressor = CohereRerankCompressor(
    cohere_api_key="your-key",
    top_n_queries=3  # åªä¿ç•™æœ€ç›¸å…³çš„3ä¸ª
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=vectorstore.as_retriever(search_kwargs={"k": 10})
)
# å…ˆæ£€ç´¢10ä¸ªï¼Œé‡æ’åºåä¿ç•™3ä¸ªæœ€ç›¸å…³çš„

# 3. ä½¿ç”¨æ›´å¼ºçš„LLM
# GPT-3.5 â†’ GPT-4ï¼ˆæ›´å‡†ç¡®ä½†æ›´æ…¢ï¼‰
llm = ChatOpenAI(model="gpt-4", temperature=0)

# é—®é¢˜3ï¼šä¸Šä¸‹æ–‡ä¸è¶³
# è§£å†³æ–¹æ¡ˆï¼š

# 1. å¢åŠ æ£€ç´¢æ•°é‡
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 10}  # ä»3ä¸ªå¢åŠ åˆ°10ä¸ª
)

# 2. ä½¿ç”¨æ›´å¤§çš„chunk
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500,  # ä»1000å¢åŠ åˆ°1500
    chunk_overlap=300
)

# 3. æ·»åŠ æ–‡æ¡£æ‘˜è¦
def add_document_summary(docs):
    """ä¸ºæ–‡æ¡£æ·»åŠ æ‘˜è¦"""
    for doc in docs:
        if "summary" not in doc.metadata:
            summary = llm.invoke(f"æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼š\n{doc.page_content[:1000]}")
            doc.metadata["summary"] = summary
    return docs

# é—®é¢˜4ï¼šå›ç­”ä¸å¤Ÿå…·ä½“
# è§£å†³æ–¹æ¡ˆï¼š

# 1. ä½¿ç”¨HyDEï¼ˆHypothetical Document Embeddingsï¼‰
def hyde_retrieval(query, retriever, llm):
    """å‡è®¾æ€§æ–‡æ¡£åµŒå…¥"""

    # å…ˆç”Ÿæˆå‡è®¾æ€§ç­”æ¡ˆ
    hypothetical_answer = llm.invoke(
        f"è¯·åŸºäºé—®é¢˜'{query}'ç”Ÿæˆä¸€ä¸ªè¯¦ç»†çš„å‡è®¾æ€§å›ç­”"
    )

    # ç”¨å‡è®¾æ€§ç­”æ¡ˆæ£€ç´¢
    docs = retriever.get_relevant_documents(hypothetical_answer)

    return docs

# 2. è¿­ä»£æ£€ç´¢
def iterative_retrieval(query, retriever, max_iterations=3):
    """è¿­ä»£æ£€ç´¢ä¼˜åŒ–"""

    for i in range(max_iterations):
        # æ£€ç´¢
        docs = retriever.get_relevant_documents(query)

        # ç”Ÿæˆå›ç­”
        answer = rag_chain.invoke(query)

        # æ£€æŸ¥å›ç­”è´¨é‡
        if is_answer_satisfactory(answer):
            break

        # æ ¹æ®å›ç­”ä¼˜åŒ–æŸ¥è¯¢
        query = refine_query(query, answer)

    return answer

# ä¼˜åŒ–æ£€æŸ¥æ¸…å•ï¼š

# æ£€ç´¢è´¨é‡ï¼š
[ ] å°è¯•ä¸åŒçš„kå€¼ï¼ˆ3, 5, 10ï¼‰
[ ] æµ‹è¯•ç›¸ä¼¼åº¦é˜ˆå€¼
[ ] å°è¯•å¤šç§æ£€ç´¢æ–¹å¼ç»„åˆ
[ ] æµ‹è¯•å¤šæŸ¥è¯¢æ£€ç´¢
[ ] æ£€æŸ¥æ–‡æœ¬åˆ†å‰²æ˜¯å¦åˆç†

# Promptä¼˜åŒ–ï¼š
[ ] æ˜ç¡®æŒ‡ç¤ºåªä½¿ç”¨ä¸Šä¸‹æ–‡
[ ] è¦æ±‚å¼•ç”¨æ¥æº
[ ] æŒ‡å®šå›ç­”æ ¼å¼
[ ] æµ‹è¯•ä¸åŒçš„Promptæ¨¡æ¿

# æ¨¡å‹é€‰æ‹©ï¼š
[ ] å°è¯•æ›´å¼ºçš„LLMï¼ˆGPT-4ï¼‰
[ ] è°ƒæ•´temperatureå‚æ•°
[ ] æµ‹è¯•ä¸åŒçš„Embeddingæ¨¡å‹

# æ•°æ®è´¨é‡ï¼š
[ ] æ£€æŸ¥æ–‡æ¡£è´¨é‡
[ ] æ¸…ç†å™ªå£°æ•°æ®
[ ] æ·»åŠ æ–‡æ¡£æ‘˜è¦
[ ] ä¼˜åŒ–å…ƒæ•°æ®

# è¯„ä¼°å’Œè¿­ä»£ï¼š
[ ] æ”¶é›†ç”¨æˆ·åé¦ˆ
[ ] åˆ†æbad cases
[ ] A/Bæµ‹è¯•ä¸åŒæ–¹æ¡ˆ
[ ] æŒç»­ä¼˜åŒ–å‚æ•°
```

### Q8: RAGç³»ç»Ÿçš„æˆæœ¬å¦‚ä½•ä¼°ç®—å’Œä¼˜åŒ–ï¼Ÿ

**A:**

```python
# RAGç³»ç»Ÿæˆæœ¬ä¼°ç®—å’Œä¼˜åŒ–ï¼š

# 1. æˆæœ¬æ„æˆ

# ä¸»è¦æˆæœ¬ï¼š
# - Embedding APIè°ƒç”¨
# - LLM APIè°ƒç”¨ï¼ˆç”Ÿæˆç­”æ¡ˆï¼‰
# - å‘é‡æ•°æ®åº“å­˜å‚¨
# - æœåŠ¡å™¨æˆæœ¬

# 2. æˆæœ¬è®¡ç®—å™¨

class RAGCostCalculator:
    """RAGæˆæœ¬è®¡ç®—å™¨"""

    def __init__(self):
        # OpenAIå®šä»·ï¼ˆ2024å¹´ä»·æ ¼ï¼‰
        self.pricing = {
            "gpt-3.5-turbo": {
                "input": 0.0005,      # $0.50 / 1M tokens
                "output": 0.0015     # $1.50 / 1M tokens
            },
            "gpt-4": {
                "input": 0.03,       # $30 / 1M tokens
                "output": 0.06       # $60 / 1M tokens
            },
            "text-embedding-3-small": {
                "input": 0.00002     # $0.02 / 1M tokens
            },
            "pinecone": {
                "starter": 70,       # $70 / æœˆ
                "production": 400    # èµ·æ­¥ $400 / æœˆ
            }
        }

    def calculate_embedding_cost(self, num_documents, avg_doc_length):
        """è®¡ç®—Embeddingæˆæœ¬"""
        # 1 token â‰ˆ 0.75 è‹±æ–‡å•è¯ â‰ˆ 3-4 å­—ç¬¦
        total_tokens = (num_documents * avg_doc_length) / 3

        # Embeddingä»·æ ¼
        cost_per_1m_tokens = self.pricing["text-embedding-3-small"]["input"]
        embedding_cost = (total_tokens / 1_000_000) * cost_per_1m_tokens

        return {
            "total_tokens": int(total_tokens),
            "estimated_cost": embedding_cost
        }

    def calculate_query_cost(self, queries_per_day, avg_context_length, avg_answer_length):
        """è®¡ç®—æŸ¥è¯¢æˆæœ¬"""
        # æ¯æ¬¡æŸ¥è¯¢çš„tokens
        input_tokens = (avg_context_length * 4) / 3  # ä¸Šä¸‹æ–‡
        input_tokens += 100  # é—®é¢˜æœ¬èº«
        output_tokens = (avg_answer_length * 4) / 3  # ç­”æ¡ˆ

        # æ¯å¤©æˆæœ¬
        daily_input_cost = (queries_per_day * input_tokens / 1_000_000) * \
                          self.pricing["gpt-3.5-turbo"]["input"]
        daily_output_cost = (queries_per_day * output_tokens / 1_000_000) * \
                           self.pricing["gpt-3.5-turbo"]["output"]

        # æ¯æœˆæˆæœ¬ï¼ˆå‡è®¾30å¤©ï¼‰
        monthly_cost = (daily_input_cost + daily_output_cost) * 30

        return {
            "input_tokens_per_query": int(input_tokens),
            "output_tokens_per_query": int(output_tokens),
            "daily_cost": daily_input_cost + daily_output_cost,
            "monthly_cost": monthly_cost
        }

    def estimate_total_monthly_cost(
        self,
        num_documents=10000,
        avg_doc_length=500,
        queries_per_day=1000,
        avg_context_length=1500,
        avg_answer_length=300
    ):
        """ä¼°ç®—æ€»æœˆåº¦æˆæœ¬"""

        # Embeddingæˆæœ¬ï¼ˆä¸€æ¬¡æ€§ï¼ŒæŒ‰æœˆæ‘Šé”€ï¼‰
        embedding_cost = self.calculate_embedding_cost(num_documents, avg_doc_length)
        monthly_embedding_cost = embedding_cost["estimated_cost"] / 12  # æ‘Šé”€1å¹´

        # æŸ¥è¯¢æˆæœ¬
        query_cost = self.calculate_query_cost(
            queries_per_day,
            avg_context_length,
            avg_answer_length
        )

        # å‘é‡æ•°æ®åº“æˆæœ¬
        vector_db_cost = self.pricing["pinecone"]["starter"]

        # æ€»æˆæœ¬
        total_monthly = (
            monthly_embedding_cost +
            query_cost["monthly_cost"] +
            vector_db_cost
        )

        return {
            "embedding_monthly": monthly_embedding_cost,
            "query_monthly": query_cost["monthly_cost"],
            "vector_db_monthly": vector_db_cost,
            "total_monthly": total_monthly,
            "breakdown": {
                "documents": num_documents,
                "queries_per_day": queries_per_day,
                "queries_per_month": queries_per_day * 30
            }
        }

# ä½¿ç”¨ç¤ºä¾‹ï¼š
calculator = RAGCostCalculator()
estimate = calculator.estimate_total_monthly_cost(
    num_documents=50000,
    avg_doc_length=800,
    queries_per_day=5000
)

print(f"æœˆåº¦æˆæœ¬ä¼°ç®—ï¼š${estimate['total_monthly']:.2f}")
print(f"  - Embedding: ${estimate['embedding_monthly']:.4f}")
print(f"  - æŸ¥è¯¢: ${estimate['query_monthly']:.2f}")
print(f"  - å‘é‡åº“: ${estimate['vector_db_monthly']:.2f}")

# 3. æˆæœ¬ä¼˜åŒ–ç­–ç•¥

# ç­–ç•¥1ï¼šä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
# GPT-4 â†’ GPT-3.5-turboï¼ˆæˆæœ¬é™ä½10-20å€ï¼‰
llm = ChatOpenAI(model="gpt-3.5-turbo")  # è€Œä¸æ˜¯ gpt-4

# ç­–ç•¥2ï¼šä½¿ç”¨å¼€æºæ¨¡å‹ï¼ˆé›¶APIæˆæœ¬ï¼‰
from langchain_community.embeddings import HuggingFaceEmbeddings

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-zh-v1.5"
)  # å®Œå…¨å…è´¹ï¼Œåªéœ€æœåŠ¡å™¨æˆæœ¬

# ç­–ç•¥3ï¼šå‡å°‘æ£€ç´¢æ•°é‡
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 3}  # ä»10å‡å°‘åˆ°3
)
# å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦ï¼Œé™ä½è¾“å…¥Token

# ç­–ç•¥4ï¼šä¼˜åŒ–chunk_size
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,    # åˆç†çš„chunkå¤§å°
    chunk_overlap=150  # é€‚å½“é‡å 
)
# å¤ªå¤§çš„chunkä¼šå¢åŠ Tokenæ¶ˆè€—

# ç­–ç•¥5ï¼šç¼“å­˜å¸¸è§æŸ¥è¯¢
from langchain.cache import InMemoryCache
set_llm_cache(InMemoryCache())

# ç›¸åŒé—®é¢˜ç›´æ¥è¿”å›ç¼“å­˜ç»“æœ

# ç­–ç•¥6ï¼šä½¿ç”¨æ›´ä¾¿å®œçš„å‘é‡åº“
# Pineconeï¼ˆ$70/æœˆï¼‰â†’ Chromaï¼ˆå…è´¹ï¼‰æˆ– FAISSï¼ˆå…è´¹ï¼‰
vectorstore = Chroma(  # å®Œå…¨å…è´¹
    persist_directory="./chroma_db",
    embedding_function=embeddings
)

# ç­–ç•¥7ï¼šé™åˆ¶å›ç­”é•¿åº¦
prompt = ChatPromptTemplate.from_template("""
è¯·ç®€æ´å›ç­”ï¼Œä¸è¶…è¿‡200å­—ã€‚

ä¸Šä¸‹æ–‡ï¼š{context}
é—®é¢˜ï¼š{question}
""")

# ç­–ç•¥8ï¼šåˆ†é˜¶æ®µå¤„ç†
# é˜¶æ®µ1ï¼šç®€å•æŸ¥è¯¢ç”¨GPT-3.5
# é˜¶æ®µ2ï¼šå¤æ‚æŸ¥è¯¢ç”¨GPT-4

def smart_rag(query):
    # åˆ¤æ–­æŸ¥è¯¢å¤æ‚åº¦
    complexity = analyze_complexity(query)

    if complexity == "simple":
        # ä½¿ç”¨ä¾¿å®œçš„æ¨¡å‹
        return query_with_gpt35(query)
    else:
        # ä½¿ç”¨å¼ºçš„æ¨¡å‹
        return query_with_gpt4(query)

# 4. æˆæœ¬ç›‘æ§

class CostMonitor:
    """æˆæœ¬ç›‘æ§"""

    def __init__(self):
        self.usage_log = []

    def log_query(self, query, input_tokens, output_tokens, model):
        """è®°å½•æŸ¥è¯¢"""
        cost = calculate_cost(input_tokens, output_tokens, model)

        self.usage_log.append({
            "timestamp": datetime.now(),
            "query": query,
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "model": model,
            "cost": cost
        })

    def get_daily_summary(self):
        """è·å–æ¯æ—¥æ±‡æ€»"""
        today = datetime.now().date()
        today_logs = [
            log for log in self.usage_log
            if log["timestamp"].date() == today
        ]

        return {
            "total_queries": len(today_logs),
            "total_tokens": sum(
                log["input_tokens"] + log["output_tokens"]
                for log in today_logs
            ),
            "total_cost": sum(log["cost"] for log in today_logs)
        }

# æˆæœ¬ä¼˜åŒ–æœ€ä½³å®è·µï¼š
âœ… é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼ˆGPT-3.5 vs GPT-4ï¼‰
âœ… è€ƒè™‘ä½¿ç”¨å¼€æºEmbeddingæ¨¡å‹
âœ… ä¼˜åŒ–chunk_sizeå’Œæ£€ç´¢æ•°é‡
âœ… ä½¿ç”¨å…è´¹çš„å‘é‡åº“ï¼ˆChroma, FAISSï¼‰
âœ… é™åˆ¶å›ç­”é•¿åº¦
âœ… å®ç°ç¼“å­˜æœºåˆ¶
âœ… ç›‘æ§å®é™…ä½¿ç”¨é‡
âœ… å®šæœŸå®¡æŸ¥æˆæœ¬æŠ¥å‘Š

# æˆæœ¬ä¼˜åŒ–é¢„æœŸï¼š
# åŸºç¡€ä¼˜åŒ–ï¼šå¯èŠ‚çœ 30-50%
# æ·±åº¦ä¼˜åŒ–ï¼šå¯èŠ‚çœ 50-70%
# æè‡´ä¼˜åŒ–ï¼šå¯èŠ‚çœ 70-90%ï¼ˆä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼‰
```

### Q9: RAGæ”¯æŒå¤šæ¨¡æ€å—ï¼Ÿæ¯”å¦‚å›¾ç‰‡ã€è¡¨æ ¼ï¼Ÿ

**A:**

```python
# å¤šæ¨¡æ€RAGç³»ç»Ÿï¼š

# 1. å›¾åƒRAG

from langchain_community.document_loaders import UnstructuredImageLoader
from langchain_community.vectorstores import Chroma
from PIL import Image

# åŠ è½½å›¾ç‰‡
image_loader = UnstructuredImageLoader("document.png")
image_docs = image_loader.load()

# æå–å›¾åƒæè¿°ï¼ˆä½¿ç”¨å¤šæ¨¡æ€LLMï¼‰
from langchain_openai import ChatOpenAI

vision_llm = ChatOpenAI(model="gpt-4-vision-preview")

def describe_image(image_path):
    """æè¿°å›¾åƒå†…å®¹"""
    from base64 import b64encode

    with open(image_path, "rb") as f:
        image_data = b64encode(f.read()).decode()

    prompt = [
        {
            "type": "text",
            "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„å†…å®¹"
        },
        {
            "type": "image_url",
            "image_url": {
                "url": f"data:image/png;base64,{image_data}"
            }
        }
    ]

    description = vision_llm.invoke(prompt)
    return description.content

# å°†å›¾åƒæè¿°ä½œä¸ºæ–‡æ¡£
image_description = describe_image("chart.png")
image_doc = Document(
    page_content=image_description,
    metadata={"type": "image", "source": "chart.png"}
)

# æ·»åŠ åˆ°å‘é‡åº“
vectorstore.add_documents([image_doc])

# 2. è¡¨æ ¼RAG

import pandas as pd
from langchain_community.document_loaders import UnstructuredExcelLoader

# åŠ è½½Excel
excel_loader = UnstructuredExcelLoader("data.xlsx")
excel_docs = excel_loader.load()

# æˆ–ä½¿ç”¨pandas
df = pd.read_excel("data.xlsx")

def dataframe_to_docs(df):
    """DataFrameè½¬æ–‡æ¡£"""
    docs = []

    # æ¯è¡Œä½œä¸ºä¸€ä¸ªæ–‡æ¡£
    for idx, row in df.iterrows():
        content = f"è¡Œ{idx}ï¼š" + "ï¼Œ".join([f"{col}={val}" for col, val in row.items()])
        doc = Document(
            page_content=content,
            metadata={"type": "table_row", "row_index": idx}
        )
        docs.append(doc)

    return docs

# æˆ–å°†æ•´ä¸ªè¡¨æ ¼æè¿°ä¸ºæ–‡æ¡£
def describe_table(df):
    """æè¿°è¡¨æ ¼å†…å®¹"""
    description = f"""
    è¡¨æ ¼åŒ…å«{len(df)}è¡Œ{len(df.columns)}åˆ—ã€‚
    åˆ—åï¼š{', '.join(df.columns)}
    å‰3è¡Œæ•°æ®ï¼š
    {df.head(3).to_string()}
    """
    return description

# 3. æ··åˆæ¨¡æ€RAG

class MultiModalRAG:
    """å¤šæ¨¡æ€RAGç³»ç»Ÿ"""

    def __init__(self):
        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            embedding_function=self.embeddings,
            persist_directory="./multimodal_db"
        )
        self.vision_llm = ChatOpenAI(model="gpt-4-vision-preview")
        self.text_llm = ChatOpenAI(model="gpt-3.5-turbo")

    def add_text_document(self, text, metadata):
        """æ·»åŠ æ–‡æœ¬æ–‡æ¡£"""
        doc = Document(page_content=text, metadata=metadata)
        self.vectorstore.add_documents([doc])

    def add_image_document(self, image_path, metadata):
        """æ·»åŠ å›¾ç‰‡æ–‡æ¡£"""
        # æè¿°å›¾ç‰‡
        description = self._describe_image(image_path)

        # æ·»åŠ åˆ°å‘é‡åº“
        doc = Document(
            page_content=description,
            metadata={**metadata, "type": "image", "source": image_path}
        )
        self.vectorstore.add_documents([doc])

    def add_table_document(self, df, metadata):
        """æ·»åŠ è¡¨æ ¼æ–‡æ¡£"""
        # æè¿°è¡¨æ ¼
        description = self._describe_table(df)

        # æ·»åŠ åˆ°å‘é‡åº“
        doc = Document(
            page_content=description,
            metadata={**metadata, "type": "table"}
        )
        self.vectorstore.add_documents([doc])

    def _describe_image(self, image_path):
        """æè¿°å›¾ç‰‡"""
        from base64 import b64encode

        with open(image_path, "rb") as f:
            image_data = b64encode(f.read()).decode()

        prompt = [
            {"type": "text", "text": "è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡"},
            {
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{image_data}"}
            }
        ]

        return self.vision_llm.invoke(prompt).content

    def _describe_table(self, df):
        """æè¿°è¡¨æ ¼"""
        return f"""
        è¡¨æ ¼æ•°æ®ï¼š
        è¡Œæ•°ï¼š{len(df)}
        åˆ—åï¼š{', '.join(df.columns)}
        ç¤ºä¾‹æ•°æ®ï¼š
        {df.head(3).to_string()}
        """

    def query(self, question):
        """æŸ¥è¯¢ï¼ˆæ··åˆæ¨¡æ€ï¼‰"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = self.vectorstore.similarity_search(question, k=3)

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"[{doc.metadata.get('type', 'text')}] {doc.page_content}"
            for doc in docs
        ])

        # ç”Ÿæˆå›ç­”
        prompt = f"""åŸºäºä»¥ä¸‹å¤šæ¨¡æ€å†…å®¹å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""

        answer = self.text_llm.invoke(prompt)
        return answer.content

# 4. ä½¿ç”¨ä¸“ç”¨å·¥å…·

# LlamaParseï¼ˆè§£æå¤æ‚æ–‡æ¡£ï¼‰
# from llama_parse import LlamaParse
#
# parser = LlamaParse(
#     api_key="your-key",
#     result_type="markdown"  # è¾“å‡ºMarkdownæ ¼å¼
# )
#
# documents = parser.load_data("document.pdf")
# è‡ªåŠ¨å¤„ç†æ–‡æœ¬ã€å›¾ç‰‡ã€è¡¨æ ¼

# Unstructuredï¼ˆå¤šç§æ ¼å¼è§£æï¼‰
# from unstructured.partition.auto import partition
#
# elements = partition("document.pdf")
# for element in elements:
#     print(f"Type: {element.category}, Content: {element.text}")

# å¤šæ¨¡æ€RAGæœ€ä½³å®è·µï¼š
âœ… å°†å›¾ç‰‡è½¬æ¢ä¸ºæ–‡æœ¬æè¿°
âœ… æå–è¡¨æ ¼ç»“æ„ä¿¡æ¯
âœ… æ ‡æ³¨æ–‡æ¡£ç±»å‹å…ƒæ•°æ®
âœ… ä½¿ç”¨å¤šæ¨¡æ€LLMï¼ˆGPT-4Vï¼‰
âœ… è€ƒè™‘ä½¿ç”¨ä¸“ç”¨è§£æå·¥å…·
âœ… è¯„ä¼°æ¯ç§æ¨¡æ€çš„æ£€ç´¢æ•ˆæœ
âš ï¸ æˆæœ¬è¾ƒé«˜ï¼ˆå¤šæ¨¡æ€APIæ›´è´µï¼‰
âš ï¸ å»¶è¿Ÿè¾ƒé«˜ï¼ˆå¤„ç†æ—¶é—´æ›´é•¿ï¼‰
```

### Q10: å¦‚ä½•æ„å»ºä¸€ä¸ªç”Ÿäº§çº§çš„RAGç³»ç»Ÿï¼Ÿ

**A:**

```python
# ç”Ÿäº§çº§RAGç³»ç»Ÿæ¶æ„ï¼š

# 1. ç³»ç»Ÿæ¶æ„

"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ç”Ÿäº§çº§RAGç³»ç»Ÿæ¶æ„                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ API Gateway â”‚ â”€â”€â”€â†’ â”‚ Load Balancerâ”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                              â†“                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚           RAGæœåŠ¡é›†ç¾¤ï¼ˆå¤šå®ä¾‹ï¼‰              â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚ Instance1â”‚  â”‚ Instance2â”‚  â”‚ Instance3â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚              â†“ â†“ â†“ â†“ â†“ â†“ â†“ â†“ â†“                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚          å…±äº«å‘é‡æ•°æ®åº“é›†ç¾¤                   â”‚    â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚    â”‚
â”‚  â”‚  â”‚  Node 1  â”‚  â”‚  Node 2  â”‚  â”‚  Node 3  â”‚  â”‚    â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚ ç›‘æ§å‘Šè­¦    â”‚      â”‚ æ—¥å¿—ç³»ç»Ÿ      â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

# 2. ç”Ÿäº§çº§RAGæœåŠ¡

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import logging
from typing import List, Optional
import redis
import json

app = FastAPI(title="ç”Ÿäº§çº§RAG API")

# é…ç½®CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Redisç¼“å­˜
redis_client = redis.Redis(host='localhost', port=6379, db=0)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class QueryRequest(BaseModel):
    question: str
    use_cache: bool = True
    top_k: int = 3

class QueryResponse(BaseModel):
    answer: str
    sources: List[dict]
    cached: bool = False
    latency_ms: int

class ProductionRAG:
    """ç”Ÿäº§çº§RAGç³»ç»Ÿ"""

    def __init__(self):
        # åˆå§‹åŒ–ç»„ä»¶
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

        # åŠ è½½å‘é‡åº“
        self.vectorstore = Chroma(
            persist_directory="./chroma_db",
            embedding_function=self.embeddings
        )
        self.retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 3}
        )

        # åˆ›å»ºé“¾
        self.chain = self._create_chain()

        # æ€§èƒ½ç›‘æ§
        self.metrics = {
            "total_queries": 0,
            "cache_hits": 0,
            "avg_latency": 0
        }

    def _create_chain(self):
        """åˆ›å»ºRAGé“¾"""
        template = """ä½ æ˜¯ä¸“ä¸šAIåŠ©æ‰‹ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åŸºäºä¸Šä¸‹æ–‡å‡†ç¡®å›ç­”
2. å¦‚æ— ç›¸å…³ä¿¡æ¯ï¼Œæ˜ç¡®è¯´æ˜
3. å¼•ç”¨ä¿¡æ¯æ¥æº

å›ç­”ï¼š"""

        prompt = ChatPromptTemplate.from_template(template)

        def format_docs(docs):
            return "\n\n".join([
                f"ã€æ¥æºï¼š{doc.metadata.get('source', 'Unknown')}ã€‘\n{doc.page_content}"
                for doc in docs
            ])

        return (
            {
                "context": self.retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | prompt
            | self.llm
            | StrOutputParser()
        )

    def query(self, question: str, use_cache: bool = True) -> dict:
        """æŸ¥è¯¢ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        import time

        start_time = time.time()

        # æ£€æŸ¥ç¼“å­˜
        if use_cache:
            cached_result = self._get_cache(question)
            if cached_result:
                self.metrics["cache_hits"] += 1
                return {
                    **cached_result,
                    "cached": True,
                    "latency_ms": int((time.time() - start_time) * 1000)
                }

        try:
            # æŸ¥è¯¢
            answer = self.chain.invoke(question)

            # æ£€ç´¢æ¥æº
            docs = self.vectorstore.similarity_search(question, k=3)
            sources = [
                {
                    "source": doc.metadata.get('source'),
                    "content": doc.page_content[:200] + "..."
                }
                for doc in docs
            ]

            result = {
                "answer": answer,
                "sources": sources,
                "cached": False
            }

            # ä¿å­˜åˆ°ç¼“å­˜
            if use_cache:
                self._set_cache(question, result)

            # æ›´æ–°æŒ‡æ ‡
            self.metrics["total_queries"] += 1
            latency = (time.time() - start_time) * 1000
            self.metrics["avg_latency"] = (
                self.metrics["avg_latency"] * (self.metrics["total_queries"] - 1) + latency
            ) / self.metrics["total_queries"]

            return {
                **result,
                "latency_ms": int(latency)
            }

        except Exception as e:
            logger.error(f"æŸ¥è¯¢å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=str(e))

    def _get_cache(self, question: str) -> Optional[dict]:
        """è·å–ç¼“å­˜"""
        try:
            cache_key = f"rag:{hash(question)}"
            cached = redis_client.get(cache_key)
            if cached:
                return json.loads(cached)
        except Exception as e:
            logger.warning(f"ç¼“å­˜è¯»å–å¤±è´¥: {e}")
        return None

    def _set_cache(self, question: str, result: dict):
        """è®¾ç½®ç¼“å­˜ï¼ˆ1å°æ—¶ï¼‰"""
        try:
            cache_key = f"rag:{hash(question)}"
            redis_client.setex(
                cache_key,
                3600,  # 1å°æ—¶
                json.dumps(result, ensure_ascii=False)
            )
        except Exception as e:
            logger.warning(f"ç¼“å­˜å†™å…¥å¤±è´¥: {e}")

    def get_metrics(self) -> dict:
        """è·å–æ€§èƒ½æŒ‡æ ‡"""
        return {
            **self.metrics,
            "cache_hit_rate": (
                self.metrics["cache_hits"] / self.metrics["total_queries"]
                if self.metrics["total_queries"] > 0 else 0
            )
        }

# åˆå§‹åŒ–
rag = ProductionRAG()

# APIç«¯ç‚¹
@app.post("/query", response_model=QueryResponse)
async def query(request: QueryRequest):
    """æŸ¥è¯¢æ¥å£"""
    logger.info(f"æ”¶åˆ°æŸ¥è¯¢: {request.question}")
    result = rag.query(request.question, request.use_cache)
    return result

@app.get("/metrics")
async def metrics():
    """æ€§èƒ½æŒ‡æ ‡"""
    return rag.get_metrics()

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}

# 3. Dockeréƒ¨ç½²

# Dockerfile
"""
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

EXPOSE 8000

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
"""

# docker-compose.yml
"""
version: '3.8'

services:
  rag-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - redis

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
"""

# 4. ç›‘æ§å’Œå‘Šè­¦

from prometheus_client import Counter, Histogram, generate_latest

# å®šä¹‰æŒ‡æ ‡
query_counter = Counter('rag_queries_total', 'Total queries')
query_latency = Histogram('rag_query_latency_seconds', 'Query latency')
cache_hits = Counter('rag_cache_hits_total', 'Cache hits')

# åœ¨æŸ¥è¯¢ä¸­ä½¿ç”¨
@query_latency.time()
def query_with_monitoring(question):
    query_counter.inc()
    result = rag.query(question)
    if result["cached"]:
        cache_hits.inc()
    return result

@app.get("/metrics")
async def prometheus_metrics():
    """PrometheusæŒ‡æ ‡"""
    return Response(content=generate_latest(), media_type="text/plain")

# 5. å®‰å…¨æ€§

# æ·»åŠ è®¤è¯
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

security = HTTPBearer()

@app.post("/query")
async def secure_query(
    request: QueryRequest,
    credentials: HTTPAuthorizationCredentials = Depends(security)
):
    # éªŒè¯Token
    if not validate_token(credentials.credentials):
        raise HTTPException(status_code=401, detail="Invalid token")

    return await query(request)

# é€Ÿç‡é™åˆ¶
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)
app.state.limiter = limiter

@app.post("/query")
@limiter.limit("10/minute")  # æ¯åˆ†é’Ÿ10æ¬¡
async def rate_limited_query(request: QueryRequest):
    return await query(request)

# ç”Ÿäº§çº§RAGæœ€ä½³å®è·µï¼š
âœ… ä½¿ç”¨API Gatewayï¼ˆKong, AWS API Gatewayï¼‰
âœ… å®ç°è´Ÿè½½å‡è¡¡ï¼ˆå¤šå®ä¾‹ï¼‰
âœ… æ·»åŠ Redisç¼“å­˜
âœ… å®ç°ç›‘æ§å‘Šè­¦ï¼ˆPrometheus + Grafanaï¼‰
âœ… ç»“æ„åŒ–æ—¥å¿—ï¼ˆELK Stackï¼‰
âœ… é™æµå’Œè®¤è¯
âœ… å¥åº·æ£€æŸ¥
âœ… è‡ªåŠ¨ä¼¸ç¼©ï¼ˆK8s HPAï¼‰
âœ… è“ç»¿éƒ¨ç½²/é‡‘ä¸é›€å‘å¸ƒ
âœ… å¤‡ä»½å’Œç¾éš¾æ¢å¤
âœ… æ–‡æ¡£å’ŒAPIè§„èŒƒ
âœ… æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ–
```

---

## å­¦ä¹ æ¸…å•

æ£€æŸ¥ä½ æŒæ¡äº†ä»¥ä¸‹æŠ€èƒ½ï¼š

### åŸºç¡€æ¦‚å¿µ âœ…

- [ ] ç†è§£RAGçš„å®šä¹‰å’Œä»·å€¼
- [ ] æŒæ¡RAGçš„å·¥ä½œæµç¨‹
- [ ] äº†è§£RAG vs Fine-tuningçš„åŒºåˆ«
- [ ] èƒ½å¤Ÿè¯†åˆ«RAGçš„åº”ç”¨åœºæ™¯
- [ ] ç†è§£æ£€ç´¢å¢å¼ºçš„åŸç†

### æ ¸å¿ƒç»„ä»¶ âœ…

- [ ] ä¼šä½¿ç”¨Document LoadersåŠ è½½å„ç§æ–‡æ¡£
- [ ] æŒæ¡Text Splittersçš„åˆ†å‰²ç­–ç•¥
- [ ] ç†è§£Embeddingsçš„ä½œç”¨
- [ ] èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„Embeddingæ¨¡å‹
- [ ] ä¼šä½¿ç”¨å‘é‡æ•°æ®åº“ï¼ˆChroma, FAISSï¼‰
- [ ] ç†è§£ç›¸ä¼¼åº¦æœç´¢åŸç†

### ç³»ç»Ÿæ„å»º âœ…

- [ ] èƒ½å¤Ÿæ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿ
- [ ] æŒæ¡RAGé“¾çš„åˆ›å»ºæ–¹æ³•
- [ ] ä¼šå®ç°å¸¦æ¥æºå¼•ç”¨çš„RAG
- [ ] èƒ½å¤Ÿä¼˜åŒ–æ£€ç´¢æ•ˆæœ
- [ ] ç†è§£RAGçš„ä¼˜åŒ–æŠ€å·§

### é«˜çº§æŠ€èƒ½ âœ…

- [ ] ä¼šä½¿ç”¨æ··åˆæœç´¢ï¼ˆå…³é”®è¯+è¯­ä¹‰ï¼‰
- [ ] ç†è§£é‡æ’åºï¼ˆRerankingï¼‰çš„ä½œç”¨
- [ ] èƒ½å¤Ÿè¯„ä¼°RAGç³»ç»Ÿçš„æ•ˆæœ
- [ ] æŒæ¡ä¸­æ–‡RAGçš„ç‰¹æ®Šå¤„ç†
- [ ] äº†è§£å¤šæ¨¡æ€RAGçš„å®ç°

### å®æˆ˜èƒ½åŠ› âœ…

- [ ] èƒ½å¤Ÿç‹¬ç«‹æ„å»ºæ–‡æ¡£é—®ç­”ç³»ç»Ÿ
- [ ] ä¼šä¼˜åŒ–RAGçš„å‡†ç¡®æ€§
- [ ] èƒ½å¤Ÿä¼°ç®—å’Œä¼˜åŒ–æˆæœ¬
- [ ] ç†è§£ç”Ÿäº§çº§RAGçš„æ¶æ„
- [ ] ä¼šéƒ¨ç½²å’Œç›‘æ§RAGç³»ç»Ÿ

### æœ€ä½³å®è·µ âœ…

- [ ] çŸ¥é“å¦‚ä½•é€‰æ‹©å‘é‡æ•°æ®åº“
- [ ] ç†è§£chunk_sizeçš„ä¼˜åŒ–æ–¹æ³•
- [ ] æŒæ¡çŸ¥è¯†åº“çš„æ›´æ–°ç­–ç•¥
- [ ] èƒ½å¤Ÿå¤„ç†ä¸­æ–‡RAGçš„ç‰¹æ®Šé—®é¢˜
- [ ] äº†è§£æˆæœ¬ä¼˜åŒ–çš„æ–¹æ³•
- [ ] èƒ½å¤Ÿè®¾è®¡ç”Ÿäº§çº§RAGç³»ç»Ÿ

---

## è¿›é˜¶ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ„å»ºæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

**ä»»åŠ¡**ï¼šåŸºäºä¼ä¸šå†…éƒ¨æ–‡æ¡£æ„å»ºRAGç³»ç»Ÿ

**è¦æ±‚**ï¼š
1. æ”¯æŒPDFã€Wordã€Markdownå¤šç§æ ¼å¼
2. å®ç°æ–‡æ¡£æ›´æ–°æœºåˆ¶
3. æä¾›æ¥æºå¼•ç”¨
4. æ·»åŠ æŸ¥è¯¢ç¼“å­˜
5. å®ç°æ€§èƒ½ç›‘æ§

**æŠ€æœ¯æ ˆ**ï¼š
- LangChain + OpenAI
- Chromaå‘é‡æ•°æ®åº“
- FastAPIï¼ˆWebæœåŠ¡ï¼‰
- Redisï¼ˆç¼“å­˜ï¼‰

### ç»ƒä¹ 2ï¼šä¼˜åŒ–RAGæ£€ç´¢æ•ˆæœ

**ä»»åŠ¡**ï¼šå¯¹æ¯”ä¸åŒæ£€ç´¢ç­–ç•¥çš„æ•ˆæœ

**å®éªŒ**ï¼š
1. çº¯è¯­ä¹‰æ£€ç´¢
2. çº¯å…³é”®è¯æ£€ç´¢ï¼ˆBM25ï¼‰
3. æ··åˆæ£€ç´¢
4. å¤šæŸ¥è¯¢æ£€ç´¢
5. é‡æ’åºä¼˜åŒ–

**è¯„ä¼°æŒ‡æ ‡**ï¼š
- Precision@K
- Recall@K
- ç­”æ¡ˆå‡†ç¡®ç‡
- ç”¨æˆ·æ»¡æ„åº¦

### ç»ƒä¹ 3ï¼šå®ç°å¤šæ¨¡æ€RAG

**ä»»åŠ¡**ï¼šæ”¯æŒå›¾ç‰‡å’Œè¡¨æ ¼çš„RAGç³»ç»Ÿ

**åŠŸèƒ½**ï¼š
1. å›¾ç‰‡æè¿°å’Œæ£€ç´¢
2. è¡¨æ ¼æ•°æ®æå–
3. æ··åˆæ¨¡æ€æŸ¥è¯¢
4. ç»Ÿä¸€çš„å‘é‡å­˜å‚¨

**æŠ€æœ¯è¦æ±‚**ï¼š
- ä½¿ç”¨GPT-4Væè¿°å›¾ç‰‡
- æå–è¡¨æ ¼ç»“æ„ä¿¡æ¯
- æ ‡æ³¨æ–‡æ¡£ç±»å‹å…ƒæ•°æ®

### ç»ƒä¹ 4ï¼šæ„å»ºç”Ÿäº§çº§RAGæœåŠ¡

**ä»»åŠ¡**ï¼šéƒ¨ç½²ä¸€ä¸ªé«˜å¯ç”¨çš„RAGæœåŠ¡

**è¦æ±‚**ï¼š
1. RESTful APIè®¾è®¡
2. è®¤è¯å’Œæˆæƒ
3. é€Ÿç‡é™åˆ¶
4. ç›‘æ§å’Œå‘Šè­¦
5. æ—¥å¿—ç³»ç»Ÿ
6. è´Ÿè½½å‡è¡¡

**éƒ¨ç½²æ–¹å¼**ï¼š
- Dockerå®¹å™¨åŒ–
- Docker Composeç¼–æ’
- Kuberneteséƒ¨ç½²ï¼ˆå¯é€‰ï¼‰

### ç»ƒä¹ 5ï¼šRAGæ•ˆæœè¯„ä¼°ç³»ç»Ÿ

**ä»»åŠ¡**ï¼šæ„å»ºè‡ªåŠ¨åŒ–çš„RAGè¯„ä¼°å·¥å…·

**åŠŸèƒ½**ï¼š
1. å‡†å¤‡æµ‹è¯•æ•°æ®é›†
2. è‡ªåŠ¨åŒ–æµ‹è¯•æµç¨‹
3. è®¡ç®—è¯„ä¼°æŒ‡æ ‡
4. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
5. A/Bæµ‹è¯•å¯¹æ¯”

**è¯„ä¼°ç»´åº¦**ï¼š
- æ£€ç´¢è´¨é‡ï¼ˆPrecision, Recall, MRRï¼‰
- ç”Ÿæˆè´¨é‡ï¼ˆFaithfulness, Relevanceï¼‰
- ç«¯åˆ°ç«¯æ•ˆæœ
- ç”¨æˆ·åé¦ˆ

---

## å®æˆ˜é¡¹ç›®

### é¡¹ç›®1ï¼šä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ

**ç›®æ ‡**ï¼šæ„å»ºä¼ä¸šå†…éƒ¨æ–‡æ¡£æ™ºèƒ½é—®ç­”ç³»ç»Ÿ

**åŠŸèƒ½**ï¼š
- æ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼
- å®æ—¶æ›´æ–°çŸ¥è¯†åº“
- æƒé™æ§åˆ¶ï¼ˆä¸åŒéƒ¨é—¨ä¸åŒçŸ¥è¯†åº“ï¼‰
- æŸ¥è¯¢å†å²è®°å½•
- ç®¡ç†åå°ï¼ˆæ–‡æ¡£ç®¡ç†ã€ç”¨æˆ·ç®¡ç†ã€ç»Ÿè®¡åˆ†æï¼‰

**æŠ€æœ¯æ ˆ**ï¼š
- åç«¯ï¼šFastAPI + LangChain + Chroma
- å‰ç«¯ï¼šVue3 + Element Plus
- æ•°æ®åº“ï¼šPostgreSQL + Redis
- éƒ¨ç½²ï¼šDocker + Nginx

### é¡¹ç›®2ï¼šæŠ€æœ¯æ–‡æ¡£åŠ©æ‰‹

**ç›®æ ‡**ï¼šä¸ºå¼€æºé¡¹ç›®æ„å»ºæ–‡æ¡£é—®ç­”ç³»ç»Ÿ

**åŠŸèƒ½**ï¼š
- çˆ¬å–GitHubæ–‡æ¡£
- ä»£ç å’Œæ–‡æ¡£æ··åˆæ£€ç´¢
- ä»£ç ç¤ºä¾‹æå–
- ç›¸å…³é—®é¢˜æ¨è
- å¤šè¯­è¨€æ”¯æŒ

**ç‰¹è‰²åŠŸèƒ½**ï¼š
- ä»£ç å—é«˜äº®
- APIæ–‡æ¡£ç»“æ„åŒ–
- ç‰ˆæœ¬åˆ‡æ¢ï¼ˆv1.0, v2.0ï¼‰
- è´¡çŒ®è€…ä¿¡æ¯å±•ç¤º

### é¡¹ç›®3ï¼šæ™ºèƒ½å­¦ä¹ åŠ©æ‰‹

**ç›®æ ‡**ï¼šåŸºäºè¯¾ç¨‹èµ„æ–™æ„å»ºå­¦ä¹ é—®ç­”ç³»ç»Ÿ

**åŠŸèƒ½**ï¼š
- æ”¯æŒè§†é¢‘å­—å¹•æ£€ç´¢
- PPTå†…å®¹æå–
- çŸ¥è¯†ç‚¹å…³è”
- å­¦ä¹ è·¯å¾„æ¨è
- ç»ƒä¹ é¢˜ç”Ÿæˆ

**åˆ›æ–°ç‚¹**ï¼š
- ç»“åˆå­¦ä¹ è¿›åº¦æ¨è
- è¯†åˆ«è–„å¼±çŸ¥è¯†ç‚¹
- ç”Ÿæˆä¸ªæ€§åŒ–å­¦ä¹ è®¡åˆ’
- å¤šè½®å¯¹è¯æ•™å­¦

---

## å­¦ä¹ èµ„æº

### æ¨èé˜…è¯»

1. **LangChain RAGæ•™ç¨‹**
   - å®˜æ–¹æ–‡æ¡£
   - æœ€ä½³å®è·µ
   - ç¤ºä¾‹ä»£ç 

2. **å‘é‡æ•°æ®åº“æ–‡æ¡£**
   - Chroma: https://docs.trychroma.com
   - FAISS: https://github.com/facebookresearch/faiss
   - Pinecone: https://docs.pinecone.io

3. **RAGè¯„ä¼°æ¡†æ¶**
   - RAGAS: https://docs.ragas.io
   - TruLens: https://www.trulens.org

### å®è·µå¹³å°

- **LangSmith**: RAGè°ƒè¯•å’Œè¯„ä¼°
- **HuggingFace**: å¼€æºEmbeddingæ¨¡å‹
- **Pinecone**: æ‰˜ç®¡å‘é‡æ•°æ®åº“

### ç¤¾åŒºèµ„æº

- **Discord**: LangChainç¤¾åŒº
- **GitHub**: RAGå¼€æºé¡¹ç›®
- **è®ºæ–‡**: arXivä¸Šçš„RAGç ”ç©¶

---

**ä¸‹ä¸€ç« ï¼š[AI Agentæ™ºèƒ½ä½“ â†’](chapter-05)**
