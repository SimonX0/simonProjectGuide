# å®æˆ˜é¡¹ç›® {#-é™„å½•c2024-2026ä¼ä¸šçº§aiå®æˆ˜é¡¹ç›®}

## æœ¬ç« å¯¼è¯»

æ­å–œä½ æ¥åˆ°æœ€åä¸€ç« ï¼æœ¬ç« å°†é€šè¿‡3ä¸ªå®Œæ•´çš„å®æˆ˜é¡¹ç›®ï¼Œç»¼åˆè¿ç”¨å‰é¢å­¦åˆ°çš„çŸ¥è¯†ï¼Œæ„å»ºç”Ÿäº§çº§çš„AIåº”ç”¨ã€‚

**å­¦ä¹ ç›®æ ‡**ï¼š
- ç»¼åˆè¿ç”¨LLMã€LangChainã€RAGã€Agentç­‰æŠ€æœ¯
- æ„å»ºå®Œæ•´çš„AIåº”ç”¨ç³»ç»Ÿ
- æŒæ¡é¡¹ç›®éƒ¨ç½²å’Œä¼˜åŒ–æŠ€å·§
- ç§¯ç´¯å®æˆ˜ç»éªŒ

**é¡¹ç›®æ¦‚è§ˆ**ï¼š
1. **æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ** - RAGåº”ç”¨
2. **ä»£ç åŠ©æ‰‹Agent** - Agentåº”ç”¨
3. **ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹** - ç»¼åˆåº”ç”¨

---

## é¡¹ç›®1ï¼šæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

### é¡¹ç›®æ¦‚è¿°

**åŠŸèƒ½**ï¼š
- ä¸Šä¼ PDF/Word/TXTæ–‡æ¡£
- è‡ªåŠ¨æ„å»ºå‘é‡æ•°æ®åº“
- æ™ºèƒ½é—®ç­”ï¼Œæ”¯æŒæº¯æº
- å¤šæ–‡æ¡£ç®¡ç†

**æŠ€æœ¯æ ˆ**ï¼š
- FastAPIï¼ˆWebæ¡†æ¶ï¼‰
- LangChainï¼ˆAIæ¡†æ¶ï¼‰
- Chromaï¼ˆå‘é‡æ•°æ®åº“ï¼‰
- Streamlitï¼ˆå‰ç«¯ç•Œé¢ï¼‰

### é¡¹ç›®ç»“æ„

```
document-qa-system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPIä¸»ç¨‹åº
â”‚   â”œâ”€â”€ models.py            # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ rag.py               # RAGæ ¸å¿ƒé€»è¾‘
â”‚   â””â”€â”€ config.py            # é…ç½®æ–‡ä»¶
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/           # å­˜å‚¨ä¸Šä¼ çš„æ–‡æ¡£
â”œâ”€â”€ vector_db/
â”‚   â””â”€â”€ chroma/              # å‘é‡æ•°æ®åº“
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py     # Streamlitç•Œé¢
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### åç«¯å®ç°

**1. é…ç½®æ–‡ä»¶ (`app/config.py`)**

```python
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    # APIé…ç½®
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_MODEL = "gpt-3.5-turbo"
    OPENAI_EMBEDDINGS = "text-embedding-3-small"

    # æ•°æ®åº“é…ç½®
    CHROMA_PERSIST_DIR = "./vector_db/chroma"

    # æ–‡æ¡£é…ç½®
    UPLOAD_DIR = "./data/documents"
    ALLOWED_EXTENSIONS = {".pdf", ".txt", ".md", ".docx"}
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

    # RAGé…ç½®
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    TOP_K_RETRIEVAL = 3
```

**2. RAGæ ¸å¿ƒé€»è¾‘ (`app/rag.py`)**

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.document_loaders import (
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
    Docx2txtLoader
)
from pathlib import Path
from typing import List, Optional
import shutil

class DocumentQA:
    """æ–‡æ¡£é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, collection_name: str = "default"):
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=Config.OPENAI_API_KEY
        )
        self.llm = ChatOpenAI(
            model=Config.OPENAI_MODEL,
            openai_api_key=Config.OPENAI_API_KEY,
            temperature=0
        )

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vectorstore = Chroma(
            collection_name=collection_name,
            persist_directory=Config.CHROMA_PERSIST_DIR,
            embedding_function=self.embeddings
        )

        # åˆ›å»ºQAé“¾
        self.qa_chain = self._create_qa_chain()

    def _create_qa_chain(self):
        """åˆ›å»ºQAé“¾"""
        prompt_template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œæ˜ç¡®è¯´æ˜
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´
4. æ ‡æ³¨ä¿¡æ¯æ¥æº

å›ç­”ï¼š"""

        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )

        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": Config.TOP_K_RETRIEVAL}
            ),
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )

    def add_document(self, file_path: str) -> dict:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        try:
            # 1. æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
            ext = Path(file_path).suffix.lower()

            loaders = {
                ".pdf": PyPDFLoader,
                ".txt": TextLoader,
                ".md": UnstructuredMarkdownLoader,
                ".docx": Docx2txtLoader
            }

            if ext not in loaders:
                return {"success": False, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"}

            loader = loaders[ext](file_path)
            documents = loader.load()

            # 2. åˆ†å‰²æ–‡æ¡£
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=Config.CHUNK_SIZE,
                chunk_overlap=Config.CHUNK_OVERLAP
            )
            splits = text_splitter.split_documents(documents)

            # 3. æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            for split in splits:
                split.metadata["source"] = Path(file_path).name

            self.vectorstore.add_documents(splits)

            return {
                "success": True,
                "message": f"æˆåŠŸæ·»åŠ  {len(splits)} ä¸ªæ–‡æ¡£å—",
                "chunks": len(splits)
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def ask(self, question: str) -> dict:
        """æé—®"""
        try:
            result = self.qa_chain({"query": question})

            # æå–æ¥æº
            sources = [
                {
                    "source": doc.metadata.get("source", "Unknown"),
                    "page": doc.metadata.get("page", 0),
                    "content": doc.page_content[:200] + "..."
                }
                for doc in result["source_documents"]
            ]

            return {
                "success": True,
                "answer": result["result"],
                "sources": sources
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def delete_collection(self):
        """åˆ é™¤å½“å‰é›†åˆ"""
        self.vectorstore.delete_collection()
        self.vectorstore = Chroma(
            collection_name=self.collection_name,
            persist_directory=Config.CHROMA_PERSIST_DIR,
            embedding_function=self.embeddings
        )

    def get_stats(self) -> dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        collection = self.vectorstore._collection
        return {
            "total_documents": collection.count(),
            "collection_name": self.collection_name
        }
```

**3. FastAPIä¸»ç¨‹åº (`app/main.py`)**

```python
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
import shutil
from pathlib import Path

from .rag import DocumentQA
from .config import Config

app = FastAPI(title="æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å­˜å‚¨QAå®ä¾‹
qa_instances = {}

class Question(BaseModel):
    question: str
    collection_name: str = "default"

@app.on_event("startup")
async def startup():
    """å¯åŠ¨æ—¶åˆ›å»ºä¸Šä¼ ç›®å½•"""
    Path(Config.UPLOAD_DIR).mkdir(parents=True, exist_ok=True)
    Path(Config.CHROMA_PERSIST_DIR).mkdir(parents=True, exist_ok=True)

@app.get("/")
async def root():
    return {"message": "æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»ŸAPI"}

@app.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    collection_name: str = "default"
):
    """ä¸Šä¼ æ–‡æ¡£"""
    # éªŒè¯æ–‡ä»¶æ ¼å¼
    ext = Path(file.filename).suffix.lower()
    if ext not in Config.ALLOWED_EXTENSIONS:
        raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")

    # ä¿å­˜æ–‡ä»¶
    file_path = Path(Config.UPLOAD_DIR) / f"{collection_name}_{file.filename}"
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    # æ·»åŠ åˆ°çŸ¥è¯†åº“
    qa = qa_instances.get(collection_name)
    if not qa:
        qa = DocumentQA(collection_name)
        qa_instances[collection_name] = qa

    result = qa.add_document(str(file_path))

    return result

@app.post("/ask")
async def ask_question(req: Question):
    """æé—®"""
    qa = qa_instances.get(req.collection_name)
    if not qa:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")

    result = qa.ask(req.question)
    return result

@app.get("/stats/{collection_name}")
async def get_stats(collection_name: str):
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    qa = qa_instances.get(collection_name)
    if not qa:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")

    return qa.get_stats()

@app.delete("/collection/{collection_name}")
async def delete_collection(collection_name: str):
    """åˆ é™¤çŸ¥è¯†åº“"""
    qa = qa_instances.get(collection_name)
    if qa:
        qa.delete_collection()
        del qa_instances[collection_name]

    return {"message": "çŸ¥è¯†åº“å·²åˆ é™¤"}
```

### å‰ç«¯ç•Œé¢

**Streamlitç•Œé¢ (`ui/streamlit_app.py`)**

```python
import streamlit as st
import requests
from pathlib import Path

# é…ç½®
API_URL = "http://localhost:8000"
st.set_page_config(
    page_title="æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ“š",
    layout="wide"
)

# ä¾§è¾¹æ 
st.sidebar.title("ğŸ“š æ–‡æ¡£ç®¡ç†")

# ä¸Šä¼ æ–‡æ¡£
uploaded_file = st.sidebar.file_uploader(
    "ä¸Šä¼ æ–‡æ¡£",
    type=["pdf", "txt", "md", "docx"]
)

collection_name = st.sidebar.text_input("çŸ¥è¯†åº“åç§°", value="default")

if uploaded_file and st.sidebar.button("æ·»åŠ åˆ°çŸ¥è¯†åº“"):
    with st.sidebar.spinner("å¤„ç†ä¸­..."):
        files = {"file": uploaded_file}
        response = requests.post(
            f"{API_URL}/upload",
            files=files,
            params={"collection_name": collection_name}
        )

        if response.status_code == 200:
            st.sidebar.success(response.json()["message"])
        else:
            st.sidebar.error("ä¸Šä¼ å¤±è´¥")

# ç»Ÿè®¡ä¿¡æ¯
stats_response = requests.get(f"{API_URL}/stats/{collection_name}")
if stats_response.status_code == 200:
    stats = stats_response.json()
    st.sidebar.metric("æ–‡æ¡£å—æ•°", stats["total_documents"])

# ä¸»ç•Œé¢
st.title("ğŸ¤– æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")

# é—®ç­”åŒºåŸŸ
question = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š", placeholder="ä¾‹å¦‚ï¼šæ–‡æ¡£ä¸­æåˆ°äº†ä»€ä¹ˆ...")

if st.button("æé—®") and question:
    with st.spinner("æ€è€ƒä¸­..."):
        response = requests.post(
            f"{API_URL}/ask",
            json={
                "question": question,
                "collection_name": collection_name
            }
        )

        if response.status_code == 200:
            result = response.json()

            st.markdown("### ğŸ“ å›ç­”")
            st.write(result["answer"])

            if "sources" in result and result["sources"]:
                st.markdown("### ğŸ“š ä¿¡æ¯æ¥æº")
                for i, source in enumerate(result["sources"], 1):
                    with st.expander(f"æ¥æº {i}: {source['source']}"):
                        st.write(source["content"])
        else:
            st.error("æŸ¥è¯¢å¤±è´¥")

# ä½¿ç”¨è¯´æ˜
with st.expander("ğŸ’¡ ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    1. **ä¸Šä¼ æ–‡æ¡£**ï¼šåœ¨å·¦ä¾§ä¸Šä¼ PDFã€TXTã€Markdownæˆ–Wordæ–‡æ¡£
    2. **æ„å»ºçŸ¥è¯†åº“**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ–‡æ¡£å¹¶å»ºç«‹ç´¢å¼•
    3. **æé—®**ï¼šè¾“å…¥é—®é¢˜ï¼Œç³»ç»Ÿä¼šåŸºäºæ–‡æ¡£å†…å®¹å›ç­”
    4. **æº¯æº**ï¼šæŸ¥çœ‹ç­”æ¡ˆçš„æ¥æºï¼Œç¡®ä¿å‡†ç¡®æ€§

    **æ”¯æŒçš„æ–‡ä»¶æ ¼å¼**ï¼š
    - PDFï¼ˆ.pdfï¼‰
    - æ–‡æœ¬ï¼ˆ.txtï¼‰
    - Markdownï¼ˆ.mdï¼‰
    - Wordæ–‡æ¡£ï¼ˆ.docxï¼‰
    """)
```

### éƒ¨ç½²å’Œè¿è¡Œ

```bash
# 1. å®‰è£…ä¾èµ–
pip install fastapi uvicorn streamlit langchain langchain-openai chromadb

# 2. å¯åŠ¨åç«¯
uvicorn app.main:app --reload --port 8000

# 3. å¯åŠ¨å‰ç«¯
streamlit run ui/streamlit_app.py

# 4. è®¿é—®
# å‰ç«¯ï¼šhttp://localhost:8501
# APIæ–‡æ¡£ï¼šhttp://localhost:8000/docs
```

---

## é¡¹ç›®2ï¼šä»£ç åŠ©æ‰‹Agent

### é¡¹ç›®æ¦‚è¿°

**åŠŸèƒ½**ï¼š
- ä»£ç ç”Ÿæˆå’Œä¼˜åŒ–
- Bugæ£€æµ‹å’Œä¿®å¤
- ä»£ç è§£é‡Š
- æŠ€æœ¯æ–‡æ¡£æŸ¥è¯¢

**æŠ€æœ¯æ ˆ**ï¼š
- LangChain Agents
- OpenAI API
- GitHub API
- Python ASTï¼ˆä»£ç åˆ†æï¼‰

### å®ç°ä»£ç 

```python
# code_agent.py
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.tools import Tool, StructuredTool
from langchain import hub
from pydantic import BaseModel, Field
import subprocess
import ast
import requests
import os

class CodeInput(BaseModel):
    """ä»£ç è¾“å…¥"""
    code: str = Field(description="è¦å¤„ç†çš„Pythonä»£ç ")
    question: str = Field(description="å…³äºä»£ç çš„é—®é¢˜")

class CodeAgent:
    """ä»£ç åŠ©æ‰‹Agent"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        self.tools = self._create_tools()
        self.agent = self._create_agent()

    def _create_tools(self):
        """åˆ›å»ºå·¥å…·é›†"""

        # 1. ä»£ç æ‰§è¡Œå·¥å…·
        def execute_code(code: str) -> str:
            """æ‰§è¡ŒPythonä»£ç """
            try:
                result = subprocess.run(
                    ["python", "-c", code],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                if result.returncode == 0:
                    return f"è¾“å‡ºï¼š\n{result.stdout}"
                else:
                    return f"é”™è¯¯ï¼š\n{result.stderr}"
            except Exception as e:
                return f"æ‰§è¡Œå¤±è´¥ï¼š{str(e)}"

        # 2. ä»£ç åˆ†æå·¥å…·
        def analyze_code(code: str) -> str:
            """åˆ†æä»£ç ç»“æ„"""
            try:
                tree = ast.parse(code)

                functions = []
                classes = []
                imports = []

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        functions.append(node.name)
                    elif isinstance(node, ast.ClassDef):
                        classes.append(node.name)
                    elif isinstance(node, (ast.Import, ast.ImportFrom)):
                        if isinstance(node, ast.Import):
                            imports.extend([alias.name for alias in node.names])
                        else:
                            imports.append(node.module)

                return f"""
                ä»£ç ç»“æ„åˆ†æï¼š
                - å‡½æ•°ï¼š{', '.join(functions) if functions else 'æ— '}
                - ç±»ï¼š{', '.join(classes) if classes else 'æ— '}
                - å¯¼å…¥æ¨¡å—ï¼š{', '.join(set(imports)) if imports else 'æ— '}
                """
            except Exception as e:
                return f"åˆ†æå¤±è´¥ï¼š{str(e)}"

        # 3. ä»£ç æ ¼å¼åŒ–å·¥å…·
        def format_code(code: str) -> str:
            """æ ¼å¼åŒ–ä»£ç """
            try:
                import black
                formatted = black.format_str(code, mode=black.Mode())
                return f"æ ¼å¼åŒ–åçš„ä»£ç ï¼š\n{formatted}"
            except:
                return "æ ¼å¼åŒ–å¤±è´¥ï¼Œè¯·ç¡®ä¿ä»£ç è¯­æ³•æ­£ç¡®"

        # 4. ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å·¥å…·
        def generate_tests(code: str) -> str:
            """ä¸ºä»£ç ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
            prompt = f"""
            ä¸ºä»¥ä¸‹ä»£ç ç”Ÿæˆå•å…ƒæµ‹è¯•ï¼ˆä½¿ç”¨pytestï¼‰ï¼š
            {code}

            è¦æ±‚ï¼š
            - æµ‹è¯•æ­£å¸¸æƒ…å†µ
            - æµ‹è¯•è¾¹ç•Œæƒ…å†µ
            - æµ‹è¯•å¼‚å¸¸æƒ…å†µ
            """
            # è°ƒç”¨LLMç”Ÿæˆ
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(model="gpt-3.5-turbo")
            return llm.predict(prompt)

        # 5. GitHubæœç´¢å·¥å…·
        def search_github(query: str) -> str:
            """æœç´¢GitHubä»£ç """
            # è¿™é‡Œéœ€è¦GitHub Token
            token = os.getenv("GITHUB_TOKEN")
            if not token:
                return "éœ€è¦è®¾ç½®GITHUB_TOKENç¯å¢ƒå˜é‡"

            url = "https://api.github.com/search/code"
            params = {"q": query}
            headers = {"Authorization": f"token {token}"}

            response = requests.get(url, params=params, headers=headers)
            if response.status_code == 200:
                data = response.json()
                results = data.get("items", [])[:5]

                output = []
                for item in results:
                    output.append(f"""
                    - ä»“åº“ï¼š{item['repository']['full_name']}
                    - æ–‡ä»¶ï¼š{item['path']}
                    - URLï¼š{item['html_url']}
                    """)

                return "\n".join(output)
            else:
                return "æœç´¢å¤±è´¥"

        # åˆ›å»ºå·¥å…·åˆ—è¡¨
        return [
            Tool(
                name="ExecuteCode",
                func=execute_code,
                description="æ‰§è¡ŒPythonä»£ç å¹¶è¿”å›ç»“æœã€‚è¾“å…¥ï¼šå®Œæ•´çš„Pythonä»£ç å­—ç¬¦ä¸²"
            ),
            Tool(
                name="AnalyzeCode",
                func=analyze_code,
                description="åˆ†æPythonä»£ç çš„ç»“æ„ï¼Œæå–å‡½æ•°ã€ç±»å’Œå¯¼å…¥ä¿¡æ¯ã€‚è¾“å…¥ï¼šPythonä»£ç "
            ),
            Tool(
                name="FormatCode",
                func=format_code,
                description="ä½¿ç”¨Blackæ ¼å¼åŒ–Pythonä»£ç ã€‚è¾“å…¥ï¼šéœ€è¦æ ¼å¼åŒ–çš„ä»£ç "
            ),
            Tool(
                name="GenerateTests",
                func=generate_tests,
                description="ä¸ºPythonä»£ç ç”Ÿæˆpytestæµ‹è¯•ç”¨ä¾‹ã€‚è¾“å…¥ï¼šéœ€è¦æµ‹è¯•çš„ä»£ç "
            ),
            Tool(
                name="SearchGitHub",
                func=search_github,
                description="åœ¨GitHubä¸Šæœç´¢ä»£ç ç¤ºä¾‹ã€‚è¾“å…¥ï¼šæœç´¢å…³é”®è¯"
            )
        ]

    def _create_agent(self):
        """åˆ›å»ºAgent"""
        prompt = hub.pull("hwchase17/openai-functions-agent")

        agent = create_openai_functions_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )

        return AgentExecutor(
            agent=agent,
            tools=self.tools,
            verbose=True,
            max_iterations=10
        )

    def assist(self, request: str) -> str:
        """ä»£ç åŠ©æ‰‹"""
        result = self.agent.invoke({"input": request})
        return result["output"]

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    agent = CodeAgent()

    # ç¤ºä¾‹1ï¼šä»£ç è§£é‡Š
    print(agent.assist("""
    è§£é‡Šä»¥ä¸‹ä»£ç çš„ä½œç”¨ï¼š
    def quicksort(arr):
        if len(arr) <= 1:
            return arr
        pivot = arr[len(arr) // 2]
        left = [x for x in arr if x < pivot]
        middle = [x for x in arr if x == pivot]
        right = [x for x in arr if x > pivot]
        return quicksort(left) + middle + quicksort(right)
    """))

    # ç¤ºä¾‹2ï¼šBugæ£€æµ‹
    print(agent.assist("""
    æ£€æŸ¥ä»¥ä¸‹ä»£ç æ˜¯å¦æœ‰Bugï¼š
    for i in range(len(arr)):
        if arr[i] > arr[i+1]:
            arr[i], arr[i+1] = arr[i+1], arr[i]
    """))

    # ç¤ºä¾‹3ï¼šä»£ç ä¼˜åŒ–
    print(agent.assist("""
    ä¼˜åŒ–ä»¥ä¸‹ä»£ç çš„æ€§èƒ½ï¼š
    squares = []
    for i in range(1000000):
        squares.append(i ** 2)
    """))
```

---

## é¡¹ç›®3ï¼šä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹

### é¡¹ç›®æ¦‚è¿°

**åŠŸèƒ½**ï¼š
- ç¬”è®°ç®¡ç†ï¼ˆå¢åˆ æ”¹æŸ¥ï¼‰
- æ™ºèƒ½é—®ç­”
- çŸ¥è¯†å…³è”
- å®šæœŸå¤ä¹ æé†’

**æŠ€æœ¯æ ˆ**ï¼š
- RAGï¼ˆçŸ¥è¯†æ£€ç´¢ï¼‰
- Agentï¼ˆä»»åŠ¡è‡ªåŠ¨åŒ–ï¼‰
- Notion APIï¼ˆç¬”è®°åŒæ­¥ï¼‰

### æ ¸å¿ƒå®ç°

```python
# knowledge_assistant.py
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.tools import Tool
from langchain.memory import ConversationBufferMemory
from datetime import datetime, timedelta
import json
from pathlib import Path

class KnowledgeAssistant:
    """ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹"""

    def __init__(self, knowledge_base_path: str = "./knowledge_base"):
        self.kb_path = Path(knowledge_base_path)
        self.kb_path.mkdir(parents=True, exist_ok=True)

        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            persist_directory=str(self.kb_path / "chroma"),
            embedding_function=self.embeddings
        )

        self.memory = ConversationBufferMemory()
        self.agent = self._create_agent()

    def _create_agent(self):
        """åˆ›å»ºAgent"""

        # å·¥å…·1ï¼šæ·»åŠ ç¬”è®°
        def add_note(title: str, content: str, tags: list = None) -> str:
            """æ·»åŠ ç¬”è®°åˆ°çŸ¥è¯†åº“"""
            note = {
                "title": title,
                "content": content,
                "tags": tags or [],
                "created_at": datetime.now().isoformat()
            }

            # ä¿å­˜åˆ°æ–‡ä»¶
            note_path = self.kb_path / "notes" / f"{title}.json"
            note_path.parent.mkdir(exist_ok=True)
            with open(note_path, "w", encoding="utf-8") as f:
                json.dump(note, f, ensure_ascii=False, indent=2)

            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            from langchain.schema import Document
            doc = Document(
                page_content=content,
                metadata={"title": title, "tags": tags or [], "source": str(note_path)}
            )
            self.vectorstore.add_documents([doc])

            return f"ç¬”è®°'{title}'å·²æ·»åŠ ï¼Œæ ‡ç­¾ï¼š{', '.join(tags or [])}"

        # å·¥å…·2ï¼šæœç´¢ç¬”è®°
        def search_notes(query: str, top_k: int = 5) -> str:
            """æœç´¢ç›¸å…³ç¬”è®°"""
            results = self.vectorstore.similarity_search(query, k=top_k)

            output = []
            for i, doc in enumerate(results, 1):
                output.append(f"""
                ç¬”è®°{i}ï¼š{doc.metadata.get('title', 'Untitled')}
                æ ‡ç­¾ï¼š{', '.join(doc.metadata.get('tags', []))}
                å†…å®¹ï¼š{doc.page_content[:200]}...
                """)

            return "\n".join(output) if output else "æœªæ‰¾åˆ°ç›¸å…³ç¬”è®°"

        # å·¥å…·3ï¼šåˆ—å‡ºæ‰€æœ‰ç¬”è®°
        def list_notes() -> str:
            """åˆ—å‡ºæ‰€æœ‰ç¬”è®°"""
            notes_dir = self.kb_path / "notes"
            if not notes_dir.exists():
                return "çŸ¥è¯†åº“ä¸ºç©º"

            notes = list(notes_dir.glob("*.json"))
            output = [f"å…±æœ‰ {len(notes)} æ¡ç¬”è®°ï¼š"]

            for note_path in notes:
                with open(note_path, "r", encoding="utf-8") as f:
                    note = json.load(f)
                output.append(f"- {note['title']}ï¼ˆæ ‡ç­¾ï¼š{', '.join(note['tags'])}ï¼‰")

            return "\n".join(output)

        # å·¥å…·4ï¼šç”Ÿæˆå¤ä¹ è®¡åˆ’
        def create_review_plan(days: int = 7) -> str:
            """ç”Ÿæˆå¤ä¹ è®¡åˆ’"""
            # ä½¿ç”¨é—´éš”é‡å¤ç®—æ³•
            notes_dir = self.kb_path / "notes"
            notes = list(notes_dir.glob("*.json"))

            plan = []
            today = datetime.now()

            for note_path in notes:
                with open(note_path, "r", encoding="utf-8") as f:
                    note = json.load(f)

                # ç®€å•çš„é—´éš”é‡å¤é€»è¾‘
                created = datetime.fromisoformat(note['created_at'])
                days_since = (today - created).days

                if days_since >= days:
                    review_date = today + timedelta(days=1)
                    plan.append(f"{review_date.strftime('%Y-%m-%d')}: å¤ä¹ ã€Š{note['title']}ã€‹")

            return "å¤ä¹ è®¡åˆ’ï¼š\n" + "\n".join(plan) if plan else "æš‚æ— éœ€è¦å¤ä¹ çš„å†…å®¹"

        # åˆ›å»ºå·¥å…·
        tools = [
            Tool(
                name="AddNote",
                func=lambda x: add_note(**json.loads(x)),
                description="æ·»åŠ ç¬”è®°åˆ°çŸ¥è¯†åº“ã€‚è¾“å…¥JSONæ ¼å¼ï¼š{'title': 'æ ‡é¢˜', 'content': 'å†…å®¹', 'tags': ['æ ‡ç­¾1', 'æ ‡ç­¾2']}"
            ),
            Tool(
                name="SearchNotes",
                func=search_notes,
                description="åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç¬”è®°ã€‚è¾“å…¥ï¼šæœç´¢å…³é”®è¯"
            ),
            Tool(
                name="ListNotes",
                func=list_notes,
                description="åˆ—å‡ºæ‰€æœ‰ç¬”è®°"
            ),
            Tool(
                name="CreateReviewPlan",
                func=lambda x: create_review_plan(int(x) if x else 7),
                description="ç”Ÿæˆå¤ä¹ è®¡åˆ’ã€‚è¾“å…¥ï¼šé—´éš”å¤©æ•°ï¼ˆé»˜è®¤7å¤©ï¼‰"
            )
        ]

        # åˆ›å»ºAgent
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        from langchain import hub
        prompt = hub.pull("hwchase17/openai-functions-agent")

        agent = create_openai_functions_agent(
            llm=llm,
            tools=tools,
            prompt=prompt
        )

        return AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            memory=self.memory
        )

    def chat(self, message: str) -> str:
        """å¯¹è¯"""
        result = self.agent.invoke({"input": message})
        return result["output"]

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    assistant = KnowledgeAssistant()

    # ç¤ºä¾‹å¯¹è¯
    print(assistant.chat("å¸®æˆ‘æ·»åŠ ä¸€æ¡Pythonè£…é¥°å™¨çš„ç¬”è®°"))
    print(assistant.chat("æœç´¢å…³äºæœºå™¨å­¦ä¹ çš„ç¬”è®°"))
    print(assistant.chat("ç”Ÿæˆæœ¬å‘¨çš„å¤ä¹ è®¡åˆ’"))
```

---

## é¡¹ç›®éƒ¨ç½²å’Œä¼˜åŒ–

### Dockeréƒ¨ç½²

**Dockerfile**:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000 8501

# å¯åŠ¨å‘½ä»¤
CMD ["sh", "-c", "uvicorn app.main:app --host 0.0.0.0 --port 8000 & streamlit run ui/streamlit_app.py --server.port 8501"]
```

**docker-compose.yml**:

```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
      - "8501:8501"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./data:/app/data
      - ./vector_db:/app/vector_db
    restart: unless-stopped
```

### æ€§èƒ½ä¼˜åŒ–

```python
# 1. ä½¿ç”¨ç¼“å­˜
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_embedding(text: str):
    return embeddings.embed_query(text)

# 2. æ‰¹é‡å¤„ç†
def batch_add_documents(docs: list, batch_size=100):
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i+batch_size]
        vectorstore.add_documents(batch)

# 3. å¼‚æ­¥å¤„ç†
import asyncio
from langchain.runnables import RunnableConfig

async def async_ask(question: str):
    config = RunnableConfig(callbacks=[...])
    return await rag_chain.ainvoke(question, config)
```

### å®‰å…¨å»ºè®®

```python
# 1. APIå¯†é’¥ç®¡ç†
import os
from dotenv import load_dotenv
load_dotenv()

# 2. è¾“å…¥éªŒè¯
from pydantic import validator

class Question(BaseModel):
    question: str

    @validator('question')
    def question_length(cls, v):
        if len(v) > 1000:
            raise ValueError('é—®é¢˜é•¿åº¦ä¸èƒ½è¶…è¿‡1000å­—ç¬¦')
        return v

# 3. é€Ÿç‡é™åˆ¶
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/ask")
@limiter.limit("10/minute")
async def ask_question(req: Question):
    ...
```

---

## æœ¬ç« å°ç»“

### é¡¹ç›®å›é¡¾

âœ… **é¡¹ç›®1ï¼šæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ**
- å®Œæ•´çš„RAGå®ç°
- Webç•Œé¢éƒ¨ç½²
- å¤šæ ¼å¼æ–‡æ¡£æ”¯æŒ

âœ… **é¡¹ç›®2ï¼šä»£ç åŠ©æ‰‹Agent**
- å·¥å…·å®šä¹‰å’Œä½¿ç”¨
- ä»£ç åˆ†æèƒ½åŠ›
- GitHubé›†æˆ

âœ… **é¡¹ç›®3ï¼šä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹**
- RAG + Agentç»“åˆ
- ç¬”è®°ç®¡ç†
- å¤ä¹ è®¡åˆ’

### å­¦ä¹ æˆæœ

æ­å–œä½ ï¼é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å·²ç»èƒ½å¤Ÿï¼š

- âœ… ç‹¬ç«‹æ„å»ºå®Œæ•´çš„AIåº”ç”¨
- âœ… é›†æˆå¤šç§AIæŠ€æœ¯
- âœ… éƒ¨ç½²ç”Ÿäº§çº§ç³»ç»Ÿ
- âœ… ä¼˜åŒ–æ€§èƒ½å’Œå®‰å…¨æ€§

---

## ä¸‹ä¸€æ­¥å­¦ä¹ 

### æ¨èèµ„æº

**æ¡†æ¶å’Œå·¥å…·**ï¼š
- LangChainå®˜æ–¹æ–‡æ¡£ï¼šhttps://python.langchain.com
- LlamaIndexï¼šhttps://docs.llamaindex.ai
- Haystackï¼šhttps://haystack.deepset.ai

**è¿›é˜¶ä¸»é¢˜**ï¼š
- å¤šæ¨¡æ€AIï¼ˆå›¾åƒã€éŸ³é¢‘ï¼‰
- å¾®è°ƒå¤§æ¨¡å‹
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- AIåº”ç”¨å®‰å…¨

**å®è·µå»ºè®®**ï¼š
1. ä»å°é¡¹ç›®å¼€å§‹ï¼Œé€æ­¥æ‰©å±•
2. å‚ä¸å¼€æºé¡¹ç›®
3. åŠ å…¥AIå¼€å‘è€…ç¤¾åŒº
4. æŒç»­å­¦ä¹ æ–°æŠ€æœ¯

---

---

# é™„å½•Cï¼š2024-2026ä¼ä¸šçº§AIå®æˆ˜é¡¹ç›®

> **2024-2026 AIæŠ€æœ¯è¶‹åŠ¿**
>
> æ ¹æ®æœ€æ–°æŠ€æœ¯åˆ†æï¼ŒAIåº”ç”¨å¼€å‘æ­£åœ¨ç»å†ä»å®éªŒåˆ°ç”Ÿäº§çš„è½¬å˜ï¼š
> - **2024**ï¼šAIåº”ç”¨çš„çˆ†å‘å…ƒå¹´
> - **2025**ï¼šAgentæ¡†æ¶æ ‡å‡†åŒ–ï¼ŒMulti-Agentç³»ç»Ÿæˆä¸ºä¸»æµ
> - **2026**ï¼šAI Agentæˆä¸ºä¼ä¸šçº§åº”ç”¨çš„æ ‡å‡†é…ç½®
>
> åŸºäºè¿™äº›è¶‹åŠ¿ï¼Œæˆ‘ä»¬æ–°å¢ **4ä¸ªä¼ä¸šçº§AIå®æˆ˜é¡¹ç›®**ï¼Œæ¶µç›–Multi-Agentã€ç”Ÿäº§çº§RAGã€Agent+RAGç»“åˆç­‰å‰æ²¿æŠ€æœ¯ã€‚

---

## é¡¹ç›®4ï¼šMulti-Agentåä½œç³»ç»Ÿ

### æŠ€æœ¯æ ˆï¼ˆ2024-2026ä¸»æµï¼‰

åŸºäº[Multi-Agentæ¡†æ¶é¢„æµ‹](https://medium.com/@akaivdo/multi-agent-frameworks-in-2025-and-2026-predictions-eaf7a5006f24)ï¼š

```
ğŸ Python 3.11+
ğŸ¤– LangGraphï¼ˆå¤æ‚Agentç¼–æ’ï¼‰
ğŸ”„ AutoGenï¼ˆMulti-Agentåä½œï¼‰
ğŸ“Š CrewAIï¼ˆè§’è‰²-based Agentï¼‰
ğŸ” Tavilyï¼ˆAIæœç´¢ï¼‰
ğŸŒ FastAPI
ğŸ¨ Chainlitï¼ˆå¯¹è¯ç•Œé¢ï¼‰
ğŸ¦™ Llama 3ï¼ˆæœ¬åœ°æ¨¡å‹ï¼‰
```

### é¡¹ç›®ç®€ä»‹

ä¸€ä¸ªå¤æ‚çš„Multi-Agentåä½œç³»ç»Ÿï¼Œæ¨¡æ‹ŸçœŸå®ä¼ä¸šçš„å†…å®¹ç”Ÿäº§æµç¨‹ã€‚

**æ ¸å¿ƒAgentè§’è‰²**ï¼š
```
ğŸ‘” é¡¹ç›®ç»ç†Agentï¼šåè°ƒå„Agentï¼Œç®¡ç†é¡¹ç›®è¿›åº¦
ğŸ”¬ ç ”ç©¶å‘˜Agentï¼šç½‘ç»œæœç´¢ã€ä¿¡æ¯æ”¶é›†ã€æ•°æ®åˆ†æ
âœï¸ ä½œè€…Agentï¼šå†…å®¹åˆ›ä½œã€æ–‡æ¡ˆç”Ÿæˆã€æ ¼å¼åŒ–
ğŸ¨ è®¾è®¡å¸ˆAgentï¼šå›¾åƒç”Ÿæˆã€è§†è§‰è®¾è®¡ã€æ’ç‰ˆ
ğŸ” å®¡æ ¸å‘˜Agentï¼šè´¨é‡æ£€æŸ¥ã€äº‹å®æ ¸æŸ¥ã€åˆè§„å®¡æŸ¥
ğŸ“Š åˆ†æå¸ˆAgentï¼šæ•°æ®åˆ†æã€æŠ¥å‘Šç”Ÿæˆã€è¶‹åŠ¿é¢„æµ‹
```

### é¡¹ç›®æ¶æ„

```python
# multi_agent_system.py
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from typing import TypedDict, Annotated, List
import operator

class AgentState(TypedDict):
    """Multi-Agentç³»ç»ŸçŠ¶æ€"""
    messages: Annotated[List[str], operator.add]
    current_step: str
    research_data: dict
    content: str
    review_result: dict
    final_output: dict

class MultiAgentSystem:
    """Multi-Agentåä½œç³»ç»Ÿ"""

    def __init__(self):
        self.pm_agent = ProjectManagerAgent()
        self.researcher = ResearcherAgent()
        self.writer = WriterAgent()
        self.designer = DesignerAgent()
        self.reviewer = ReviewerAgent()
        self.analyst = AnalystAgent()

        self.workflow = self._create_workflow()

    def _create_workflow(self) -> StateGraph:
        """åˆ›å»ºå·¥ä½œæµ"""
        workflow = StateGraph(AgentState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("plan", self._plan_node)
        workflow.add_node("research", self._research_node)
        workflow.add_node("write", self._write_node)
        workflow.add_node("design", self._design_node)
        workflow.add_node("review", self._review_node)
        workflow.add_node("analyze", self._analyze_node)

        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("plan")

        # æ·»åŠ è¾¹ï¼ˆå·¥ä½œæµç¨‹ï¼‰
        workflow.add_edge("plan", "research")
        workflow.add_edge("research", "write")
        workflow.add_edge("write", "design")
        workflow.add_edge("design", "review")

        # æ¡ä»¶è¾¹ï¼šå®¡æ ¸é€šè¿‡åˆ™åˆ†æï¼Œå¦åˆ™é‡æ–°å†™ä½œ
        workflow.add_conditional_edges(
            "review",
            self._should_proceed,
            {
                "analyze": "analyze",
                "rewrite": "write"
            }
        )

        workflow.add_edge("analyze", END)

        return workflow.compile()

    def _plan_node(self, state: AgentState) -> AgentState:
        """é¡¹ç›®ç»ç†ï¼šåˆ¶å®šè®¡åˆ’"""
        plan = self.pm_agent.create_plan(state["messages"][0])
        return {
            **state,
            "messages": state["messages"] + [f"è®¡åˆ’ï¼š{plan}"],
            "current_step": "planning"
        }

    def _research_node(self, state: AgentState) -> AgentState:
        """ç ”ç©¶å‘˜ï¼šæ”¶é›†ä¿¡æ¯"""
        research = self.researcher.research(state["messages"][0])
        return {
            **state,
            "research_data": research,
            "messages": state["messages"] + ["ç ”ç©¶å®Œæˆ"],
            "current_step": "research"
        }

    def _write_node(self, state: AgentState) -> AgentState:
        """ä½œè€…ï¼šåˆ›ä½œå†…å®¹"""
        content = self.writer.write(state["research_data"])
        return {
            **state,
            "content": content,
            "messages": state["messages"] + ["åˆ›ä½œå®Œæˆ"],
            "current_step": "writing"
        }

    def _design_node(self, state: AgentState) -> AgentState:
        """è®¾è®¡å¸ˆï¼šè§†è§‰è®¾è®¡"""
        design = self.designer.create_visuals(state["content"])
        return {
            **state,
            "messages": state["messages"] + [f"è®¾è®¡å®Œæˆï¼š{design}"],
            "current_step": "designing"
        }

    def _review_node(self, state: AgentState) -> AgentState:
        """å®¡æ ¸å‘˜ï¼šè´¨é‡æ£€æŸ¥"""
        review = self.reviewer.review(state["content"])
        return {
            **state,
            "review_result": review,
            "messages": state["messages"] + ["å®¡æ ¸å®Œæˆ"],
            "current_step": "reviewing"
        }

    def _analyze_node(self, state: AgentState) -> AgentState:
        """åˆ†æå¸ˆï¼šæ•°æ®åˆ†æ"""
        analysis = self.analyst.analyze(state)
        return {
            **state,
            "final_output": analysis,
            "messages": state["messages"] + ["åˆ†æå®Œæˆ"],
            "current_step": "analyzing"
        }

    def _should_proceed(self, state: AgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­"""
        if state["review_result"]["approved"]:
            return "analyze"
        else:
            return "rewrite"

    def run(self, user_request: str) -> dict:
        """è¿è¡ŒMulti-Agentç³»ç»Ÿ"""
        initial_state: AgentState = {
            "messages": [user_request],
            "current_step": "start",
            "research_data": {},
            "content": "",
            "review_result": {},
            "final_output": {}
        }

        result = self.workflow.invoke(initial_state)
        return result

# Agentå®ç°
class ProjectManagerAgent:
    """é¡¹ç›®ç»ç†Agent"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4-turbo-preview")

    def create_plan(self, request: str) -> str:
        """åˆ›å»ºé¡¹ç›®è®¡åˆ’"""
        # ä½¿ç”¨LLMç”Ÿæˆè¯¦ç»†è®¡åˆ’
        pass

class ResearcherAgent:
    """ç ”ç©¶å‘˜Agent"""

    def __init__(self):
        self.tools = [
            TavilySearch(max_results=5),
            WikipediaQueryRun(),
        ]
        self.llm = ChatOpenAI(model="gpt-4-turbo-preview")

    def research(self, topic: str) -> dict:
        """è¿›è¡Œæ·±åº¦ç ”ç©¶"""
        # ä½¿ç”¨æœç´¢å·¥å…·æ”¶é›†ä¿¡æ¯
        pass

class WriterAgent:
    """ä½œè€…Agent"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4-turbo-preview", temperature=0.8)

    def write(self, research_data: dict) -> str:
        """åŸºäºç ”ç©¶æ•°æ®åˆ›ä½œ"""
        pass

class DesignerAgent:
    """è®¾è®¡å¸ˆAgent"""

    def __init__(self):
        # DALL-E 3æˆ–Stable Diffusion
        pass

    def create_visuals(self, content: str) -> dict:
        """åˆ›å»ºè§†è§‰å†…å®¹"""
        pass

class ReviewerAgent:
    """å®¡æ ¸å‘˜Agent"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4-turbo-preview")

    def review(self, content: str) -> dict:
        """å®¡æ ¸å†…å®¹è´¨é‡"""
        pass

class AnalystAgent:
    """åˆ†æå¸ˆAgent"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-4-turbo-preview")

    def analyze(self, state: AgentState) -> dict:
        """åˆ†ææ•´ä½“ç»“æœ"""
        pass
```

### è¿è¡Œç¤ºä¾‹

```python
# example.py
from multi_agent_system import MultiAgentSystem

# åˆ›å»ºç³»ç»Ÿ
system = MultiAgentSystem()

# è¿è¡Œ
result = system.run(
    "åˆ›å»ºä¸€ç¯‡å…³äºé‡å­è®¡ç®—æœ€æ–°è¿›å±•çš„æŠ€æœ¯æ–‡ç« ï¼ŒåŒ…å«å›¾è¡¨å’Œæ•°æ®åˆ†æ"
)

# æŸ¥çœ‹ç»“æœ
print(result["final_output"])
```

---

## é¡¹ç›®5ï¼šç”Ÿäº§çº§RAGç³»ç»Ÿ

### æŠ€æœ¯æ ˆ

```
ğŸ” LangChain 0.2+
ğŸ“Š Pinecone/Weaviateï¼ˆå‘é‡æ•°æ®åº“ï¼‰
ğŸ¤– OpenAI/Claudeï¼ˆEmbeddingsï¼‰
ğŸŒ FastAPI
ğŸ¨ Streamlit
ğŸ“¦ pgvectorï¼ˆPostgreSQLå‘é‡æ‰©å±•ï¼‰
ğŸ”„ LangSmithï¼ˆç›‘æ§å’Œè°ƒè¯•ï¼‰
```

### æ ¸å¿ƒåŠŸèƒ½

```
ğŸ“š å¤šæºæ–‡æ¡£å¯¼å…¥ï¼ˆPDFã€Webã€Databaseï¼‰
ğŸ” æ··åˆæœç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰
ğŸ¯ æ™ºèƒ½åˆ†å—å’Œç´¢å¼•
ğŸ’¬ å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡
ğŸ“Š å¼•ç”¨æº¯æº
ğŸ” æƒé™æ§åˆ¶
ğŸ“ˆ æ€§èƒ½ç›‘æ§
ğŸš€ æµå¼å“åº”
```

### é¡¹ç›®æ¶æ„

```python
# production_rag/system.py
from langchain.vectorstores import Pinecone
from langchain.embeddings import OpenAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.chat_models import ChatOpenAI

class ProductionRAG:
    """ç”Ÿäº§çº§RAGç³»ç»Ÿ"""

    def __init__(self):
        # åˆå§‹åŒ–embeddings
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",
            chunk_size=1000
        )

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vectorstore = Pinecone(
            index_name="documents",
            embedding_function=self.embeddings
        )

        # åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0,
            streaming=True
        )

        # åˆå§‹åŒ–å¯¹è¯è®°å¿†
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )

        # åˆ›å»ºQAé“¾
        self.qa_chain = self._create_chain()

    def _create_chain(self):
        """åˆ›å»ºQAé“¾"""
        retriever = self.vectorstore.as_retriever(
            search_type="similarity_score_threshold",
            search_kwargs={
                "k": 5,
                "score_threshold": 0.7
            }
        )

        return ConversationalRetrievalChain.from_llm(
            llm=self.llm,
            retriever=retriever,
            memory=self.memory,
            return_source_documents=True,
            verbose=True
        )

    async def add_documents(self, documents: List[Document]):
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        # æ™ºèƒ½åˆ†å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separators=["\n\n", "\n", " ", ""]
        )

        splits = text_splitter.split_documents(documents)

        # æ‰¹é‡æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        await self.vectorstore.aadd_documents(splits)

    async def query(self, question: str) -> dict:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        result = await self.qa_chain.ainvoke({"question": question})

        # æå–æ¥æº
        sources = [
            {
                "content": doc.page_content[:200],
                "metadata": doc.metadata
            }
            for doc in result["source_documents"]
        ]

        return {
            "answer": result["answer"],
            "sources": sources,
            "chat_history": self.memory.chat_memory.messages
        }
```

### é«˜çº§ç‰¹æ€§

**1. æ··åˆæœç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰**

```python
from langchain.retrievers import BM25Retriever, EnsembleRetriever

class HybridSearchRAG(ProductionRAG):
    """æ··åˆæœç´¢RAGç³»ç»Ÿ"""

    def __init__(self):
        super().__init__()

        # å‘é‡æ£€ç´¢å™¨
        vector_retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 5}
        )

        # å…³é”®è¯æ£€ç´¢å™¨
        bm25_retriever = BM25Retriever.from_documents(
            documents=self.documents,
            k=5
        )

        # é›†æˆæ£€ç´¢å™¨
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[vector_retriever, bm25_retriever],
            weights=[0.5, 0.5]
        )
```

**2. é‡æ’åºï¼ˆRerankingï¼‰**

```python
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

class RerankingRAG(ProductionRAG):
    """å¸¦é‡æ’åºçš„RAGç³»ç»Ÿ"""

    def __init__(self):
        super().__init__()

        # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹
        self.reranker = HuggingFaceCrossEncoder(
            model_name="BAAI/bge-reranker-large"
        )

    async def query_with_reranking(self, question: str) -> dict:
        """æŸ¥è¯¢å¹¶é‡æ’åº"""
        # ç¬¬ä¸€é˜¶æ®µï¼šæ£€ç´¢
        docs = await self.vectorstore.asimilarity_search(question, k=20)

        # ç¬¬äºŒé˜¶æ®µï¼šé‡æ’åº
        reranked_docs = self.reranker.rank(
            query=question,
            documents=docs,
            top_k=5
        )

        # ç¬¬ä¸‰é˜¶æ®µï¼šç”Ÿæˆç­”æ¡ˆ
        answer = await self.llm.agenerate([
            f"åŸºäºä»¥ä¸‹æ–‡æ¡£å›ç­”é—®é¢˜ï¼š\n\n{reranked_docs}\n\né—®é¢˜ï¼š{question}"
        ])

        return {"answer": answer, "sources": reranked_docs}
```

**3. æŸ¥è¯¢æ‰©å±•**

```python
class QueryExpansionRAG(ProductionRAG):
    """æŸ¥è¯¢æ‰©å±•RAGç³»ç»Ÿ"""

    async def expand_query(self, query: str) -> List[str]:
        """æ‰©å±•æŸ¥è¯¢"""
        # ä½¿ç”¨LLMç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“
        prompt = f"""
        ç”Ÿæˆä»¥ä¸‹æŸ¥è¯¢çš„3ä¸ªä¸åŒç‰ˆæœ¬ï¼Œä»¥æ”¹å–„æœç´¢ç»“æœï¼š

        åŸå§‹æŸ¥è¯¢ï¼š{query}

        æ‰©å±•æŸ¥è¯¢ï¼š
        """

        response = await self.llm.ainvoke(prompt)
        return [query] + response.strip().split("\n")

    async def query_with_expansion(self, question: str) -> dict:
        """ä½¿ç”¨æŸ¥è¯¢æ‰©å±•"""
        # æ‰©å±•æŸ¥è¯¢
        expanded_queries = await self.expand_query(question)

        # å¯¹æ¯ä¸ªæŸ¥è¯¢è¿›è¡Œæ£€ç´¢
        all_docs = []
        for query in expanded_queries:
            docs = await self.vectorstore.asimilarity_search(query, k=3)
            all_docs.extend(docs)

        # å»é‡å’Œæ’åº
        unique_docs = self._deduplicate_and_rank(all_docs)

        # ç”Ÿæˆç­”æ¡ˆ
        answer = await self._generate_answer(question, unique_docs)

        return {"answer": answer, "sources": unique_docs}
```

---

## é¡¹ç›®6ï¼šAgent + RAG ç»“åˆç³»ç»Ÿ

### æŠ€æœ¯æ ˆ

```
ğŸ¤– LangGraphï¼ˆAgentç¼–æ’ï¼‰
ğŸ” RAGç³»ç»Ÿï¼ˆçŸ¥è¯†æ£€ç´¢ï¼‰
ğŸ› ï¸ Function Callingï¼ˆå·¥å…·è°ƒç”¨ï¼‰
ğŸ“Š Tavilyï¼ˆå®æ—¶æœç´¢ï¼‰
ğŸŒ FastAPI
ğŸ¨ Chainlit
```

### é¡¹ç›®ç®€ä»‹

ä¸€ä¸ªç»“åˆAgentå’ŒRAGçš„æ™ºèƒ½åŠ©æ‰‹ï¼Œæ—¢èƒ½ä½¿ç”¨å·¥å…·ï¼Œåˆèƒ½æ£€ç´¢çŸ¥è¯†åº“ã€‚

### æ ¸å¿ƒæ¶æ„

```python
# agent_rag_system.py
from langgraph.graph import StateGraph, END
from langchain.tools import Tool
from typing import TypedDict, List

class AgentRAGState(TypedDict):
    """Agent+RAGçŠ¶æ€"""
    messages: List[str]
    user_query: str
    rag_context: str
    tool_results: dict
    final_answer: str

class AgentRAGSystem:
    """Agent+RAGç»“åˆç³»ç»Ÿ"""

    def __init__(self):
        self.rag_system = ProductionRAG()
        self.tools = self._create_tools()
        self.workflow = self._create_workflow()

    def _create_tools(self) -> List[Tool]:
        """åˆ›å»ºå·¥å…·é›†"""
        tools = [
            Tool(
                name="KnowledgeBase",
                func=self._query_knowledge_base,
                description="æŸ¥è¯¢çŸ¥è¯†åº“è·å–ä¿¡æ¯"
            ),
            Tool(
                name="WebSearch",
                func=self._web_search,
                description="æœç´¢ç½‘ç»œè·å–æœ€æ–°ä¿¡æ¯"
            ),
            Tool(
                name="Calculator",
                func=self._calculator,
                description="æ‰§è¡Œæ•°å­¦è®¡ç®—"
            ),
            Tool(
                name="Database",
                func=self._query_database,
                description="æŸ¥è¯¢æ•°æ®åº“"
            )
        ]
        return tools

    def _create_workflow(self) -> StateGraph:
        """åˆ›å»ºå·¥ä½œæµ"""
        workflow = StateGraph(AgentRAGState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("analyze_query", self._analyze_query_node)
        workflow.add_node("rag_retrieve", self._rag_retrieve_node)
        workflow.add_node("tool_execute", self._tool_execute_node)
        workflow.add_node("synthesize", self._synthesize_node)

        # è®¾ç½®å…¥å£
        workflow.set_entry_point("analyze_query")

        # æ·»åŠ è¾¹
        workflow.add_conditional_edges(
            "analyze_query",
            self._decide_approach,
            {
                "rag": "rag_retrieve",
                "tools": "tool_execute",
                "both": "rag_retrieve"  # å…ˆRAGå†tools
            }
        )

        workflow.add_edge("rag_retrieve", "tool_execute")
        workflow.add_edge("tool_execute", "synthesize")
        workflow.add_edge("synthesize", END)

        return workflow.compile()

    def _analyze_query_node(self, state: AgentRAGState) -> AgentRAGState:
        """åˆ†ææŸ¥è¯¢èŠ‚ç‚¹"""
        # ä½¿ç”¨LLMåˆ†ææŸ¥è¯¢ç±»å‹
        analysis = self._analyze_query_type(state["user_query"])

        return {
            **state,
            "messages": state["messages"] + [f"æŸ¥è¯¢åˆ†æï¼š{analysis}"]
        }

    def _rag_retrieve_node(self, state: AgentRAGState) -> AgentRAGState:
        """RAGæ£€ç´¢èŠ‚ç‚¹"""
        result = self.rag_system.query(state["user_query"])

        return {
            **state,
            "rag_context": result["answer"],
            "messages": state["messages"] + ["RAGæ£€ç´¢å®Œæˆ"]
        }

    def _tool_execute_node(self, state: AgentRAGState) -> AgentRAGState:
        """å·¥å…·æ‰§è¡ŒèŠ‚ç‚¹"""
        # ä½¿ç”¨Agentæ‰§è¡Œå·¥å…·
        results = self._execute_tools(state["user_query"], state["rag_context"])

        return {
            **state,
            "tool_results": results,
            "messages": state["messages"] + ["å·¥å…·æ‰§è¡Œå®Œæˆ"]
        }

    def _synthesize_node(self, state: AgentRAGState) -> AgentRAGState:
        """ç»¼åˆç­”æ¡ˆèŠ‚ç‚¹"""
        # ç»¼åˆRAGå’Œå·¥å…·ç»“æœ
        answer = self._synthesize_answer(
            state["user_query"],
            state["rag_context"],
            state["tool_results"]
        )

        return {
            **state,
            "final_answer": answer,
            "messages": state["messages"] + ["ç­”æ¡ˆç”Ÿæˆå®Œæˆ"]
        }

    def _decide_approach(self, state: AgentRAGState) -> str:
        """å†³ç­–æ–¹æ³•"""
        # åˆ†ææŸ¥è¯¢å†³å®šä½¿ç”¨RAGè¿˜æ˜¯å·¥å…·
        if "æœ€æ–°" in state["user_query"] or "å®æ—¶" in state["user_query"]:
            return "tools"
        elif "çŸ¥è¯†" in state["user_query"] or "æ–‡æ¡£" in state["user_query"]:
            return "rag"
        else:
            return "both"

    async def query(self, user_query: str) -> dict:
        """æŸ¥è¯¢"""
        initial_state: AgentRAGState = {
            "messages": [],
            "user_query": user_query,
            "rag_context": "",
            "tool_results": {},
            "final_answer": ""
        }

        result = await self.workflow.ainvoke(initial_state)
        return result
```

---

## é¡¹ç›®7ï¼šæœ¬åœ°AIåŠ©æ‰‹

### æŠ€æœ¯æ ˆ

```
ğŸ¦™ Ollamaï¼ˆæœ¬åœ°LLMï¼‰
ğŸ¤– LangChain
ğŸ¯ Llama 3.1/Mistral 7B
ğŸšï¸ Streamlit
ğŸ“Š Chromaï¼ˆæœ¬åœ°å‘é‡æ•°æ®åº“ï¼‰
```

### é¡¹ç›®ç®€ä»‹

ä¸€ä¸ªå®Œå…¨è¿è¡Œåœ¨æœ¬åœ°çš„AIåŠ©æ‰‹ï¼Œä¿æŠ¤æ•°æ®éšç§ã€‚

### æ ¸å¿ƒä»£ç 

```python
# local_assistant.py
import ollama
from langchain.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

class LocalAIAssistant:
    """æœ¬åœ°AIåŠ©æ‰‹"""

    def __init__(self, model_name="llama3.1"):
        # åˆå§‹åŒ–æœ¬åœ°LLM
        self.model = model_name
        self.embeddings = OllamaEmbeddings(model=model_name)

        # åˆå§‹åŒ–æœ¬åœ°å‘é‡æ•°æ®åº“
        self.vectorstore = Chroma(
            persist_directory="./chroma_db",
            embedding_function=self.embeddings
        )

    def chat(self, message: str) -> str:
        """å¯¹è¯"""
        response = ollama.chat(model=self.model, messages=[
            {
                "role": "user",
                "content": message
            }
        ])

        return response["message"]["content"]

    async def chat_with_rag(self, message: str) -> dict:
        """RAGå¯¹è¯"""
        # æ£€ç´¢ç›¸å…³æ–‡æ¡£
        docs = await self.vectorstore.asimilarity_search(message, k=3)

        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc.page_content for doc in docs])

        # ç”Ÿæˆå›ç­”
        prompt = f"""
        åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

        ä¸Šä¸‹æ–‡ï¼š
        {context}

        é—®é¢˜ï¼š{message}

        å›ç­”ï¼š
        """

        response = ollama.chat(model=self.model, messages=[
            {
                "role": "user",
                "content": prompt
            }
        ])

        return {
            "answer": response["message"]["content"],
            "sources": docs
        }

    def add_document(self, file_path: str):
        """æ·»åŠ æ–‡æ¡£"""
        from langchain.document_loaders import TextLoader
        from langchain.text_splitter import RecursiveCharacterTextSplitter

        # åŠ è½½æ–‡æ¡£
        loader = TextLoader(file_path)
        documents = loader.load()

        # åˆ†å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        splits = text_splitter.split_documents(documents)

        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        self.vectorstore.add_documents(splits)
        self.vectorstore.persist()
```

### Streamlitç•Œé¢

```python
# ui/app.py
import streamlit as st
from local_assistant import LocalAIAssistant

st.set_page_config(page_title="æœ¬åœ°AIåŠ©æ‰‹", page_icon="ğŸ¤–")

st.title("ğŸ¤– æœ¬åœ°AIåŠ©æ‰‹")

# ä¾§è¾¹æ 
with st.sidebar:
    st.title("è®¾ç½®")
    model = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        ["llama3.1", "mistral", "qwen2"],
        index=0
    )

    # ä¸Šä¼ æ–‡æ¡£
    uploaded_file = st.file_uploader("ä¸Šä¼ æ–‡æ¡£", type=["txt", "md"])
    if uploaded_file:
        assistant.add_document(uploaded_file)
        st.success("æ–‡æ¡£å·²æ·»åŠ ")

# åˆå§‹åŒ–assistant
if "assistant" not in st.session_state:
    st.session_state.assistant = LocalAIAssistant(model)

assistant = st.session_state.assistant

# å¯¹è¯å†å²
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ˜¾ç¤ºå¯¹è¯å†å²
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# ç”¨æˆ·è¾“å…¥
if prompt := st.chat_input("è¾“å…¥æ¶ˆæ¯"):
    # æ˜¾ç¤ºç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # ç”ŸæˆåŠ©æ‰‹å›å¤
    with st.chat_message("assistant"):
        with st.spinner("æ€è€ƒä¸­..."):
            response = assistant.chat_with_rag(prompt)
            st.markdown(response["answer"])

    st.session_state.messages.append({"role": "assistant", "content": response["answer"]})
```

---

## å­¦ä¹ å»ºè®®

### æ¨èå­¦ä¹ é¡ºåº

```
ç¬¬1é˜¶æ®µï¼šåŸºç¡€ï¼ˆ1-2å‘¨ï¼‰
â”œâ”€ æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿï¼ˆç°æœ‰ï¼‰
â””â”€ ä»£ç åŠ©æ‰‹Agentï¼ˆç°æœ‰ï¼‰

ç¬¬2é˜¶æ®µï¼šè¿›é˜¶ï¼ˆ2-3å‘¨ï¼‰
â”œâ”€ Multi-Agentåä½œç³»ç»Ÿ â­ NEW
â””â”€ ç”Ÿäº§çº§RAGç³»ç»Ÿ â­ NEW

ç¬¬3é˜¶æ®µï¼šé«˜çº§ï¼ˆ2-3å‘¨ï¼‰
â”œâ”€ Agent + RAGç»“åˆç³»ç»Ÿ â­ NEW
â””â”€ æœ¬åœ°AIåŠ©æ‰‹ â­ NEW
```

### 2024-2026æŠ€æœ¯è¦ç‚¹

æ ¹æ®[AI Agentå‘å±•è¶‹åŠ¿](https://www.ibm.com/think/insights/ai-agents-2025-expectations-vs-reality)ï¼š

- âœ… **Multi-Agentç³»ç»Ÿ**ï¼š2025-2026ä¸»æµ
- âœ… **ç”Ÿäº§çº§RAG**ï¼šä¼ä¸šåº”ç”¨æ ‡é…
- âœ… **Agent + RAGç»“åˆ**ï¼šæœ€ä½³å®è·µ
- âœ… **æœ¬åœ°æ¨¡å‹éƒ¨ç½²**ï¼šæ•°æ®éšç§ä¿æŠ¤
- âœ… **æµå¼AIå“åº”**ï¼šæå‡ç”¨æˆ·ä½“éªŒ
- âœ… **Agentæ ‡å‡†åŒ–**ï¼šå·¥å…·å’Œæ¡†æ¶æˆç†Ÿ

---

**æ­å–œä½ å®Œæˆäº†AIåº”ç”¨å¼€å‘å®Œå…¨æŒ‡å—ï¼** ğŸ‰

ä»åŸºç¡€åˆ°å®æˆ˜ï¼Œä½ å·²ç»æŒæ¡äº†æ„å»ºç°ä»£AIåº”ç”¨çš„æ ¸å¿ƒæŠ€èƒ½ã€‚

**ç»§ç»­ä¿æŒå­¦ä¹ çš„çƒ­æƒ…ï¼Œåˆ›é€ æ›´å¤šæœ‰ä»·å€¼çš„AIåº”ç”¨ï¼**

---

**æ•™ç¨‹ç»“æŸ**
**æœ‰é—®é¢˜ï¼Ÿ** esimonx@163.com
**æ›´å¤šæ•™ç¨‹ï¼Ÿ** æŸ¥çœ‹ [Git](/git/)ã€[å‰ç«¯](/guide/) ç­‰å…¶ä»–ç« èŠ‚
