# å®æˆ˜é¡¹ç›®

## æœ¬ç« å¯¼è¯»

æ­å–œä½ æ¥åˆ°æœ€åä¸€ç« ï¼æœ¬ç« å°†é€šè¿‡3ä¸ªå®Œæ•´çš„å®æˆ˜é¡¹ç›®ï¼Œç»¼åˆè¿ç”¨å‰é¢å­¦åˆ°çš„çŸ¥è¯†ï¼Œæ„å»ºç”Ÿäº§çº§çš„AIåº”ç”¨ã€‚

**å­¦ä¹ ç›®æ ‡**ï¼š
- ç»¼åˆè¿ç”¨LLMã€LangChainã€RAGã€Agentç­‰æŠ€æœ¯
- æ„å»ºå®Œæ•´çš„AIåº”ç”¨ç³»ç»Ÿ
- æŒæ¡é¡¹ç›®éƒ¨ç½²å’Œä¼˜åŒ–æŠ€å·§
- ç§¯ç´¯å®æˆ˜ç»éªŒ

**é¡¹ç›®æ¦‚è§ˆ**ï¼š
1. **æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ** - RAGåº”ç”¨
2. **ä»£ç åŠ©æ‰‹Agent** - Agentåº”ç”¨
3. **ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹** - ç»¼åˆåº”ç”¨

---

## é¡¹ç›®1ï¼šæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ

### é¡¹ç›®æ¦‚è¿°

**åŠŸèƒ½**ï¼š
- ä¸Šä¼ PDF/Word/TXTæ–‡æ¡£
- è‡ªåŠ¨æ„å»ºå‘é‡æ•°æ®åº“
- æ™ºèƒ½é—®ç­”ï¼Œæ”¯æŒæº¯æº
- å¤šæ–‡æ¡£ç®¡ç†

**æŠ€æœ¯æ ˆ**ï¼š
- FastAPIï¼ˆWebæ¡†æ¶ï¼‰
- LangChainï¼ˆAIæ¡†æ¶ï¼‰
- Chromaï¼ˆå‘é‡æ•°æ®åº“ï¼‰
- Streamlitï¼ˆå‰ç«¯ç•Œé¢ï¼‰

### é¡¹ç›®ç»“æ„

```
document-qa-system/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPIä¸»ç¨‹åº
â”‚   â”œâ”€â”€ models.py            # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ rag.py               # RAGæ ¸å¿ƒé€»è¾‘
â”‚   â””â”€â”€ config.py            # é…ç½®æ–‡ä»¶
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/           # å­˜å‚¨ä¸Šä¼ çš„æ–‡æ¡£
â”œâ”€â”€ vector_db/
â”‚   â””â”€â”€ chroma/              # å‘é‡æ•°æ®åº“
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py     # Streamlitç•Œé¢
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

### åç«¯å®ç°

**1. é…ç½®æ–‡ä»¶ (`app/config.py`)**

```python
import os
from dotenv import load_dotenv

load_dotenv()

class Config:
    # APIé…ç½®
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    OPENAI_MODEL = "gpt-3.5-turbo"
    OPENAI_EMBEDDINGS = "text-embedding-3-small"

    # æ•°æ®åº“é…ç½®
    CHROMA_PERSIST_DIR = "./vector_db/chroma"

    # æ–‡æ¡£é…ç½®
    UPLOAD_DIR = "./data/documents"
    ALLOWED_EXTENSIONS = {".pdf", ".txt", ".md", ".docx"}
    MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

    # RAGé…ç½®
    CHUNK_SIZE = 1000
    CHUNK_OVERLAP = 200
    TOP_K_RETRIEVAL = 3
```

**2. RAGæ ¸å¿ƒé€»è¾‘ (`app/rag.py`)**

```python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.document_loaders import (
    PyPDFLoader,
    TextLoader,
    UnstructuredMarkdownLoader,
    Docx2txtLoader
)
from pathlib import Path
from typing import List, Optional
import shutil

class DocumentQA:
    """æ–‡æ¡£é—®ç­”ç³»ç»Ÿ"""

    def __init__(self, collection_name: str = "default"):
        self.collection_name = collection_name
        self.embeddings = OpenAIEmbeddings(
            openai_api_key=Config.OPENAI_API_KEY
        )
        self.llm = ChatOpenAI(
            model=Config.OPENAI_MODEL,
            openai_api_key=Config.OPENAI_API_KEY,
            temperature=0
        )

        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vectorstore = Chroma(
            collection_name=collection_name,
            persist_directory=Config.CHROMA_PERSIST_DIR,
            embedding_function=self.embeddings
        )

        # åˆ›å»ºQAé“¾
        self.qa_chain = self._create_qa_chain()

    def _create_qa_chain(self):
        """åˆ›å»ºQAé“¾"""
        prompt_template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚

ä¸Šä¸‹æ–‡ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”è¦æ±‚ï¼š
1. åªä½¿ç”¨ä¸Šä¸‹æ–‡ä¸­çš„ä¿¡æ¯
2. å¦‚æœä¸Šä¸‹æ–‡ä¸­æ²¡æœ‰ç­”æ¡ˆï¼Œæ˜ç¡®è¯´æ˜
3. å›ç­”è¦å‡†ç¡®ã€ç®€æ´
4. æ ‡æ³¨ä¿¡æ¯æ¥æº

å›ç­”ï¼š"""

        prompt = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )

        return RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vectorstore.as_retriever(
                search_kwargs={"k": Config.TOP_K_RETRIEVAL}
            ),
            return_source_documents=True,
            chain_type_kwargs={"prompt": prompt}
        )

    def add_document(self, file_path: str) -> dict:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        try:
            # 1. æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åŠ è½½å™¨
            ext = Path(file_path).suffix.lower()

            loaders = {
                ".pdf": PyPDFLoader,
                ".txt": TextLoader,
                ".md": UnstructuredMarkdownLoader,
                ".docx": Docx2txtLoader
            }

            if ext not in loaders:
                return {"success": False, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"}

            loader = loaders[ext](file_path)
            documents = loader.load()

            # 2. åˆ†å‰²æ–‡æ¡£
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=Config.CHUNK_SIZE,
                chunk_overlap=Config.CHUNK_OVERLAP
            )
            splits = text_splitter.split_documents(documents)

            # 3. æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            for split in splits:
                split.metadata["source"] = Path(file_path).name

            self.vectorstore.add_documents(splits)

            return {
                "success": True,
                "message": f"æˆåŠŸæ·»åŠ  {len(splits)} ä¸ªæ–‡æ¡£å—",
                "chunks": len(splits)
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def ask(self, question: str) -> dict:
        """æé—®"""
        try:
            result = self.qa_chain({"query": question})

            # æå–æ¥æº
            sources = [
                {
                    "source": doc.metadata.get("source", "Unknown"),
                    "page": doc.metadata.get("page", 0),
                    "content": doc.page_content[:200] + "..."
                }
                for doc in result["source_documents"]
            ]

            return {
                "success": True,
                "answer": result["result"],
                "sources": sources
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def delete_collection(self):
        """åˆ é™¤å½“å‰é›†åˆ"""
        self.vectorstore.delete_collection()
        self.vectorstore = Chroma(
            collection_name=self.collection_name,
            persist_directory=Config.CHROMA_PERSIST_DIR,
            embedding_function=self.embeddings
        )

    def get_stats(self) -> dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        collection = self.vectorstore._collection
        return {
            "total_documents": collection.count(),
            "collection_name": self.collection_name
        }
```

**3. FastAPIä¸»ç¨‹åº (`app/main.py`)**

```python
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List
import shutil
from pathlib import Path

from .rag import DocumentQA
from .config import Config

app = FastAPI(title="æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å­˜å‚¨QAå®ä¾‹
qa_instances = {}

class Question(BaseModel):
    question: str
    collection_name: str = "default"

@app.on_event("startup")
async def startup():
    """å¯åŠ¨æ—¶åˆ›å»ºä¸Šä¼ ç›®å½•"""
    Path(Config.UPLOAD_DIR).mkdir(parents=True, exist_ok=True)
    Path(Config.CHROMA_PERSIST_DIR).mkdir(parents=True, exist_ok=True)

@app.get("/")
async def root():
    return {"message": "æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»ŸAPI"}

@app.post("/upload")
async def upload_document(
    file: UploadFile = File(...),
    collection_name: str = "default"
):
    """ä¸Šä¼ æ–‡æ¡£"""
    # éªŒè¯æ–‡ä»¶æ ¼å¼
    ext = Path(file.filename).suffix.lower()
    if ext not in Config.ALLOWED_EXTENSIONS:
        raise HTTPException(status_code=400, detail="ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼")

    # ä¿å­˜æ–‡ä»¶
    file_path = Path(Config.UPLOAD_DIR) / f"{collection_name}_{file.filename}"
    with open(file_path, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)

    # æ·»åŠ åˆ°çŸ¥è¯†åº“
    qa = qa_instances.get(collection_name)
    if not qa:
        qa = DocumentQA(collection_name)
        qa_instances[collection_name] = qa

    result = qa.add_document(str(file_path))

    return result

@app.post("/ask")
async def ask_question(req: Question):
    """æé—®"""
    qa = qa_instances.get(req.collection_name)
    if not qa:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")

    result = qa.ask(req.question)
    return result

@app.get("/stats/{collection_name}")
async def get_stats(collection_name: str):
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    qa = qa_instances.get(collection_name)
    if not qa:
        raise HTTPException(status_code=404, detail="çŸ¥è¯†åº“ä¸å­˜åœ¨")

    return qa.get_stats()

@app.delete("/collection/{collection_name}")
async def delete_collection(collection_name: str):
    """åˆ é™¤çŸ¥è¯†åº“"""
    qa = qa_instances.get(collection_name)
    if qa:
        qa.delete_collection()
        del qa_instances[collection_name]

    return {"message": "çŸ¥è¯†åº“å·²åˆ é™¤"}
```

### å‰ç«¯ç•Œé¢

**Streamlitç•Œé¢ (`ui/streamlit_app.py`)**

```python
import streamlit as st
import requests
from pathlib import Path

# é…ç½®
API_URL = "http://localhost:8000"
st.set_page_config(
    page_title="æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ",
    page_icon="ğŸ“š",
    layout="wide"
)

# ä¾§è¾¹æ 
st.sidebar.title("ğŸ“š æ–‡æ¡£ç®¡ç†")

# ä¸Šä¼ æ–‡æ¡£
uploaded_file = st.sidebar.file_uploader(
    "ä¸Šä¼ æ–‡æ¡£",
    type=["pdf", "txt", "md", "docx"]
)

collection_name = st.sidebar.text_input("çŸ¥è¯†åº“åç§°", value="default")

if uploaded_file and st.sidebar.button("æ·»åŠ åˆ°çŸ¥è¯†åº“"):
    with st.sidebar.spinner("å¤„ç†ä¸­..."):
        files = {"file": uploaded_file}
        response = requests.post(
            f"{API_URL}/upload",
            files=files,
            params={"collection_name": collection_name}
        )

        if response.status_code == 200:
            st.sidebar.success(response.json()["message"])
        else:
            st.sidebar.error("ä¸Šä¼ å¤±è´¥")

# ç»Ÿè®¡ä¿¡æ¯
stats_response = requests.get(f"{API_URL}/stats/{collection_name}")
if stats_response.status_code == 200:
    stats = stats_response.json()
    st.sidebar.metric("æ–‡æ¡£å—æ•°", stats["total_documents"])

# ä¸»ç•Œé¢
st.title("ğŸ¤– æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")

# é—®ç­”åŒºåŸŸ
question = st.text_input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š", placeholder="ä¾‹å¦‚ï¼šæ–‡æ¡£ä¸­æåˆ°äº†ä»€ä¹ˆ...")

if st.button("æé—®") and question:
    with st.spinner("æ€è€ƒä¸­..."):
        response = requests.post(
            f"{API_URL}/ask",
            json={
                "question": question,
                "collection_name": collection_name
            }
        )

        if response.status_code == 200:
            result = response.json()

            st.markdown("### ğŸ“ å›ç­”")
            st.write(result["answer"])

            if "sources" in result and result["sources"]:
                st.markdown("### ğŸ“š ä¿¡æ¯æ¥æº")
                for i, source in enumerate(result["sources"], 1):
                    with st.expander(f"æ¥æº {i}: {source['source']}"):
                        st.write(source["content"])
        else:
            st.error("æŸ¥è¯¢å¤±è´¥")

# ä½¿ç”¨è¯´æ˜
with st.expander("ğŸ’¡ ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    1. **ä¸Šä¼ æ–‡æ¡£**ï¼šåœ¨å·¦ä¾§ä¸Šä¼ PDFã€TXTã€Markdownæˆ–Wordæ–‡æ¡£
    2. **æ„å»ºçŸ¥è¯†åº“**ï¼šç³»ç»Ÿä¼šè‡ªåŠ¨å¤„ç†æ–‡æ¡£å¹¶å»ºç«‹ç´¢å¼•
    3. **æé—®**ï¼šè¾“å…¥é—®é¢˜ï¼Œç³»ç»Ÿä¼šåŸºäºæ–‡æ¡£å†…å®¹å›ç­”
    4. **æº¯æº**ï¼šæŸ¥çœ‹ç­”æ¡ˆçš„æ¥æºï¼Œç¡®ä¿å‡†ç¡®æ€§

    **æ”¯æŒçš„æ–‡ä»¶æ ¼å¼**ï¼š
    - PDFï¼ˆ.pdfï¼‰
    - æ–‡æœ¬ï¼ˆ.txtï¼‰
    - Markdownï¼ˆ.mdï¼‰
    - Wordæ–‡æ¡£ï¼ˆ.docxï¼‰
    """)
```

### éƒ¨ç½²å’Œè¿è¡Œ

```bash
# 1. å®‰è£…ä¾èµ–
pip install fastapi uvicorn streamlit langchain langchain-openai chromadb

# 2. å¯åŠ¨åç«¯
uvicorn app.main:app --reload --port 8000

# 3. å¯åŠ¨å‰ç«¯
streamlit run ui/streamlit_app.py

# 4. è®¿é—®
# å‰ç«¯ï¼šhttp://localhost:8501
# APIæ–‡æ¡£ï¼šhttp://localhost:8000/docs
```

---

## é¡¹ç›®2ï¼šä»£ç åŠ©æ‰‹Agent

### é¡¹ç›®æ¦‚è¿°

**åŠŸèƒ½**ï¼š
- ä»£ç ç”Ÿæˆå’Œä¼˜åŒ–
- Bugæ£€æµ‹å’Œä¿®å¤
- ä»£ç è§£é‡Š
- æŠ€æœ¯æ–‡æ¡£æŸ¥è¯¢

**æŠ€æœ¯æ ˆ**ï¼š
- LangChain Agents
- OpenAI API
- GitHub API
- Python ASTï¼ˆä»£ç åˆ†æï¼‰

### å®ç°ä»£ç 

```python
# code_agent.py
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain.tools import Tool, StructuredTool
from langchain import hub
from pydantic import BaseModel, Field
import subprocess
import ast
import requests
import os

class CodeInput(BaseModel):
    """ä»£ç è¾“å…¥"""
    code: str = Field(description="è¦å¤„ç†çš„Pythonä»£ç ")
    question: str = Field(description="å…³äºä»£ç çš„é—®é¢˜")

class CodeAgent:
    """ä»£ç åŠ©æ‰‹Agent"""

    def __init__(self):
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        self.tools = self._create_tools()
        self.agent = self._create_agent()

    def _create_tools(self):
        """åˆ›å»ºå·¥å…·é›†"""

        # 1. ä»£ç æ‰§è¡Œå·¥å…·
        def execute_code(code: str) -> str:
            """æ‰§è¡ŒPythonä»£ç """
            try:
                result = subprocess.run(
                    ["python", "-c", code],
                    capture_output=True,
                    text=True,
                    timeout=10
                )
                if result.returncode == 0:
                    return f"è¾“å‡ºï¼š\n{result.stdout}"
                else:
                    return f"é”™è¯¯ï¼š\n{result.stderr}"
            except Exception as e:
                return f"æ‰§è¡Œå¤±è´¥ï¼š{str(e)}"

        # 2. ä»£ç åˆ†æå·¥å…·
        def analyze_code(code: str) -> str:
            """åˆ†æä»£ç ç»“æ„"""
            try:
                tree = ast.parse(code)

                functions = []
                classes = []
                imports = []

                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        functions.append(node.name)
                    elif isinstance(node, ast.ClassDef):
                        classes.append(node.name)
                    elif isinstance(node, (ast.Import, ast.ImportFrom)):
                        if isinstance(node, ast.Import):
                            imports.extend([alias.name for alias in node.names])
                        else:
                            imports.append(node.module)

                return f"""
                ä»£ç ç»“æ„åˆ†æï¼š
                - å‡½æ•°ï¼š{', '.join(functions) if functions else 'æ— '}
                - ç±»ï¼š{', '.join(classes) if classes else 'æ— '}
                - å¯¼å…¥æ¨¡å—ï¼š{', '.join(set(imports)) if imports else 'æ— '}
                """
            except Exception as e:
                return f"åˆ†æå¤±è´¥ï¼š{str(e)}"

        # 3. ä»£ç æ ¼å¼åŒ–å·¥å…·
        def format_code(code: str) -> str:
            """æ ¼å¼åŒ–ä»£ç """
            try:
                import black
                formatted = black.format_str(code, mode=black.Mode())
                return f"æ ¼å¼åŒ–åçš„ä»£ç ï¼š\n{formatted}"
            except:
                return "æ ¼å¼åŒ–å¤±è´¥ï¼Œè¯·ç¡®ä¿ä»£ç è¯­æ³•æ­£ç¡®"

        # 4. ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹å·¥å…·
        def generate_tests(code: str) -> str:
            """ä¸ºä»£ç ç”Ÿæˆæµ‹è¯•ç”¨ä¾‹"""
            prompt = f"""
            ä¸ºä»¥ä¸‹ä»£ç ç”Ÿæˆå•å…ƒæµ‹è¯•ï¼ˆä½¿ç”¨pytestï¼‰ï¼š
            {code}

            è¦æ±‚ï¼š
            - æµ‹è¯•æ­£å¸¸æƒ…å†µ
            - æµ‹è¯•è¾¹ç•Œæƒ…å†µ
            - æµ‹è¯•å¼‚å¸¸æƒ…å†µ
            """
            # è°ƒç”¨LLMç”Ÿæˆ
            from langchain_openai import ChatOpenAI
            llm = ChatOpenAI(model="gpt-3.5-turbo")
            return llm.predict(prompt)

        # 5. GitHubæœç´¢å·¥å…·
        def search_github(query: str) -> str:
            """æœç´¢GitHubä»£ç """
            # è¿™é‡Œéœ€è¦GitHub Token
            token = os.getenv("GITHUB_TOKEN")
            if not token:
                return "éœ€è¦è®¾ç½®GITHUB_TOKENç¯å¢ƒå˜é‡"

            url = "https://api.github.com/search/code"
            params = {"q": query}
            headers = {"Authorization": f"token {token}"}

            response = requests.get(url, params=params, headers=headers)
            if response.status_code == 200:
                data = response.json()
                results = data.get("items", [])[:5]

                output = []
                for item in results:
                    output.append(f"""
                    - ä»“åº“ï¼š{item['repository']['full_name']}
                    - æ–‡ä»¶ï¼š{item['path']}
                    - URLï¼š{item['html_url']}
                    """)

                return "\n".join(output)
            else:
                return "æœç´¢å¤±è´¥"

        # åˆ›å»ºå·¥å…·åˆ—è¡¨
        return [
            Tool(
                name="ExecuteCode",
                func=execute_code,
                description="æ‰§è¡ŒPythonä»£ç å¹¶è¿”å›ç»“æœã€‚è¾“å…¥ï¼šå®Œæ•´çš„Pythonä»£ç å­—ç¬¦ä¸²"
            ),
            Tool(
                name="AnalyzeCode",
                func=analyze_code,
                description="åˆ†æPythonä»£ç çš„ç»“æ„ï¼Œæå–å‡½æ•°ã€ç±»å’Œå¯¼å…¥ä¿¡æ¯ã€‚è¾“å…¥ï¼šPythonä»£ç "
            ),
            Tool(
                name="FormatCode",
                func=format_code,
                description="ä½¿ç”¨Blackæ ¼å¼åŒ–Pythonä»£ç ã€‚è¾“å…¥ï¼šéœ€è¦æ ¼å¼åŒ–çš„ä»£ç "
            ),
            Tool(
                name="GenerateTests",
                func=generate_tests,
                description="ä¸ºPythonä»£ç ç”Ÿæˆpytestæµ‹è¯•ç”¨ä¾‹ã€‚è¾“å…¥ï¼šéœ€è¦æµ‹è¯•çš„ä»£ç "
            ),
            Tool(
                name="SearchGitHub",
                func=search_github,
                description="åœ¨GitHubä¸Šæœç´¢ä»£ç ç¤ºä¾‹ã€‚è¾“å…¥ï¼šæœç´¢å…³é”®è¯"
            )
        ]

    def _create_agent(self):
        """åˆ›å»ºAgent"""
        prompt = hub.pull("hwchase17/openai-functions-agent")

        agent = create_openai_functions_agent(
            llm=self.llm,
            tools=self.tools,
            prompt=prompt
        )

        return AgentExecutor(
            agent=agent,
            tools=self.tools,
            verbose=True,
            max_iterations=10
        )

    def assist(self, request: str) -> str:
        """ä»£ç åŠ©æ‰‹"""
        result = self.agent.invoke({"input": request})
        return result["output"]

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    agent = CodeAgent()

    # ç¤ºä¾‹1ï¼šä»£ç è§£é‡Š
    print(agent.assist("""
    è§£é‡Šä»¥ä¸‹ä»£ç çš„ä½œç”¨ï¼š
    def quicksort(arr):
        if len(arr) <= 1:
            return arr
        pivot = arr[len(arr) // 2]
        left = [x for x in arr if x < pivot]
        middle = [x for x in arr if x == pivot]
        right = [x for x in arr if x > pivot]
        return quicksort(left) + middle + quicksort(right)
    """))

    # ç¤ºä¾‹2ï¼šBugæ£€æµ‹
    print(agent.assist("""
    æ£€æŸ¥ä»¥ä¸‹ä»£ç æ˜¯å¦æœ‰Bugï¼š
    for i in range(len(arr)):
        if arr[i] > arr[i+1]:
            arr[i], arr[i+1] = arr[i+1], arr[i]
    """))

    # ç¤ºä¾‹3ï¼šä»£ç ä¼˜åŒ–
    print(agent.assist("""
    ä¼˜åŒ–ä»¥ä¸‹ä»£ç çš„æ€§èƒ½ï¼š
    squares = []
    for i in range(1000000):
        squares.append(i ** 2)
    """))
```

---

## é¡¹ç›®3ï¼šä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹

### é¡¹ç›®æ¦‚è¿°

**åŠŸèƒ½**ï¼š
- ç¬”è®°ç®¡ç†ï¼ˆå¢åˆ æ”¹æŸ¥ï¼‰
- æ™ºèƒ½é—®ç­”
- çŸ¥è¯†å…³è”
- å®šæœŸå¤ä¹ æé†’

**æŠ€æœ¯æ ˆ**ï¼š
- RAGï¼ˆçŸ¥è¯†æ£€ç´¢ï¼‰
- Agentï¼ˆä»»åŠ¡è‡ªåŠ¨åŒ–ï¼‰
- Notion APIï¼ˆç¬”è®°åŒæ­¥ï¼‰

### æ ¸å¿ƒå®ç°

```python
# knowledge_assistant.py
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.agents import create_openai_functions_agent, AgentExecutor
from langchain.tools import Tool
from langchain.memory import ConversationBufferMemory
from datetime import datetime, timedelta
import json
from pathlib import Path

class KnowledgeAssistant:
    """ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹"""

    def __init__(self, knowledge_base_path: str = "./knowledge_base"):
        self.kb_path = Path(knowledge_base_path)
        self.kb_path.mkdir(parents=True, exist_ok=True)

        self.embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma(
            persist_directory=str(self.kb_path / "chroma"),
            embedding_function=self.embeddings
        )

        self.memory = ConversationBufferMemory()
        self.agent = self._create_agent()

    def _create_agent(self):
        """åˆ›å»ºAgent"""

        # å·¥å…·1ï¼šæ·»åŠ ç¬”è®°
        def add_note(title: str, content: str, tags: list = None) -> str:
            """æ·»åŠ ç¬”è®°åˆ°çŸ¥è¯†åº“"""
            note = {
                "title": title,
                "content": content,
                "tags": tags or [],
                "created_at": datetime.now().isoformat()
            }

            # ä¿å­˜åˆ°æ–‡ä»¶
            note_path = self.kb_path / "notes" / f"{title}.json"
            note_path.parent.mkdir(exist_ok=True)
            with open(note_path, "w", encoding="utf-8") as f:
                json.dump(note, f, ensure_ascii=False, indent=2)

            # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
            from langchain.schema import Document
            doc = Document(
                page_content=content,
                metadata={"title": title, "tags": tags or [], "source": str(note_path)}
            )
            self.vectorstore.add_documents([doc])

            return f"ç¬”è®°'{title}'å·²æ·»åŠ ï¼Œæ ‡ç­¾ï¼š{', '.join(tags or [])}"

        # å·¥å…·2ï¼šæœç´¢ç¬”è®°
        def search_notes(query: str, top_k: int = 5) -> str:
            """æœç´¢ç›¸å…³ç¬”è®°"""
            results = self.vectorstore.similarity_search(query, k=top_k)

            output = []
            for i, doc in enumerate(results, 1):
                output.append(f"""
                ç¬”è®°{i}ï¼š{doc.metadata.get('title', 'Untitled')}
                æ ‡ç­¾ï¼š{', '.join(doc.metadata.get('tags', []))}
                å†…å®¹ï¼š{doc.page_content[:200]}...
                """)

            return "\n".join(output) if output else "æœªæ‰¾åˆ°ç›¸å…³ç¬”è®°"

        # å·¥å…·3ï¼šåˆ—å‡ºæ‰€æœ‰ç¬”è®°
        def list_notes() -> str:
            """åˆ—å‡ºæ‰€æœ‰ç¬”è®°"""
            notes_dir = self.kb_path / "notes"
            if not notes_dir.exists():
                return "çŸ¥è¯†åº“ä¸ºç©º"

            notes = list(notes_dir.glob("*.json"))
            output = [f"å…±æœ‰ {len(notes)} æ¡ç¬”è®°ï¼š"]

            for note_path in notes:
                with open(note_path, "r", encoding="utf-8") as f:
                    note = json.load(f)
                output.append(f"- {note['title']}ï¼ˆæ ‡ç­¾ï¼š{', '.join(note['tags'])}ï¼‰")

            return "\n".join(output)

        # å·¥å…·4ï¼šç”Ÿæˆå¤ä¹ è®¡åˆ’
        def create_review_plan(days: int = 7) -> str:
            """ç”Ÿæˆå¤ä¹ è®¡åˆ’"""
            # ä½¿ç”¨é—´éš”é‡å¤ç®—æ³•
            notes_dir = self.kb_path / "notes"
            notes = list(notes_dir.glob("*.json"))

            plan = []
            today = datetime.now()

            for note_path in notes:
                with open(note_path, "r", encoding="utf-8") as f:
                    note = json.load(f)

                # ç®€å•çš„é—´éš”é‡å¤é€»è¾‘
                created = datetime.fromisoformat(note['created_at'])
                days_since = (today - created).days

                if days_since >= days:
                    review_date = today + timedelta(days=1)
                    plan.append(f"{review_date.strftime('%Y-%m-%d')}: å¤ä¹ ã€Š{note['title']}ã€‹")

            return "å¤ä¹ è®¡åˆ’ï¼š\n" + "\n".join(plan) if plan else "æš‚æ— éœ€è¦å¤ä¹ çš„å†…å®¹"

        # åˆ›å»ºå·¥å…·
        tools = [
            Tool(
                name="AddNote",
                func=lambda x: add_note(**json.loads(x)),
                description="æ·»åŠ ç¬”è®°åˆ°çŸ¥è¯†åº“ã€‚è¾“å…¥JSONæ ¼å¼ï¼š{'title': 'æ ‡é¢˜', 'content': 'å†…å®¹', 'tags': ['æ ‡ç­¾1', 'æ ‡ç­¾2']}"
            ),
            Tool(
                name="SearchNotes",
                func=search_notes,
                description="åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç¬”è®°ã€‚è¾“å…¥ï¼šæœç´¢å…³é”®è¯"
            ),
            Tool(
                name="ListNotes",
                func=list_notes,
                description="åˆ—å‡ºæ‰€æœ‰ç¬”è®°"
            ),
            Tool(
                name="CreateReviewPlan",
                func=lambda x: create_review_plan(int(x) if x else 7),
                description="ç”Ÿæˆå¤ä¹ è®¡åˆ’ã€‚è¾“å…¥ï¼šé—´éš”å¤©æ•°ï¼ˆé»˜è®¤7å¤©ï¼‰"
            )
        ]

        # åˆ›å»ºAgent
        llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
        from langchain import hub
        prompt = hub.pull("hwchase17/openai-functions-agent")

        agent = create_openai_functions_agent(
            llm=llm,
            tools=tools,
            prompt=prompt
        )

        return AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            memory=self.memory
        )

    def chat(self, message: str) -> str:
        """å¯¹è¯"""
        result = self.agent.invoke({"input": message})
        return result["output"]

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    assistant = KnowledgeAssistant()

    # ç¤ºä¾‹å¯¹è¯
    print(assistant.chat("å¸®æˆ‘æ·»åŠ ä¸€æ¡Pythonè£…é¥°å™¨çš„ç¬”è®°"))
    print(assistant.chat("æœç´¢å…³äºæœºå™¨å­¦ä¹ çš„ç¬”è®°"))
    print(assistant.chat("ç”Ÿæˆæœ¬å‘¨çš„å¤ä¹ è®¡åˆ’"))
```

---

## é¡¹ç›®éƒ¨ç½²å’Œä¼˜åŒ–

### Dockeréƒ¨ç½²

**Dockerfile**:

```dockerfile
FROM python:3.11-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000 8501

# å¯åŠ¨å‘½ä»¤
CMD ["sh", "-c", "uvicorn app.main:app --host 0.0.0.0 --port 8000 & streamlit run ui/streamlit_app.py --server.port 8501"]
```

**docker-compose.yml**:

```yaml
version: '3.8'

services:
  app:
    build: .
    ports:
      - "8000:8000"
      - "8501:8501"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./data:/app/data
      - ./vector_db:/app/vector_db
    restart: unless-stopped
```

### æ€§èƒ½ä¼˜åŒ–

```python
# 1. ä½¿ç”¨ç¼“å­˜
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_embedding(text: str):
    return embeddings.embed_query(text)

# 2. æ‰¹é‡å¤„ç†
def batch_add_documents(docs: list, batch_size=100):
    for i in range(0, len(docs), batch_size):
        batch = docs[i:i+batch_size]
        vectorstore.add_documents(batch)

# 3. å¼‚æ­¥å¤„ç†
import asyncio
from langchain.runnables import RunnableConfig

async def async_ask(question: str):
    config = RunnableConfig(callbacks=[...])
    return await rag_chain.ainvoke(question, config)
```

### å®‰å…¨å»ºè®®

```python
# 1. APIå¯†é’¥ç®¡ç†
import os
from dotenv import load_dotenv
load_dotenv()

# 2. è¾“å…¥éªŒè¯
from pydantic import validator

class Question(BaseModel):
    question: str

    @validator('question')
    def question_length(cls, v):
        if len(v) > 1000:
            raise ValueError('é—®é¢˜é•¿åº¦ä¸èƒ½è¶…è¿‡1000å­—ç¬¦')
        return v

# 3. é€Ÿç‡é™åˆ¶
from slowapi import Limiter
from slowapi.util import get_remote_address

limiter = Limiter(key_func=get_remote_address)

@app.post("/ask")
@limiter.limit("10/minute")
async def ask_question(req: Question):
    ...
```

---

## æœ¬ç« å°ç»“

### é¡¹ç›®å›é¡¾

âœ… **é¡¹ç›®1ï¼šæ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ**
- å®Œæ•´çš„RAGå®ç°
- Webç•Œé¢éƒ¨ç½²
- å¤šæ ¼å¼æ–‡æ¡£æ”¯æŒ

âœ… **é¡¹ç›®2ï¼šä»£ç åŠ©æ‰‹Agent**
- å·¥å…·å®šä¹‰å’Œä½¿ç”¨
- ä»£ç åˆ†æèƒ½åŠ›
- GitHubé›†æˆ

âœ… **é¡¹ç›®3ï¼šä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹**
- RAG + Agentç»“åˆ
- ç¬”è®°ç®¡ç†
- å¤ä¹ è®¡åˆ’

### å­¦ä¹ æˆæœ

æ­å–œä½ ï¼é€šè¿‡æœ¬ç« çš„å­¦ä¹ ï¼Œä½ å·²ç»èƒ½å¤Ÿï¼š

- âœ… ç‹¬ç«‹æ„å»ºå®Œæ•´çš„AIåº”ç”¨
- âœ… é›†æˆå¤šç§AIæŠ€æœ¯
- âœ… éƒ¨ç½²ç”Ÿäº§çº§ç³»ç»Ÿ
- âœ… ä¼˜åŒ–æ€§èƒ½å’Œå®‰å…¨æ€§

---

## ä¸‹ä¸€æ­¥å­¦ä¹ 

### æ¨èèµ„æº

**æ¡†æ¶å’Œå·¥å…·**ï¼š
- LangChainå®˜æ–¹æ–‡æ¡£ï¼šhttps://python.langchain.com
- LlamaIndexï¼šhttps://docs.llamaindex.ai
- Haystackï¼šhttps://haystack.deepset.ai

**è¿›é˜¶ä¸»é¢˜**ï¼š
- å¤šæ¨¡æ€AIï¼ˆå›¾åƒã€éŸ³é¢‘ï¼‰
- å¾®è°ƒå¤§æ¨¡å‹
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- AIåº”ç”¨å®‰å…¨

**å®è·µå»ºè®®**ï¼š
1. ä»å°é¡¹ç›®å¼€å§‹ï¼Œé€æ­¥æ‰©å±•
2. å‚ä¸å¼€æºé¡¹ç›®
3. åŠ å…¥AIå¼€å‘è€…ç¤¾åŒº
4. æŒç»­å­¦ä¹ æ–°æŠ€æœ¯

---

**æ­å–œä½ å®Œæˆäº†AIåº”ç”¨å¼€å‘å®Œå…¨æŒ‡å—ï¼** ğŸ‰

ä»åŸºç¡€åˆ°å®æˆ˜ï¼Œä½ å·²ç»æŒæ¡äº†æ„å»ºç°ä»£AIåº”ç”¨çš„æ ¸å¿ƒæŠ€èƒ½ã€‚

**ç»§ç»­ä¿æŒå­¦ä¹ çš„çƒ­æƒ…ï¼Œåˆ›é€ æ›´å¤šæœ‰ä»·å€¼çš„AIåº”ç”¨ï¼**

---

**æ•™ç¨‹ç»“æŸ**
**æœ‰é—®é¢˜ï¼Ÿ** esimonx@163.com
**æ›´å¤šæ•™ç¨‹ï¼Ÿ** æŸ¥çœ‹ [Git](/git/)ã€[å‰ç«¯](/guide/) ç­‰å…¶ä»–ç« èŠ‚
