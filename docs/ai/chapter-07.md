# åº”ç”¨è¿›é˜¶

## æœ¬ç« å¯¼è¯»

æ­å–œä½ è¿›å…¥è¿›é˜¶å­¦ä¹ ï¼æœ¬ç« å°†ä»‹ç»AIåº”ç”¨å¼€å‘çš„å‰æ²¿æŠ€æœ¯å’Œé«˜çº§ä¸»é¢˜ï¼Œå¸®åŠ©ä½ æ„å»ºæ›´å¼ºå¤§ã€æ›´ä¸“ä¸šçš„AIåº”ç”¨ã€‚

**å­¦ä¹ ç›®æ ‡**ï¼š
- äº†è§£ä¸»æµLLMæ¨¡å‹çš„ç‰¹ç‚¹å’Œé€‰æ‹©ç­–ç•¥
- æŒæ¡Claudeç­‰å…¶ä»–æ¨¡å‹APIçš„ä½¿ç”¨
- å­¦ä¹ MCPåè®®å’ŒLangGraphæ¡†æ¶
- æŒæ¡AIåº”ç”¨çš„è¯„ä¼°å’Œæµ‹è¯•æ–¹æ³•

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š80åˆ†é’Ÿ

---

## ä¸»æµLLMæ¨¡å‹å¯¹æ¯” {#ä¸»æµllmæ¨¡å‹å¯¹æ¯”}

### æ¨¡å‹å…¨æ™¯å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            å¤§è¯­è¨€æ¨¡å‹ç”Ÿæ€å…¨æ™¯ (2024-2026)            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                     â”‚
â”‚  ğŸ”µ é—­æºå•†ä¸šæ¨¡å‹ (2024-2026)                        â”‚
â”‚  â”œâ”€â”€ GPT-4o / GPT-4o-mini (OpenAI)                 â”‚
â”‚  â”œâ”€â”€ Claude 3.5 Sonnet / Claude 4.0 (Anthropic)    â”‚
â”‚  â”œâ”€â”€ Gemini 2.0 Pro (Google)                       â”‚
â”‚  â””â”€â”€ DeepSeek-V3 (æ·±åº¦æ±‚ç´¢)                        â”‚
â”‚                                                     â”‚
â”‚  ğŸŸ¢ å¼€æºæ¨¡å‹ (2024-2026)                            â”‚
â”‚  â”œâ”€â”€ Llama 3.3 / 3.2 (Meta)                        â”‚
â”‚  â”œâ”€â”€ Qwen 2.5 (é˜¿é‡Œ)                               â”‚
â”‚  â”œâ”€â”€ Mistral Large 2 (Mistral AI)                  â”‚
â”‚  â”œâ”€â”€ DeepSeek-V2 / V3 (æ·±åº¦æ±‚ç´¢)                   â”‚
â”‚  â””â”€â”€ Phi-4 (Microsoft)                             â”‚
â”‚                                                     â”‚
â”‚  ğŸŸ¡ æœ¬åœ°éƒ¨ç½²æ¡†æ¶                                     â”‚
â”‚  â”œâ”€â”€ Ollama (è½»é‡çº§)                                â”‚
â”‚  â”œâ”€â”€ vLLM (é«˜æ€§èƒ½æ¨ç†)                              â”‚
â”‚  â”œâ”€â”€ LM Studio (å›¾å½¢ç•Œé¢)                           â”‚
â”‚  â””â”€â”€ TensorRT-LLM (NVIDIAä¼˜åŒ–)                     â”‚
â”‚                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸»æµæ¨¡å‹è¯¦ç»†å¯¹æ¯” (2024-2026)

| æ¨¡å‹ | å¼€å‘è€… | ä¸Šä¸‹æ–‡ | ä¼˜åŠ¿ | åŠ£åŠ¿ | ä»·æ ¼ | æœ€é€‚åˆåœºæ™¯ |
|------|--------|--------|------|------|------|-----------|
| **Claude 3.5 Sonnet** | Anthropic | 200K | ç»¼åˆæœ€å¼ºã€Artifacts | ä»·æ ¼é«˜ | $$$$ | å¤æ‚ä»»åŠ¡ã€ä»£ç ã€2024é¦–é€‰ |
| **Claude 4.0 Opus** | Anthropic | 200K+ | æœ€å¼ºæ¨ç†èƒ½åŠ› | ä»·æ ¼æé«˜ | $$$$$ | 2025æœ€å¤æ‚ä»»åŠ¡ |
| **GPT-4o** | OpenAI | 128K | å¤šæ¨¡æ€ã€é€Ÿåº¦å¿« | ä¸Šä¸‹æ–‡è¾ƒå° | $$$ | å¤šæ¨¡æ€åº”ç”¨ã€å®æ—¶å¯¹è¯ |
| **GPT-4o-mini** | OpenAI | 128K | æ€§ä»·æ¯”æœ€é«˜ | èƒ½åŠ›ç•¥å¼± | $ | é«˜é¢‘ç®€å•ä»»åŠ¡ |
| **DeepSeek-V3** | æ·±åº¦æ±‚ç´¢ | 128K | å…è´¹å¼€æºã€MoE | éœ€è¦éƒ¨ç½² | å…è´¹ | ä¸­æ–‡ã€æœ¬åœ°éƒ¨ç½² |
| **Llama 3.3 70B** | Meta | 128K | å¼€æºæœ€å¼º | éœ€è¦éƒ¨ç½² | å…è´¹ | æœ¬åœ°éƒ¨ç½² |
| **Qwen 2.5 72B** | é˜¿é‡Œ | 128K | ä¸­æ–‡æœ€å¼º | éœ€è¦éƒ¨ç½² | å…è´¹ | ä¸­æ–‡åœºæ™¯ |
| **Gemini 2.0 Pro** | Google | 1M+ | è¶…é•¿ä¸Šä¸‹æ–‡ | ç¨³å®šæ€§ | $$$$ | è¶…é•¿æ–‡æ¡£åˆ†æ |
| **Phi-4** | Microsoft | 128K | å°è€Œç¾ | èƒ½åŠ›æœ‰é™ | å…è´¹ | è¾¹ç¼˜è®¾å¤‡ |

### æ¨¡å‹é€‰æ‹©ç­–ç•¥ (2024-2026æ›´æ–°)

#### å†³ç­–æ ‘

```
å¼€å§‹é€‰æ‹© (2024-2026)
  â†“
éœ€è¦å¤„ç†è¶…é•¿æ–‡æ¡£(>500K tokens)?
  â”œâ”€ æ˜¯ â†’ Gemini 2.0 Pro (1M+ ä¸Šä¸‹æ–‡)
  â””â”€ å¦ â†“
      éœ€è¦æœ€å¼ºçš„ç»¼åˆèƒ½åŠ›(ä»£ç +æ¨ç†)?
      â”œâ”€ æ˜¯ â†’ Claude 3.5 Sonnet / Claude 4.0 Opus
      â””â”€ å¦ â†“
          éœ€è¦å¤šæ¨¡æ€(å›¾åƒ+éŸ³é¢‘+è§†é¢‘)?
          â”œâ”€ æ˜¯ â†’ GPT-4o
          â””â”€ å¦ â†“
              é¢„ç®—æœ‰é™æˆ–é«˜é¢‘è°ƒç”¨?
              â”œâ”€ æ˜¯ â†’ GPT-4o-mini / DeepSeek-V3
              â””â”€ å¦ â†“
                  éœ€è¦ä¸­æ–‡ä¼˜åŒ–?
                  â”œâ”€ æ˜¯ â†’ Qwen 2.5 / DeepSeek-V3
                  â””â”€ å¦ â†’ Claude 3.5 Sonnet
```

#### 2024-2026æ¨èç»„åˆ

**ä¸ªäººå¼€å‘è€…/å°å›¢é˜Ÿ**ï¼š
```yaml
ä¸»åŠ›æ¨¡å‹: Claude 3.5 Sonnet
  - ç»¼åˆèƒ½åŠ›æœ€å¼º
  - ArtifactsåŠŸèƒ½å¼ºå¤§
  - é€‚åˆå¤æ‚ä»»åŠ¡

å¤‡ç”¨æ¨¡å‹: GPT-4o-mini
  - æ€§ä»·æ¯”æœ€é«˜
  - é«˜é¢‘ç®€å•ä»»åŠ¡
  - é™ä½æˆæœ¬

æœ¬åœ°æ¨¡å‹: DeepSeek-V3
  - æ•æ„Ÿæ•°æ®å¤„ç†
  - ç¦»çº¿ç¯å¢ƒ
  - é›¶æˆæœ¬
```

**ä¼ä¸šçº§åº”ç”¨**ï¼š
```yaml
æ ¸å¿ƒä¸šåŠ¡: Claude 4.0 Opus
  - æœ€å¼ºæ¨ç†èƒ½åŠ›
  - å…³é”®å†³ç­–ä»»åŠ¡

å¤šæ¨¡æ€: GPT-4o
  - å›¾åƒã€éŸ³é¢‘å¤„ç†
  - å®æ—¶äº¤äº’

å¾®è°ƒæ¨¡å‹: Qwen 2.5 / Llama 3.3
  - é¢†åŸŸä¸“å®¶
  - éƒ¨ç½²åœ¨å†…ç½‘
```

#### å…·ä½“å»ºè®® (2024-2026)

```python
# åœºæ™¯1ï¼šä»£ç ç”Ÿæˆå’Œè°ƒè¯• (2024-2026é¦–é€‰)
æ¨èæ¨¡å‹ï¼šClaude 3.5 Sonnet
ç†ç”±ï¼š
- 2024å¹´ä»£ç èƒ½åŠ›æœ€å¼º
- ArtifactsåŠŸèƒ½å¯ç›´æ¥è¿è¡Œä»£ç 
- ç†è§£å¤æ‚æ¶æ„
- æ”¯æŒå¤§å‹é¡¹ç›®é‡æ„

å¤‡é€‰ï¼šGPT-4o (å®æ—¶æ€§è¦æ±‚é«˜)

# åœºæ™¯2ï¼šé•¿æ–‡æ¡£åˆ†æ
æ¨èæ¨¡å‹ï¼šClaude 3.5 Sonnet / Gemini 2.0 Pro
ç†ç”±ï¼š
- Claude: 200Kä¸Šä¸‹æ–‡ï¼Œåˆ†ææ·±å…¥
- Gemini: è¶…é•¿æ–‡æ¡£(>500K tokens)
- ä¸æ˜“é—æ¼ç»†èŠ‚
- æ”¯æŒå¤šæ–‡æ¡£å¯¹æ¯”

# åœºæ™¯3ï¼šé«˜é¢‘èŠå¤©æœºå™¨äºº (2024-2026æˆæœ¬ä¼˜åŒ–)
æ¨èæ¨¡å‹ï¼šGPT-4o-mini / DeepSeek-V3
ç†ç”±ï¼š
- GPT-4o-mini: æ€§ä»·æ¯”æœ€é«˜($0.15/1M tokens)
- DeepSeek-V3: å¼€æºå…è´¹
- å“åº”é€Ÿåº¦å¿«
- ç”¨æˆ·ä½“éªŒå¥½

# åœºæ™¯4ï¼šæ•æ„Ÿæ•°æ®å¤„ç† (2024-2026æœ¬åœ°éƒ¨ç½²)
æ¨èæ¨¡å‹ï¼šDeepSeek-V3 / Qwen 2.5 72B
ç†ç”±ï¼š
- MoEæ¶æ„ï¼Œæ€§èƒ½æ¥è¿‘GPT-4
- æ•°æ®ä¸å‡ºåŸŸ
- éšç§å®‰å…¨
- éƒ¨ç½²æˆæœ¬å¯æ§

# åœºæ™¯5ï¼šå¤šæ¨¡æ€åº”ç”¨ (2024-2026åŸç”Ÿå¤šæ¨¡æ€)
æ¨èæ¨¡å‹ï¼šGPT-4o / Gemini 2.0 Pro
ç†ç”±ï¼š
- åŸç”Ÿå¤šæ¨¡æ€(å›¾åƒ+éŸ³é¢‘+è§†é¢‘)
- å®æ—¶è¯­éŸ³å¯¹è¯
- ç†è§£è§†é¢‘å†…å®¹
- ç»¼åˆèƒ½åŠ›å¼º

# åœºæ™¯6ï¼šAI Agentå¼€å‘ (2024-2026æ–°åœºæ™¯)
æ¨èæ¨¡å‹ï¼šClaude 3.5 Sonnet
ç†ç”±ï¼š
- æ¨ç†èƒ½åŠ›å¼º
- å·¥å…·è°ƒç”¨ç¨³å®š
- æ”¯æŒå¤æ‚å†³ç­–
- é€‚åˆå¤šAgentåä½œ

# åœºæ™¯7ï¼šå‰ç«¯+AIå¼€å‘ (2024-2026çƒ­é—¨)
æ¨èæ¨¡å‹ï¼šClaude 3.5 Sonnet + Artifacts
ç†ç”±ï¼š
- Artifactså®æ—¶é¢„è§ˆ
- ç”Ÿæˆå®Œæ•´Vue/Reactç»„ä»¶
- æ”¯æŒè¿­ä»£ä¿®æ”¹
- å¼€å‘æ•ˆç‡æå‡10å€
```

---

## 2024-2026 AIæŠ€æœ¯çƒ­ç‚¹

### Claude 3.5 Sonnet - 2024å¹´åº¦æ¨¡å‹

**æ ¸å¿ƒç‰¹æ€§**ï¼š

```python
import anthropic

client = anthropic.Anthropic(api_key="your-api-key")

# 1. ArtifactsåŠŸèƒ½ - ä»£ç å®æ—¶é¢„è§ˆ
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=4096,
    messages=[{
        "role": "user",
        "content": "åˆ›å»ºä¸€ä¸ªVue3å¾…åŠäº‹é¡¹ç»„ä»¶ï¼Œæ”¯æŒæ‹–æ‹½æ’åº"
    }]
)

# Artifactsä¼šåœ¨å³ä¾§é¢æ¿å®æ—¶æ¸²æŸ“ä»£ç 
# æ”¯æŒHTML/CSS/JavaScript/Vue/Reactç­‰

# 2. è¶…é•¿ä¸Šä¸‹æ–‡å¤„ç†
long_text = open("huge_document.txt").read()  # 200K tokens

response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=8192,
    messages=[{
        "role": "user",
        "content": f"åˆ†æä»¥ä¸‹æ–‡æ¡£å¹¶æå–å…³é”®ä¿¡æ¯ï¼š\n{long_text}"
    }]
)

# 3. å¤æ‚ä»£ç é‡æ„
response = client.messages.create(
    model="claude-3-5-sonnet-20241022",
    max_tokens=16384,
    messages=[{
        "role": "user",
        "content": """
        æˆ‘æœ‰ä¸€ä¸ªå¤§å‹Vueé¡¹ç›®ï¼Œè¯·å¸®æˆ‘ï¼š
        1. å°†Options APIæ”¹ä¸ºComposition API
        2. æ·»åŠ TypeScriptç±»å‹
        3. ä¼˜åŒ–æ€§èƒ½
        [é¡¹ç›®ä»£ç ...]
        """
    }]
)
```

**Artifactså®æˆ˜æ¡ˆä¾‹**ï¼š

```vue
<!-- Claude 3.5 Sonnetç”Ÿæˆçš„Vueç»„ä»¶ -->
<template>
  <div class="todo-app">
    <h2>{{ title }}</h2>
    <input
      v-model="newTodo"
      @keyup.enter="addTodo"
      placeholder="æ·»åŠ æ–°ä»»åŠ¡..."
    />
    <TransitionGroup name="list" tag="ul">
      <li
        v-for="todo in sortedTodos"
        :key="todo.id"
        draggable="true"
        @dragstart="onDragStart($event, todo)"
        @drop="onDrop($event, todo)"
        @dragover.prevent
      >
        <input
          type="checkbox"
          v-model="todo.completed"
        />
        <span
          :class="{ completed: todo.completed }"
        >
          {{ todo.text }}
        </span>
        <button @click="removeTodo(todo.id)">
          åˆ é™¤
        </button>
      </li>
    </TransitionGroup>
  </div>
</template>

<script setup lang="ts">
import { ref, computed } from 'vue'

interface Todo {
  id: number
  text: string
  completed: boolean
  order: number
}

const title = ref('å¾…åŠäº‹é¡¹')
const newTodo = ref('')
const todos = ref<Todo[]>([])
const draggedItem = ref<Todo | null>(null)

const sortedTodos = computed(() =>
  [...todos.value].sort((a, b) => a.order - b.order)
)

function addTodo() {
  if (!newTodo.value.trim()) return
  todos.value.push({
    id: Date.now(),
    text: newTodo.value,
    completed: false,
    order: todos.value.length
  })
  newTodo.value = ''
}

function onDragStart(event: DragEvent, todo: Todo) {
  draggedItem.value = todo
}

function onDrop(event: DragEvent, targetTodo: Todo) {
  if (!draggedItem.value) return
  // æ‹–æ‹½æ’åºé€»è¾‘...
}
</script>

<style scoped>
.todo-app {
  max-width: 500px;
  margin: 0 auto;
  padding: 20px;
}

.list-move,
.list-enter-active,
.list-leave-active {
  transition: all 0.5s ease;
}

.list-enter-from,
.list-leave-to {
  opacity: 0;
  transform: translateX(30px);
}

.completed {
  text-decoration: line-through;
  color: #999;
}
</style>
```

### GPT-4o - åŸç”Ÿå¤šæ¨¡æ€æ¨¡å‹

**å®æ—¶è¯­éŸ³å¯¹è¯**ï¼š

```python
from openai import OpenAI
import pyaudio

client = OpenAI(api_key="your-api-key")

# å®æ—¶è¯­éŸ³å¯¹è¯
def real_time_voice_chat():
    """GPT-4oå®æ—¶è¯­éŸ³å¯¹è¯"""

    # 1. å½•éŸ³
    audio = record_audio()

    # 2. è½¬æ–‡å­—(Whisper)
    transcript = client.audio.transcriptions.create(
        model="whisper-1",
        file=audio
    )

    # 3. GPT-4oç”Ÿæˆå›å¤
    response = client.chat.completions.create(
        model="gpt-4o",  # åŸç”Ÿå¤šæ¨¡æ€
        messages=[{
            "role": "user",
            "content": transcript.text
        }]
    )

    # 4. æ–‡å­—è½¬è¯­éŸ³(TTS)
    speech = client.audio.speech.create(
        model="tts-1",
        voice="alloy",
        input=response.choices[0].message.content
    )

    # 5. æ’­æ”¾
    play_audio(speech)

# å®æ—¶è§†é¢‘ç†è§£
def analyze_video(video_path: str):
    """GPT-4oåˆ†æè§†é¢‘å†…å®¹"""

    # æå–è§†é¢‘å¸§
    frames = extract_video_frames(video_path)

    # é€å¸§åˆ†æ
    responses = []
    for frame in frames:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{frame}"
                        }
                    },
                    {
                        "type": "text",
                        "text": "æè¿°è¿™ä¸€å¸§å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ"
                    }
                ]
            }]
        )
        responses.append(response.choices[0].message.content)

    return responses
```

### AI Agentæ¡†æ¶å¯¹æ¯” (2024-2026)

| æ¡†æ¶ | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ | å­¦ä¹ æ›²çº¿ |
|------|------|----------|----------|
| **LangGraph** | çŠ¶æ€å›¾ã€å¯è§†åŒ– | å¤æ‚Agentæµç¨‹ | ä¸­ç­‰ |
| **AutoGen** | å¤šAgentå¯¹è¯ | åä½œä»»åŠ¡ | è¾ƒä½ |
| **CrewAI** | è§’è‰²æ˜ç¡® | ä¸“ä¸šå›¢é˜Ÿæ¨¡æ‹Ÿ | è¾ƒä½ |
| **Semantic Kernel** | ä¼ä¸šçº§ | å¾®è½¯ç”Ÿæ€ | è¾ƒé«˜ |

**LangGraphå®æˆ˜** (2024çƒ­é—¨)ï¼š

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict

# 1. å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    task: str
    research_result: str
    code: str
    test_result: str
    final_output: str

# 2. å®šä¹‰èŠ‚ç‚¹
def researcher(state: AgentState):
    """ç ”ç©¶èŠ‚ç‚¹"""
    # ä½¿ç”¨Claude 3.5 Sonnetè¿›è¡Œç ”ç©¶
    result = claude_client.messages.create(
        model="claude-3-5-sonnet-20241022",
        messages=[{
            "role": "user",
            "content": f"ç ”ç©¶ä»¥ä¸‹ä»»åŠ¡çš„æœ€ä½³å®è·µï¼š\n{state['task']}"
        }]
    )
    state["research_result"] = result.content[0].text
    return state

def coder(state: AgentState):
    """ç¼–ç èŠ‚ç‚¹"""
    result = claude_client.messages.create(
        model="claude-3-5-sonnet-20241022",
        messages=[{
            "role": "user",
            "content": f"""
            æ ¹æ®ç ”ç©¶ç»“æœç¼–å†™ä»£ç ï¼š

            ç ”ç©¶ï¼š{state['research_result']}
            ä»»åŠ¡ï¼š{state['task']}

            è¦æ±‚ï¼š
            1. ä½¿ç”¨Vue3 + TypeScript
            2. éµå¾ªæœ€ä½³å®è·µ
            3. åŒ…å«å®Œæ•´ç±»å‹å®šä¹‰
            """
        }]
    )
    state["code"] = result.content[0].text
    return state

def tester(state: AgentState):
    """æµ‹è¯•èŠ‚ç‚¹"""
    # ç”Ÿæˆæµ‹è¯•ä»£ç 
    result = claude_client.messages.create(
        model="claude-3-5-sonnet-20241022",
        messages=[{
            "role": "user",
            "content": f"ä¸ºä»¥ä¸‹ä»£ç ç”Ÿæˆå•å…ƒæµ‹è¯•ï¼š\n{state['code']}"
        }]
    )
    state["test_result"] = result.content[0].text
    return state

# 3. æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("researcher", researcher)
workflow.add_node("coder", coder)
workflow.add_node("tester", tester)

# æ·»åŠ è¾¹
workflow.add_edge("researcher", "coder")
workflow.add_edge("coder", "tester")
workflow.add_edge("tester", END)

# è®¾ç½®å…¥å£
workflow.set_entry_point("researcher")

# 4. ç¼–è¯‘å’Œè¿è¡Œ
app = workflow.compile()

# æ‰§è¡Œ
result = app.invoke({
    "task": "åˆ›å»ºä¸€ä¸ªæ”¯æŒæ‹–æ‹½çš„å¾…åŠäº‹é¡¹ç»„ä»¶"
})

print(result["final_output"])
```

**AutoGenå¤šAgentåä½œ** (2024çƒ­é—¨)ï¼š

```python
import autogen

# 1. å®šä¹‰åŠ©æ‰‹
assistant = autogen.AssistantAgent(
    name="assistant",
    llm_config={
        "model": "claude-3-5-sonnet-20241022",
        "api_key": "your-api-key"
    }
)

# 2. å®šä¹‰ä»£ç æ‰§è¡Œå™¨
user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    max_consecutive_auto_reply=10,
    code_execution_config={
        "work_dir": "coding",
        "use_docker": False
    }
)

# 3. Agentå¯¹è¯
user_proxy.initiate_chat(
    assistant,
    message="""
    è¯·å¸®æˆ‘åˆ›å»ºä¸€ä¸ªVue3ç»„ä»¶ï¼š
    1. å¾…åŠäº‹é¡¹åˆ—è¡¨
    2. æ”¯æŒæ·»åŠ ã€åˆ é™¤ã€å®Œæˆ
    3. ä½¿ç”¨TypeScript
    4. åŒ…å«å•å…ƒæµ‹è¯•
    """
)

# AutoGenä¼šè‡ªåŠ¨ï¼š
# 1. åˆ†æéœ€æ±‚
# 2. ç¼–å†™ä»£ç 
# 3. è¿è¡Œæµ‹è¯•
# 4. ä¿®å¤é”™è¯¯
# 5. è¿­ä»£ä¼˜åŒ–
```

### æœ¬åœ°æ¨¡å‹éƒ¨ç½² (2024-2025æˆç†Ÿæ–¹æ¡ˆ)

**Ollama + vLLMç»„åˆ**ï¼š

```bash
# 1. å®‰è£…Ollama(æœ€ç®€å•)
curl -fsSL https://ollama.ai/install.sh | sh

# æ‹‰å–æ¨¡å‹
ollama pull deepseek-v3:70b
ollama pull qwen2.5:72b
ollama pull llama3.3:70b

# è¿è¡Œæœ¬åœ°æ¨¡å‹
ollama run deepseek-v3:70b

# 2. APIæœåŠ¡(ä¸OpenAIå…¼å®¹)
ollama serve

# 3. ä½¿ç”¨(ä»£ç æ— éœ€ä¿®æ”¹)
import openai

client = openai.OpenAI(
    base_url="http://localhost:11434/v1",
    api_key="ollama"  # required but unused
)

response = client.chat.completions.create(
    model="deepseek-v3:70b",
    messages=[{
        "role": "user",
        "content": "ä½ å¥½"
    }]
)
```

**vLLMé«˜æ€§èƒ½æ¨ç†** (2024ä¼ä¸šé¦–é€‰)ï¼š

```python
from vllm import LLM, SamplingParams

# åˆå§‹åŒ–æ¨¡å‹
llm = LLM(
    model="deepseek-ai/DeepSeek-V3",
    tensor_parallel_size=4,  # 4å¡GPU
    max_model_len=128000,
    gpu_memory_utilization=0.9
)

# æ‰¹é‡æ¨ç†
prompts = [
    "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
    "å¦‚ä½•å­¦ä¹ Vue3ï¼Ÿ",
    "è§£é‡Šä»€ä¹ˆæ˜¯DevOps"
]

sampling_params = SamplingParams(
    temperature=0.7,
    top_p=0.9,
    max_tokens=1024
)

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}\n")
```

**æ¨¡å‹é‡åŒ–** (2024-2025é™ä½æˆæœ¬)ï¼š

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# 1. é‡åŒ–åˆ°4-bit(Reduce 75% memory)
model = AutoModelForCausalLM.from_pretrained(
    "deepseek-ai/DeepSeek-V3",
    load_in_4bit=True,  # 4-bité‡åŒ–
    device_map="auto"
)

# 2. AWQé‡åŒ–(æ›´å¥½çš„æ€§èƒ½)
# pip install autoawq
from awq import AutoAWQForCausalLM

model = AutoAWQForCausalLM.from_quantized(
    "deepseek-ai/DeepSeek-V3-AWQ",
    fuse_layers=True,
    safetensors=True
)

# ç¡¬ä»¶è¦æ±‚å¯¹æ¯”:
# FP16: 70Bæ¨¡å‹éœ€è¦140GB VRAM
# 8-bit: 70Bæ¨¡å‹éœ€è¦70GB VRAM
# 4-bit: 70Bæ¨¡å‹éœ€è¦35GB VRAM (2x RTX 3090)
```

---

## Claude APIä½¿ç”¨ {#claude-apiä½¿ç”¨}

### Claudeç®€ä»‹

**Claude** æ˜¯Anthropicå¼€å‘çš„AIåŠ©æ‰‹ï¼Œä»¥å®‰å…¨æ€§ã€é•¿æ–‡æœ¬å¤„ç†èƒ½åŠ›è‘—ç§°ã€‚

**Claude 3ç³»åˆ—**ï¼š
- **Opus**ï¼šæœ€å¼ºï¼Œ200Kä¸Šä¸‹æ–‡
- **Sonnet**ï¼šå¹³è¡¡ï¼Œ200Kä¸Šä¸‹æ–‡
- **Haiku**ï¼šæœ€å¿«ï¼Œ100Kä¸Šä¸‹æ–‡

### å®‰è£…å’Œé…ç½®

```bash
# å®‰è£…
pip install anthropic

# è®¾ç½®ç¯å¢ƒå˜é‡
export ANTHROPIC_API_KEY="your-api-key"
```

```python
# config.py
import os
from dotenv import load_dotenv

load_dotenv()

class ClaudeConfig:
    API_KEY = os.getenv("ANTHROPIC_API_KEY")

    # æ¨¡å‹é€‰æ‹©
    MODEL_OPUS = "claude-3-opus-20240229"
    MODEL_SONNET = "claude-3-sonnet-20240229"
    MODEL_HAIKU = "claude-3-haiku-20240307"

    # é»˜è®¤æ¨¡å‹
    DEFAULT_MODEL = MODEL_SONNET

    @classmethod
    def get_model(cls, tier: str = "sonnet"):
        models = {
            "opus": cls.MODEL_OPUS,
            "sonnet": cls.MODEL_SONNET,
            "haiku": cls.MODEL_HAIKU
        }
        return models.get(tier, cls.DEFAULT_MODEL)
```

### åŸºç¡€ä½¿ç”¨

```python
from anthropic import Anthropic
from config import ClaudeConfig

# åˆå§‹åŒ–
client = Anthropic(api_key=ClaudeConfig.API_KEY)

# ç®€å•å¯¹è¯
message = client.messages.create(
    model=ClaudeConfig.DEFAULT_MODEL,
    max_tokens=1024,
    messages=[
        {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ]
)

print(message.content[0].text)
# è¾“å‡ºï¼šä½ å¥½ï¼æˆ‘æ˜¯Claudeï¼Œç”±Anthropicå…¬å¸å¼€å‘çš„AIåŠ©æ‰‹...
```

### æµå¼è¾“å‡º

```python
def stream_chat(prompt: str):
    """æµå¼å¯¹è¯"""
    print("Claudeï¼š", end="", flush=True)

    with client.messages.stream(
        model=ClaudeConfig.DEFAULT_MODEL,
        max_tokens=1024,
        messages=[{"role": "user", "content": prompt}]
    ) as stream:
        for text in stream.text_stream:
            print(text, end="", flush=True)

    print()  # æ¢è¡Œ

# ä½¿ç”¨
stream_chat("ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åº")
```

### é•¿æ–‡æ¡£å¤„ç†ï¼ˆClaudeçš„å¼ºé¡¹ï¼‰

```python
def analyze_long_document(document_path: str):
    """åˆ†æé•¿æ–‡æ¡£"""
    # è¯»å–æ–‡æ¡£
    with open(document_path, 'r', encoding='utf-8') as f:
        document = f.read()

    # Claudeå¯ä»¥å¤„ç†è¶…é•¿æ–‡æ¡£ï¼ˆ200K tokens â‰ˆ 15ä¸‡æ±‰å­—ï¼‰
    message = client.messages.create(
        model=ClaudeConfig.MODEL_OPUS,  # ä½¿ç”¨Opusè·å¾—æœ€ä½³æ•ˆæœ
        max_tokens=4096,
        messages=[{
            "role": "user",
            "content": f"""è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼Œæå–å…³é”®ä¿¡æ¯ï¼š

{document}

è¯·æä¾›ï¼š
1. æ–‡æ¡£æ‘˜è¦
2. å…³é”®è¦ç‚¹ï¼ˆæœ€å¤š10æ¡ï¼‰
3. ä¸»è¦ç»“è®º
4. éœ€è¦å…³æ³¨çš„ç»†èŠ‚
"""
        }]
    )

    return message.content[0].text

# ä½¿ç”¨
result = analyze_long_document("long_document.txt")
print(result)
```

### å¤šå›¾ç†è§£ï¼ˆå¤šæ¨¡æ€ï¼‰

```python
import base64

def encode_image(image_path: str) -> str:
    """ç¼–ç å›¾ç‰‡"""
    with open(image_path, "rb") as f:
        return base64.b64encode(f.read()).decode('utf-8')

def analyze_image(image_path: str, question: str):
    """åˆ†æå›¾ç‰‡"""
    image_data = encode_image(image_path)

    message = client.messages.create(
        model=ClaudeConfig.MODEL_SONNET,
        max_tokens=1024,
        messages=[{
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/png",
                        "data": image_data
                    }
                },
                {
                    "type": "text",
                    "text": question
                }
            ]
        }]
    )

    return message.content[0].text

# ä½¿ç”¨
result = analyze_image(
    "screenshot.png",
    "è¯·æè¿°è¿™ä¸ªç•Œé¢çš„å¸ƒå±€ï¼Œå¹¶æå‡ºæ”¹è¿›å»ºè®®"
)
print(result)
```

### LangChainä¸­ä½¿ç”¨Claude

```python
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import ChatPromptTemplate

# åˆå§‹åŒ–
llm = ChatAnthropic(
    model=ClaudeConfig.MODEL_SONNET,
    api_key=ClaudeConfig.API_KEY,
    temperature=0.7,
    max_tokens=1024
)

# åˆ›å»ºé“¾
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸“ä¸šçš„AIåŠ©æ‰‹"),
    ("human", "{input}")
])

chain = prompt | llm

# ä½¿ç”¨
response = chain.invoke({"input": "è§£é‡Šä»€ä¹ˆæ˜¯RAG"})
print(response.content)
```

### Claude vs GPTå¯¹æ¯”

| ç‰¹æ€§ | Claude 3 | GPT-4 |
|------|----------|-------|
| **ä¸Šä¸‹æ–‡é•¿åº¦** | 200K (Opus/Sonnet) | 128K |
| **é•¿æ–‡æœ¬å¤„ç†** | â­â­â­â­â­ | â­â­â­â­ |
| **ä»£ç èƒ½åŠ›** | â­â­â­â­ | â­â­â­â­â­ |
| **ä¸­æ–‡æ”¯æŒ** | â­â­â­â­ | â­â­â­â­â­ |
| **å®‰å…¨æ€§** | â­â­â­â­â­ | â­â­â­â­ |
| **ä»·æ ¼** | è¾ƒé«˜ | è¾ƒé«˜ |
| **è¾“å‡ºé€Ÿåº¦** | â­â­â­â­ | â­â­â­ |

**é€‰æ‹©å»ºè®®**ï¼š
```python
# ä½¿ç”¨Claudeçš„åœºæ™¯ï¼š
- å¤„ç†è¶…é•¿æ–‡æ¡£ï¼ˆ>100é¡µï¼‰
- éœ€è¦æ·±åº¦åˆ†æå’Œæ¨ç†
- å¯¹å®‰å…¨æ€§è¦æ±‚é«˜
- ä¸ç¡®å®šçš„é•¿æ–‡æœ¬ä»»åŠ¡

# ä½¿ç”¨GPT-4çš„åœºæ™¯ï¼š
- ä»£ç ç”Ÿæˆå’Œè°ƒè¯•
- éœ€è¦æœ€å¿«çš„å“åº”
- å¤æ‚é€»è¾‘æ¨ç†
- ä¸OpenAIç”Ÿæ€é›†æˆ
```

---

## å¼€æºæ¨¡å‹å’Œæœ¬åœ°éƒ¨ç½² {#å¼€æºæ¨¡å‹å’Œæœ¬åœ°éƒ¨ç½²}

### Ollamaï¼šæœ€ç®€å•çš„æœ¬åœ°éƒ¨ç½²

**å®‰è£…Ollama**ï¼š
```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows
# ä» https://ollama.com/download ä¸‹è½½å®‰è£…
```

**æ‹‰å–æ¨¡å‹**ï¼š
```bash
# Llama 3 (æ¨è)
ollama pull llama3

# Qwen (ä¸­æ–‡)
ollama pull qwen

# Mistral
ollama pull mistral

# ä»£ç æ¨¡å‹
ollama pull deepseek-coder
```

**ä½¿ç”¨Ollama**ï¼š
```bash
# å‘½ä»¤è¡Œäº¤äº’
ollama run llama3

# APIæœåŠ¡ï¼ˆé»˜è®¤ç«¯å£11434ï¼‰
ollama serve
```

```python
# Pythonä¸­ä½¿ç”¨
import requests
import json

def chat_with_ollama(prompt: str, model: str = "llama3"):
    """ä½¿ç”¨OllamaèŠå¤©"""
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": model,
            "prompt": prompt,
            "stream": False
        }
    )

    return response.json()['response']

# ä½¿ç”¨
print(chat_with_ollama("ä½ å¥½"))
```

**LangChainé›†æˆ**ï¼š
```python
from langchain_community.llms import Ollama

# åˆå§‹åŒ–
llm = Ollama(model="llama3")

# ä½¿ç”¨
response = llm.invoke("è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ")
print(response)
```

### æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | æ˜¾å­˜éœ€æ±‚ | é€Ÿåº¦ | è´¨é‡ |
|------|--------|----------|------|------|
| **Llama 3 8B** | 8B | 6GB | â­â­â­â­â­ | â­â­â­ |
| **Llama 3 70B** | 70B | 40GB | â­â­â­ | â­â­â­â­â­ |
| **Mistral 7B** | 7B | 5GB | â­â­â­â­â­ | â­â­â­â­ |
| **Qwen 72B** | 72B | 48GB | â­â­â­ | â­â­â­â­â­ |

**ç¡¬ä»¶å»ºè®®**ï¼š
```python
# 8GBæ˜¾å­˜ï¼šLlama 3 8B, Mistral 7B
# 16GBæ˜¾å­˜ï¼šLlama 3 70B (é‡åŒ–ç‰ˆ)
# 24GB+æ˜¾å­˜ï¼šLlama 3 70Bå®Œæ•´ç‰ˆ
# æ— GPUï¼šä½¿ç”¨Ollama CPUæ¨¡å¼ï¼ˆè¾ƒæ…¢ï¼‰
```

### æˆæœ¬å¯¹æ¯”

```
åœºæ™¯ï¼šå¤„ç†100ä¸‡ä¸ªtokens

OpenAI GPT-3.5ï¼š
  è¾“å…¥ï¼š$0.0005/1K Ã— 1000 = $0.5
  è¾“å‡ºï¼š$0.0015/1K Ã— 1000 = $1.5
  æ€»è®¡ï¼š$2

Claude 3 Sonnetï¼š
  è¾“å…¥ï¼š$3/1M Ã— 1 = $3
  è¾“å‡ºï¼š$15/1M Ã— 1 = $15
  æ€»è®¡ï¼š$18

æœ¬åœ°éƒ¨ç½²ï¼ˆLlama 3ï¼‰ï¼š
  GPUç”µè´¹ï¼šçº¦$0.1
  æ€»è®¡ï¼š$0.1 ğŸ’°

ç»“è®ºï¼šé«˜é¢‘ä½¿ç”¨åœºæ™¯ï¼Œæœ¬åœ°éƒ¨ç½²æœ€ç»æµ
```

### Ollama Docker éƒ¨ç½²

**ä¸ºä»€ä¹ˆä½¿ç”¨ Docker éƒ¨ç½² Ollama**ï¼Ÿ
- ğŸ³ ç¯å¢ƒéš”ç¦»ï¼Œä¸æ±¡æŸ“ç³»ç»Ÿ
- ğŸš€ å¿«é€Ÿéƒ¨ç½²å’Œæ‰©ç¼©å®¹
- ğŸ”§ ä¾¿äºé…ç½®å’Œç®¡ç†
- ğŸ“¦ ç‰ˆæœ¬æ§åˆ¶æ–¹ä¾¿

#### æ–¹æ¡ˆ 1ï¼šåŸºç¡€ Docker éƒ¨ç½²

**å¿«é€Ÿå¯åŠ¨**ï¼š

```bash
# æ‹‰å– Ollama å®˜æ–¹é•œåƒ
docker pull ollama/ollama:latest

# è¿è¡Œå®¹å™¨
docker run -d \
  --name ollama \
  -p 11434:11434 \
  -v ollama_models:/root/.ollama \
  ollama/ollama:latest

# è¿›å…¥å®¹å™¨ä¸‹è½½æ¨¡å‹
docker exec -it ollama ollama pull llama3

# æµ‹è¯•
curl http://localhost:11434/api/generate -d '{
  "model": "llama3",
  "prompt": "ä½ å¥½",
  "stream": false
}'
```

#### æ–¹æ¡ˆ 2ï¼šDocker Compose éƒ¨ç½²ï¼ˆæ¨èï¼‰

**`docker-compose.yml`**ï¼š

```yaml
version: '3.8'

services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*  # å…è®¸æ‰€æœ‰æ¥æºè®¿é—®
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - ai_network
    # GPU æ”¯æŒï¼ˆå¯é€‰ï¼‰
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

  # Open WebUIï¼ˆå¯é€‰ï¼šWeb ç•Œé¢ï¼‰
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    restart: unless-stopped
    ports:
      - "3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
    volumes:
      - open_webui_data:/app/backend/data
    depends_on:
      - ollama
    networks:
      - ai_network

volumes:
  ollama_data:
    driver: local
  open_webui_data:
    driver: local

networks:
  ai_network:
    driver: bridge
```

**å¯åŠ¨å’Œç®¡ç†**ï¼š

```bash
# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f ollama

# ä¸‹è½½æ¨¡å‹
docker exec -it ollama ollama pull llama3
docker exec -it ollama ollama pull qwen
docker exec -it ollama ollama pull mistral

# æŸ¥çœ‹å·²ä¸‹è½½çš„æ¨¡å‹
docker exec -it ollama ollama list

# åœæ­¢æœåŠ¡
docker-compose down

# åˆ é™¤æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬æ¨¡å‹ï¼‰
docker-compose down -v
```

#### æ–¹æ¡ˆ 3ï¼šå¸¦ GPU åŠ é€Ÿçš„éƒ¨ç½²

**NVIDIA GPU æ”¯æŒ**ï¼š

```yaml
# docker-compose.gpu.yml
version: '3.8'

services:
  ollama-gpu:
    image: ollama/ollama:latest
    container_name: ollama-gpu
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    volumes:
      - ollama_gpu_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - ai_network

volumes:
  ollama_gpu_data:
    driver: local

networks:
  ai_network:
    driver: bridge
```

**å¯åŠ¨ GPU ç‰ˆæœ¬**ï¼š

```bash
# éœ€è¦å…ˆå®‰è£… nvidia-docker
# å®‰è£…ï¼šhttps://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html

# å¯åŠ¨
docker-compose -f docker-compose.gpu.yml up -d

# æ£€æŸ¥ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi

# æŸ¥çœ‹ Ollama GPU ä½¿ç”¨
docker exec -it ollama-gpu ollama ps
```

#### æ–¹æ¡ˆ 4ï¼šç”Ÿäº§çº§éƒ¨ç½²ï¼ˆå¸¦ Nginx åå‘ä»£ç†ï¼‰

**æ¶æ„**ï¼š

```
                    Internet
                       â†“
                 [Nginx :443]
                       â†“
              [Ollama :11434]
                       â†“
              [æ¨¡å‹å­˜å‚¨å·]
```

**`docker-compose.prod.yml`**ï¼š

```yaml
version: '3.8'

services:
  # Nginx åå‘ä»£ç†
  nginx:
    image: nginx:alpine
    container_name: ollama-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx_logs:/var/log/nginx
    depends_on:
      - ollama
    networks:
      - ai_network

  # Ollama æœåŠ¡
  ollama:
    image: ollama/ollama:latest
    container_name: ollama-prod
    restart: unless-stopped
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=https://your-domain.com
    volumes:
      - ollama_prod_data:/root/.ollama
    networks:
      - ai_network
    # ä¸å¯¹å¤–æš´éœ²ç«¯å£ï¼Œåªé€šè¿‡ Nginx è®¿é—®
    expose:
      - "11434"

  # Prometheus ç›‘æ§ï¼ˆå¯é€‰ï¼‰
  prometheus:
    image: prom/prometheus:latest
    container_name: ollama-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - ai_network

volumes:
  ollama_prod_data:
    driver: local
  nginx_logs:
    driver: local
  prometheus_data:
    driver: local

networks:
  ai_network:
    driver: bridge
```

**Nginx é…ç½®** (`nginx/nginx.conf`):

```nginx
events {
    worker_connections 1024;
}

http {
    upstream ollama_backend {
        server ollama:11434;
    }

    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;

    server {
        listen 80;
        server_name your-domain.com;
        return 301 https://$server_name$request_uri;
    }

    server {
        listen 443 ssl http2;
        server_name your-domain.com;

        ssl_certificate /etc/nginx/ssl/cert.pem;
        ssl_certificate_key /etc/nginx/ssl/key.pem;
        ssl_protocols TLSv1.2 TLSv1.3;

        # API ç«¯ç‚¹
        location /api/ {
            limit_req zone=api_limit burst=20 nodelay;

            proxy_pass http://ollama_backend/api/;
            proxy_http_version 1.1;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # è¶…æ—¶é…ç½®ï¼ˆæµå¼å“åº”ï¼‰
            proxy_read_timeout 3600s;
            proxy_send_timeout 3600s;
            chunked_transfer_encoding on;
        }

        # å¥åº·æ£€æŸ¥
        location /health {
            proxy_pass http://ollama_backend/;
        }
    }
}
```

**Prometheus é…ç½®** (`prometheus/prometheus.yml`):

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'ollama'
    static_configs:
      - targets: ['ollama:11434']
    metrics_path: '/metrics'
```

#### Python å®¢æˆ·ç«¯è¿æ¥ Docker Ollama

```python
import requests
import json
from typing import Optional, Iterator

class DockerOllamaClient:
    """Docker Ollama å®¢æˆ·ç«¯"""

    def __init__(
        self,
        base_url: str = "http://localhost:11434",
        model: str = "llama3"
    ):
        self.base_url = base_url.rstrip('/')
        self.model = model

    def chat(self, prompt: str, stream: bool = False) -> str:
        """èŠå¤©å¯¹è¯"""
        response = requests.post(
            f"{self.base_url}/api/chat",
            json={
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "stream": stream
            }
        )

        if stream:
            return self._stream_response(response)
        else:
            return response.json()['message']['content']

    def generate(self, prompt: str, stream: bool = False) -> str:
        """æ–‡æœ¬ç”Ÿæˆ"""
        response = requests.post(
            f"{self.base_url}/api/generate",
            json={
                "model": self.model,
                "prompt": prompt,
                "stream": stream
            }
        )

        if stream:
            return self._stream_response(response)
        else:
            return response.json()['response']

    def _stream_response(self, response) -> Iterator[str]:
        """å¤„ç†æµå¼å“åº”"""
        for line in response.iter_lines():
            if line:
                data = json.loads(line)
                if 'response' in data:
                    yield data['response']
                elif 'message' in data:
                    yield data['message']['content']

    def list_models(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰æ¨¡å‹"""
        response = requests.get(f"{self.base_url}/api/tags")
        return response.json()['models']

    def pull_model(self, model: str) -> dict:
        """æ‹‰å–æ¨¡å‹"""
        response = requests.post(
            f"{self.base_url}/api/pull",
            json={"name": model},
            stream=True
        )

        for line in response.iter_lines():
            if line:
                data = json.loads(line)
                print(f"Downloading: {data.get('completed', 0)}/{data.get('total', 0)}")

        return {"status": "success"}

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = DockerOllamaClient(
        base_url="http://localhost:11434",
        model="llama3"
    )

    # åˆ—å‡ºæ¨¡å‹
    print("å¯ç”¨æ¨¡å‹ï¼š", client.list_models())

    # èŠå¤©
    response = client.chat("è§£é‡Šä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ ")
    print("å›å¤ï¼š", response)

    # æµå¼ç”Ÿæˆ
    for chunk in client.generate("å†™ä¸€é¦–å…³äºAIçš„è¯—", stream=True):
        print(chunk, end="", flush=True)
```

#### LangChain é›†æˆ Docker Ollama

```python
from langchain_community.llms import Ollama
from langchain_community.embeddings import OllamaEmbeddings
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

# åˆå§‹åŒ– LLM
llm = Ollama(
    base_url="http://localhost:11434",  # Docker Ollama åœ°å€
    model="llama3",
    temperature=0.7
)

# åˆå§‹åŒ– Embeddings
embeddings = OllamaEmbeddings(
    base_url="http://localhost:11434",
    model="llama3"
)

# åˆ›å»ºå¯¹è¯é“¾
memory = ConversationBufferMemory()
conversation = ConversationChain(
    llm=llm,
    memory=memory,
    verbose=True
)

# ä½¿ç”¨
response = conversation.predict(input="ä½ å¥½ï¼Œæˆ‘æ˜¯å°æ˜")
print(response)

response = conversation.predict(input="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
print(response)  # åº”è¯¥è®°å¾—"å°æ˜"
```

#### Moltbot é›†æˆ Docker Ollama

```python
from moltbot import Agent
from moltbot.llm import OllamaLLM

# ä½¿ç”¨ Docker Ollama
llm = OllamaLLM(
    base_url="http://localhost:11434",
    model="llama3"
)

# åˆ›å»º Agent
agent = Agent(
    name="æœ¬åœ°åŠ©æ‰‹",
    llm=llm,
    instructions="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹"
)

# å¯¹è¯
response = agent.chat("ä½ å¥½")
print(response)
```

#### æ€§èƒ½ä¼˜åŒ–å»ºè®®

**1. æ¨¡å‹é‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰**ï¼š

```bash
# æ‹‰å–é‡åŒ–ç‰ˆæœ¬ï¼ˆ4-bitï¼‰
ollama pull llama3:8b-q4_0  # 4-bit é‡åŒ–
ollama pull llama3:8b-q8_0  # 8-bit é‡åŒ–

# å¯¹æ¯”æ˜¾å­˜å ç”¨
# llama3:8b         - çº¦ 6GB
# llama3:8b-q4_0    - çº¦ 4GB
# llama3:8b-q8_0    - çº¦ 5GB
```

**2. å¹¶å‘å¤„ç†**ï¼š

```yaml
# docker-compose.scale.yml
services:
  ollama:
    image: ollama/ollama:latest
    deploy:
      replicas: 3  # å¯åŠ¨ 3 ä¸ªå®ä¾‹
    # ... å…¶ä»–é…ç½®
```

**3. ç¼“å­˜é…ç½®**ï¼š

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡
OLLAMA_NUM_PARALLEL=4  # å¹¶å‘è¯·æ±‚æ•°
OLLAMA_MAX_QUEUE=100   # æœ€å¤§é˜Ÿåˆ—é•¿åº¦
OLLAMA_LOAD_TIMEOUT=5m  # æ¨¡å‹åŠ è½½è¶…æ—¶
```

**4. ç›‘æ§å’Œæ—¥å¿—**ï¼š

```bash
# æŸ¥çœ‹ Ollama ç»Ÿè®¡ä¿¡æ¯
curl http://localhost:11434/api/tags

# æŸ¥çœ‹è¿è¡Œä¸­çš„æ¨¡å‹
docker exec ollama ollama ps

# æŸ¥çœ‹æ—¥å¿—
docker logs -f ollama
```

### RAGFlowï¼šä¼ä¸šçº§ RAG å¹³å°

**RAGFlow** æ˜¯ä¸€ä¸ªåŸºäºæ·±åº¦æ–‡æ¡£ç†è§£çš„å¼€æº RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰å¼•æ“ï¼Œç”± infiniflow/ragflow å¼€å‘ã€‚

#### æ ¸å¿ƒç‰¹æ€§

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          RAGFlow æ ¸å¿ƒç‰¹æ€§                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  ğŸ“š æ™ºèƒ½æ–‡æ¡£è§£æ                              â”‚
â”‚  - æ”¯æŒå¤æ‚ PDF è¡¨æ ¼è§£æ                     â”‚
â”‚  - å¤šæ¨¡æ€æ–‡æ¡£ç†è§£                            â”‚
â”‚  - OCR æ–‡å­—è¯†åˆ«                              â”‚
â”‚  - è‡ªåŠ¨æ–‡æ¡£åˆ†å—                              â”‚
â”‚                                             â”‚
â”‚  ğŸ¯ é«˜è´¨é‡æ£€ç´¢                                â”‚
â”‚  - æ··åˆæ£€ç´¢ï¼ˆå‘é‡+å…³é”®è¯ï¼‰                   â”‚
â”‚  - é‡æ’åºä¼˜åŒ–                                â”‚
â”‚  - ä¸Šä¸‹æ–‡æ™ºèƒ½å¬å›                            â”‚
â”‚  - å¤šè·¯å¬å›èåˆ                              â”‚
â”‚                                             â”‚
â”‚  ğŸ¤– å¤šæ¨¡å‹æ”¯æŒ                                â”‚
â”‚  - OpenAI GPTç³»åˆ—                            â”‚
â”‚  - Claude ç³»åˆ—                               â”‚
â”‚  - æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰                        â”‚
â”‚  - å›½äº§æ¨¡å‹ï¼ˆé€šä¹‰åƒé—®ã€DeepSeekï¼‰             â”‚
â”‚                                             â”‚
â”‚  ğŸ”„ å·¥ä½œæµç¼–æ’                                â”‚
â”‚  - å¯è§†åŒ–æµç¨‹è®¾è®¡                            â”‚
â”‚  - è‡ªå®šä¹‰å¤„ç†èŠ‚ç‚¹                            â”‚
â”‚  - API é›†æˆ                                  â”‚
â”‚                                             â”‚
â”‚  ğŸ“Š ä¼ä¸šçº§ç‰¹æ€§                                â”‚
â”‚  - å¤šç§Ÿæˆ·æ”¯æŒ                                â”‚
â”‚  - æƒé™ç®¡ç†                                  â”‚
â”‚  - å®¡è®¡æ—¥å¿—                                  â”‚
â”‚  - API é™æµ                                  â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### RAGFlow vs å…¶ä»– RAG æ¡†æ¶

| ç‰¹æ€§ | RAGFlow | LangChain | LlamaIndex | Dify |
|------|---------|-----------|------------|------|
| **æ–‡æ¡£è§£æ** | â­â­â­â­â­ | â­â­â­ | â­â­â­ | â­â­â­â­ |
| **éƒ¨ç½²éš¾åº¦** | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ |
| **å¯è§†åŒ–** | â­â­â­â­â­ | â­â­ | â­â­ | â­â­â­â­â­ |
| **ä¸­æ–‡æ”¯æŒ** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ |
| **æœ¬åœ°æ¨¡å‹** | âœ… | âœ… | âœ… | âœ… |
| **å¼€æºå…è´¹** | âœ… | âœ… | âœ… | âœ… |
| **å­¦ä¹ æ›²çº¿** | ä¸­ç­‰ | è¾ƒé™¡ | è¾ƒé™¡ | å¹³ç¼“ |

**RAGFlow æœ€é€‚åˆ**ï¼š
- ğŸ¯ éœ€è¦å¤„ç†å¤§é‡å¤æ‚æ–‡æ¡£ï¼ˆPDFã€è¡¨æ ¼ç­‰ï¼‰
- ğŸ¯ éœ€è¦é«˜è´¨é‡æ£€ç´¢å’Œé—®ç­”
- ğŸ¯ ä¼ä¸šçº§çŸ¥è¯†åº“ç³»ç»Ÿ
- ğŸ¯ éœ€è¦å¯è§†åŒ–é…ç½®ç•Œé¢

#### Docker å¿«é€Ÿéƒ¨ç½²

**1. åŸºç¡€éƒ¨ç½²**ï¼š

```bash
# å…‹éš†ä»“åº“
git clone https://github.com/infiniflow/ragflow.git
cd ragflow/docker

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# è®¿é—® Web ç•Œé¢
# æµè§ˆå™¨æ‰“å¼€ï¼šhttp://localhost:80
# é»˜è®¤è´¦å·ï¼šadmin / admin
```

**2. å®Œæ•´éƒ¨ç½²é…ç½®**ï¼š

```yaml
# docker-compose.yml
version: '3.8'

services:
  # MySQL æ•°æ®åº“
  mysql:
    image: mysql:8.0
    container_name: ragflow-mysql
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: root
      MYSQL_DATABASE: ragflow
      MYSQL_USER: ragflow
      MYSQL_PASSWORD: ragflow
    volumes:
      - mysql_data:/var/lib/mysql
    networks:
      - ragflow_network
    command: --default-authentication-plugin=mysql_native_password

  # Redis ç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: ragflow-redis
    restart: unless-stopped
    volumes:
      - redis_data:/data
    networks:
      - ragflow_network

  # Elasticsearchï¼ˆå¯é€‰ï¼Œç”¨äºå…¨æ–‡æ£€ç´¢ï¼‰
  elasticsearch:
    image: elasticsearch:8.11.0
    container_name: ragflow-es
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms2g -Xmx2g
    volumes:
      - es_data:/usr/share/elasticsearch/data
    networks:
      - ragflow_network

  # MinIOï¼ˆå¯¹è±¡å­˜å‚¨ï¼‰
  minio:
    image: minio/minio:latest
    container_name: ragflow-minio
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    volumes:
      - minio_data:/data
    networks:
      - ragflow_network
    command: server /data --console-address ":9001"

  # RAGFlow ä¸»æœåŠ¡
  ragflow:
    image: infiniflow/ragflow:latest
    container_name: ragflow-server
    restart: unless-stopped
    depends_on:
      - mysql
      - redis
      - minio
    ports:
      - "80:80"
      - "443:443"
    environment:
      - MYSQL_HOST=mysql
      - MYSQL_PORT=3306
      - MYSQL_USER=ragflow
      - MYSQL_PASSWORD=ragflow
      - MYSQL_DATABASE=ragflow
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minioadmin
      - MINIO_SECRET_KEY=minioadmin
      - ES_ENDPOINT=http://elasticsearch:9200
    volumes:
      - ragflow_data:/ragflow/data
    networks:
      - ragflow_network

volumes:
  mysql_data:
    driver: local
  redis_data:
    driver: local
  es_data:
    driver: local
  minio_data:
    driver: local
  ragflow_data:
    driver: local

networks:
  ragflow_network:
    driver: bridge
```

**3. ä½¿ç”¨ Ollama æœ¬åœ°æ¨¡å‹**ï¼š

```yaml
# ä¿®æ”¹ ragflow æœåŠ¡çš„ç¯å¢ƒå˜é‡
  ragflow:
    image: infiniflow/ragflow:latest
    environment:
      # ... å…¶ä»–é…ç½®
      - LLM_TYPE=ollama  # ä½¿ç”¨ Ollama
      - OLLAMA_BASE_URL=http://ollama:11434
      - EMBEDDING_MODEL=ollama:llama3
      - LLM_MODEL=ollama:llama3
    depends_on:
      - ollama  # æ·»åŠ  Ollama æœåŠ¡ä¾èµ–

  # æ·»åŠ  Ollama æœåŠ¡
  ollama:
    image: ollama/ollama:latest
    container_name: ragflow-ollama
    restart: unless-stopped
    volumes:
      - ollama_data:/root/.ollama
    networks:
      - ragflow_network

volumes:
  ollama_data:
    driver: local
```

#### ä½¿ç”¨æŒ‡å—

**1. åˆ›å»ºçŸ¥è¯†åº“**ï¼š

```bash
# è®¿é—® Web ç•Œé¢ï¼šhttp://localhost:80
# ç™»å½•ï¼šadmin / admin

# æ­¥éª¤ï¼š
# 1. ç‚¹å‡»"çŸ¥è¯†åº“" â†’ "åˆ›å»ºçŸ¥è¯†åº“"
# 2. ä¸Šä¼ æ–‡æ¡£ï¼ˆPDFã€Wordã€TXT ç­‰ï¼‰
# 3. ç­‰å¾…æ–‡æ¡£è§£æå’Œå‘é‡åŒ–
# 4. æµ‹è¯•æ£€ç´¢æ•ˆæœ
```

**2. API ä½¿ç”¨**ï¼š

```python
import requests
import json

RAGFLOW_API_URL = "http://localhost:80/api/v1"

class RAGFlowClient:
    """RAGFlow å®¢æˆ·ç«¯"""

    def __init__(self, base_url: str = RAGFLOW_API_URL):
        self.base_url = base_url
        self.token = None

    def login(self, username: str, password: str):
        """ç™»å½•"""
        response = requests.post(
            f"{self.base_url}/login",
            json={"username": username, "password": password}
        )
        self.token = response.json()['token']
        return self.token

    def create_dataset(self, name: str, description: str = ""):
        """åˆ›å»ºæ•°æ®é›†"""
        headers = {"Authorization": f"Bearer {self.token}"}
        response = requests.post(
            f"{self.base_url}/datasets",
            headers=headers,
            json={"name": name, "description": description}
        )
        return response.json()

    def upload_document(self, dataset_id: str, file_path: str):
        """ä¸Šä¼ æ–‡æ¡£"""
        headers = {"Authorization": f"Bearer {self.token}"}
        with open(file_path, 'rb') as f:
            files = {'file': f}
            response = requests.post(
                f"{self.base_url}/datasets/{dataset_id}/documents",
                headers=headers,
                files=files
            )
        return response.json()

    def search(self, dataset_id: str, query: str, top_k: int = 5):
        """æœç´¢æ–‡æ¡£"""
        headers = {"Authorization": f"Bearer {self.token}"}
        response = requests.post(
            f"{self.base_url}/datasets/{dataset_id}/search",
            headers=headers,
            json={"query": query, "top_k": top_k}
        )
        return response.json()

    def chat(self, dataset_id: str, question: str):
        """åŸºäºçŸ¥è¯†åº“é—®ç­”"""
        headers = {"Authorization": f"Bearer {self.token}"}
        response = requests.post(
            f"{self.base_url}/datasets/{dataset_id}/chat",
            headers=headers,
            json={"question": question}
        )
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # åˆå§‹åŒ–å®¢æˆ·ç«¯
    client = RAGFlowClient()

    # ç™»å½•
    client.login("admin", "admin")

    # åˆ›å»ºæ•°æ®é›†
    dataset = client.create_dataset(
        name="äº§å“æ–‡æ¡£",
        description="å…¬å¸äº§å“ä½¿ç”¨æ‰‹å†Œ"
    )
    dataset_id = dataset['id']

    # ä¸Šä¼ æ–‡æ¡£
    client.upload_document(dataset_id, "manual.pdf")

    # æœç´¢
    results = client.search(dataset_id, "å¦‚ä½•å®‰è£…ï¼Ÿ")
    print("æœç´¢ç»“æœï¼š", results)

    # é—®ç­”
    answer = client.chat(dataset_id, "äº§å“æ”¯æŒå“ªäº›æ“ä½œç³»ç»Ÿï¼Ÿ")
    print("å›ç­”ï¼š", answer['answer'])
```

**3. é›†æˆåˆ° FastAPI**ï¼š

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import requests

app = FastAPI()

class ChatRequest(BaseModel):
    question: str

class ChatResponse(BaseModel):
    answer: str
    sources: list

@app.post("/api/chat", response_model=ChatResponse)
async def chat_with_ragflow(request: ChatRequest):
    """ä½¿ç”¨ RAGFlow è¿›è¡Œé—®ç­”"""

    # è°ƒç”¨ RAGFlow API
    ragflow_response = requests.post(
        "http://ragflow:80/api/v1/datasets/1/chat",
        json={"question": request.question},
        headers={"Authorization": "Bearer YOUR_TOKEN"}
    )

    if ragflow_response.status_code != 200:
        raise HTTPException(status_code=500, detail="RAGFlow API error")

    data = ragflow_response.json()

    return ChatResponse(
        answer=data['answer'],
        sources=data.get('sources', [])
    )
```

#### é«˜çº§é…ç½®

**1. è‡ªå®šä¹‰è§£æå™¨**ï¼š

```python
# RAGFlow æ”¯æŒè‡ªå®šä¹‰æ–‡æ¡£è§£æå™¨
# é€šè¿‡é…ç½®æ–‡ä»¶æŒ‡å®šè§£æè§„åˆ™

{
  "parsers": {
    "pdf": {
      "extract_tables": true,
      "ocr_enabled": true,
      "layout_analysis": true
    },
    "docx": {
      "extract_images": true,
      "preserve_format": true
    }
  }
}
```

**2. æ£€ç´¢ä¼˜åŒ–**ï¼š

```yaml
# é«˜çº§æ£€ç´¢é…ç½®
retrieval:
  method: "hybrid"  # æ··åˆæ£€ç´¢ï¼šå‘é‡ + å…³é”®è¯
  vector_similarity_weight: 0.7  # å‘é‡ç›¸ä¼¼åº¦æƒé‡
  keyword_weight: 0.3  # å…³é”®è¯æƒé‡
  rerank_enabled: true  # å¯ç”¨é‡æ’åº
  top_k: 20  # åˆæ­¥å¬å›æ•°é‡
  final_top_k: 5  # æœ€ç»ˆè¿”å›æ•°é‡
```

**3. æ€§èƒ½ä¼˜åŒ–**ï¼š

```bash
# å¢åŠ å¹¶å‘å¤„ç†
docker-compose up -d --scale ragflow=3

# è°ƒæ•´ Elasticsearch å†…å­˜
ES_JAVA_OPTS=-Xms4g -Xmx4g

# å¯ç”¨ç¼“å­˜
REDIS_CACHE_TTL=3600
```

#### å®æˆ˜æ¡ˆä¾‹

**ä¼ä¸šçŸ¥è¯†åº“ç³»ç»Ÿæ¶æ„**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ä¼ä¸šçŸ¥è¯†åº“ç³»ç»Ÿæ¶æ„                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   ç”¨æˆ·     â”‚ â†â†’  â”‚  Web UI   â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜              â”‚
â”‚                           â†“                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         RAGFlow æ ¸å¿ƒæœåŠ¡                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚   â”‚
â”‚  â”‚  â”‚ æ–‡æ¡£è§£æ   â”‚  â”‚  æ£€ç´¢å¼•æ“  â”‚           â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜           â”‚   â”‚
â”‚  â”‚                       â†“                  â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚   â”‚
â”‚  â”‚  â”‚      LLM (GPT-4/Ollama)          â”‚    â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚           â†“          â†“          â†“              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚  MySQL   â”‚  â”‚  Redis  â”‚  â”‚ MinIO    â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Moltbotæ¡†æ¶ {#moltbotæ¡†æ¶}

### ä»€ä¹ˆæ˜¯Moltbotï¼Ÿ

**Moltbot**ï¼ˆåŸåClawdBotï¼‰æ˜¯ä¸€ä¸ªè½»é‡çº§ã€æ˜“ç”¨çš„AI Agentå¼€å‘æ¡†æ¶ï¼Œä¸“é—¨ä¸ºå¿«é€Ÿæ„å»ºæ™ºèƒ½åŠ©æ‰‹è€Œè®¾è®¡ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Moltbot æ ¸å¿ƒç‰¹ç‚¹                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                             â”‚
â”‚  âœ¨ ç®€å•æ˜“ç”¨                                â”‚
â”‚  - å‡ è¡Œä»£ç å³å¯åˆ›å»ºAgent                    â”‚
â”‚  - Pythonic APIè®¾è®¡                         â”‚
â”‚  - ä¸°å¯Œçš„æ–‡æ¡£å’Œç¤ºä¾‹                         â”‚
â”‚                                             â”‚
â”‚  ğŸš€ é«˜æ€§èƒ½                                  â”‚
â”‚  - å¼‚æ­¥æ‰§è¡Œæ”¯æŒ                             â”‚
â”‚  - å†…ç½®è¿æ¥æ±                                â”‚
â”‚  - æ™ºèƒ½ç¼“å­˜æœºåˆ¶                             â”‚
â”‚                                             â”‚
â”‚  ğŸ”§ çµæ´»æ‰©å±•                                â”‚
â”‚  - æ’ä»¶åŒ–æ¶æ„                               â”‚
â”‚  - è‡ªå®šä¹‰å·¥å…·                               â”‚
â”‚  - å¤šæ¨¡å‹æ”¯æŒ                               â”‚
â”‚                                             â”‚
â”‚  ğŸ’° æˆæœ¬å‹å¥½                                â”‚
â”‚  - å¼€æºå…è´¹                                 â”‚
â”‚  - æœ¬åœ°è¿è¡Œä¼˜å…ˆ                             â”‚
â”‚  - æ™ºèƒ½é™çº§ç­–ç•¥                             â”‚
â”‚                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä¸ºä»€ä¹ˆé€‰æ‹©Moltbotï¼Ÿ

| ç‰¹æ€§ | Moltbot | LangChain | AutoGen |
|------|---------|-----------|---------|
| **å­¦ä¹ æ›²çº¿** | â­â­â­â­â­ æœ€ç®€å• | â­â­â­ ä¸­ç­‰ | â­â­ è¾ƒé™¡ |
| **ä»£ç é‡** | æœ€å°‘ | ä¸­ç­‰ | è¾ƒå¤š |
| **çµæ´»æ€§** | â­â­â­â­ é«˜ | â­â­â­â­â­ æé«˜ | â­â­â­â­â­ æé«˜ |
| **æ€§èƒ½** | â­â­â­â­â­ ä¼˜ç§€ | â­â­â­â­ è‰¯å¥½ | â­â­â­ ä¸€èˆ¬ |
| **æ–‡æ¡£** | â­â­â­â­ å®Œå–„ | â­â­â­â­â­ æå®Œå–„ | â­â­â­ è¾ƒå°‘ |
| **é€‚åˆåœºæ™¯** | å¿«é€Ÿå¼€å‘ | å¤æ‚åº”ç”¨ | ç ”ç©¶ |

**Moltbotæœ€é€‚åˆ**ï¼š
- ğŸ¯ å¿«é€ŸåŸå‹å¼€å‘
- ğŸ¯ ä¸­å°å‹é¡¹ç›®
- ğŸ¯ å­¦ä¹ Agentå¼€å‘
- ğŸ¯ å›¢é˜Ÿåä½œå·¥å…·

### å®‰è£…å’Œé…ç½®

```bash
# å®‰è£…Moltbot
pip install moltbot

# æˆ–ä½¿ç”¨å›½å†…é•œåƒåŠ é€Ÿ
pip install moltbot -i https://pypi.tuna.tsinghua.edu.cn/simple
```

```python
# config.py
import os
from dotenv import load_dotenv

load_dotenv()

class MoltbotConfig:
    """Moltboté…ç½®"""

    # LLMé…ç½®
    LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai")  # openai, claude, ollama
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    ANTHROPIC_API_KEY = os.getenv("ANTHROPIC_API_KEY")

    # æ¨¡å‹é€‰æ‹©
    MODEL_NAME = os.getenv("MODEL_NAME", "gpt-3.5-turbo")

    # Agenté…ç½®
    MAX_TOOLS_CALLS = int(os.getenv("MAX_TOOLS_CALLS", "5"))
    TIMEOUT = int(os.getenv("TIMEOUT", "30"))

    # ç¼“å­˜é…ç½®
    ENABLE_CACHE = os.getenv("ENABLE_CACHE", "true").lower() == "true"
    CACHE_TTL = int(os.getenv("CACHE_TTL", "3600"))
```

### å¿«é€Ÿå¼€å§‹

#### åˆ›å»ºç¬¬ä¸€ä¸ªMoltbot Agent

```python
from moltbot import Agent, Tool
from moltbot.llm import OpenAILLM

# 1. åˆå§‹åŒ–LLM
llm = OpenAILLM(
    api_key="your-api-key",
    model="gpt-3.5-turbo",
    temperature=0.7
)

# 2. åˆ›å»ºAgent
agent = Agent(
    name="å°åŠ©æ‰‹",
    llm=llm,
    instructions="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œæ“…é•¿å›ç­”é—®é¢˜"
)

# 3. å¯¹è¯
response = agent.chat("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
print(response)
# è¾“å‡ºï¼šä½ å¥½ï¼æˆ‘æ˜¯å°åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºä½ æœåŠ¡...
```

#### æ·»åŠ å·¥å…·

```python
from moltbot import Tool

# 1. å®šä¹‰å·¥å…·å‡½æ•°
def get_weather(city: str) -> str:
    """æŸ¥è¯¢å¤©æ°”"""
    # å®é™…è°ƒç”¨å¤©æ°”API
    return f"{city}ä»Šå¤©æ™´å¤©ï¼Œæ¸©åº¦25Â°C"

def calculate(expression: str) -> float:
    """è®¡ç®—æ•°å­¦è¡¨è¾¾å¼"""
    try:
        return eval(expression)
    except:
        return "è®¡ç®—é”™è¯¯"

# 2. åˆ›å»ºå·¥å…·
weather_tool = Tool(
    name="get_weather",
    description="æŸ¥è¯¢æŒ‡å®šåŸå¸‚çš„å¤©æ°”",
    function=get_weather
)

calculator_tool = Tool(
    name="calculator",
    description="è®¡ç®—æ•°å­¦è¡¨è¾¾å¼",
    function=calculate
)

# 3. æ·»åŠ å·¥å…·åˆ°Agent
agent.add_tools([weather_tool, calculator_tool])

# 4. ä½¿ç”¨
response = agent.chat("åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ")
# Agentä¼šè‡ªåŠ¨è°ƒç”¨get_weatherå·¥å…·

response = agent.chat("è®¡ç®—123 * 456")
# Agentä¼šè‡ªåŠ¨è°ƒç”¨calculatorå·¥å…·
```

### é«˜çº§åŠŸèƒ½

#### 1. è®°å¿†ç®¡ç†

```python
from moltbot.memory import ConversationMemory

# åˆ›å»ºå¸¦è®°å¿†çš„Agent
memory = ConversationMemory(
    max_history=100,  # æœ€å¤šä¿å­˜100è½®å¯¹è¯
    persist=True,      # æŒä¹…åŒ–åˆ°ç£ç›˜
    storage_path="./memory"
)

agent = Agent(
    name="è®°å¿†åŠ©æ‰‹",
    llm=llm,
    memory=memory
)

# Agentä¼šè®°ä½ä¹‹å‰çš„å¯¹è¯
agent.chat("æˆ‘å«å°æ˜")
agent.chat("æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
# è¾“å‡ºï¼šä½ å«å°æ˜

# æŸ¥çœ‹è®°å¿†å†å²
history = memory.get_history()
print(history)
```

#### 2. å¼‚æ­¥æ‰§è¡Œ

```python
import asyncio
from moltbot import AsyncAgent

async def main():
    # åˆ›å»ºå¼‚æ­¥Agent
    agent = AsyncAgent(
        name="å¼‚æ­¥åŠ©æ‰‹",
        llm=llm
    )

    # å¹¶å‘å¤„ç†å¤šä¸ªä»»åŠ¡
    tasks = [
        agent.chat("é—®é¢˜1"),
        agent.chat("é—®é¢˜2"),
        agent.chat("é—®é¢˜3")
    ]

    results = await asyncio.gather(*tasks)
    for result in results:
        print(result)

asyncio.run(main())
```

#### 3. å¤šAgentåä½œ

```python
from moltbot import Agent, Team

# åˆ›å»ºä¸“ä¸šAgent
researcher = Agent(
    name="ç ”ç©¶å‘˜",
    llm=llm,
    instructions="ä½ æ“…é•¿æœé›†å’Œåˆ†æä¿¡æ¯"
)

writer = Agent(
    name="å†™æ‰‹",
    llm=llm,
    instructions="ä½ æ“…é•¿æ’°å†™æ–‡ç« "
)

reviewer = Agent(
    name="å®¡æ ¸å‘˜",
    llm=llm,
    instructions="ä½ æ“…é•¿å®¡æ ¸å†…å®¹è´¨é‡"
)

# åˆ›å»ºå›¢é˜Ÿ
team = Team(
    name="å†…å®¹åˆ›ä½œå›¢é˜Ÿ",
    members=[researcher, writer, reviewer],
    workflow="researcher â†’ writer â†’ reviewer"  # å·¥ä½œæµ
)

# æ‰§è¡Œä»»åŠ¡
result = team.execute("ä¸»é¢˜ï¼šäººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿")
# è¾“å‡ºï¼šç ”ç©¶å‘˜æœé›†ä¿¡æ¯ â†’ å†™æ‰‹æ’°å†™æ–‡ç«  â†’ å®¡æ ¸å‘˜è´¨é‡æ£€æŸ¥
```

#### 4. è‡ªå®šä¹‰æ’ä»¶

```python
from moltbot import Plugin

class DatabasePlugin(Plugin):
    """æ•°æ®åº“æ’ä»¶"""

    def __init__(self, connection_string: str):
        super().__init__(name="database")
        self.db = self.connect(connection_string)

    def query(self, sql: str):
        """æ‰§è¡ŒSQLæŸ¥è¯¢"""
        cursor = self.db.cursor()
        cursor.execute(sql)
        return cursor.fetchall()

    def to_tools(self):
        """è½¬æ¢ä¸ºå·¥å…·"""
        return [
            Tool(
                name="db_query",
                description="æŸ¥è¯¢æ•°æ®åº“",
                function=self.query
            )
        ]

# ä½¿ç”¨æ’ä»¶
db_plugin = DatabasePlugin("sqlite:///mydb.db")
agent.add_tools(db_plugin.to_tools())

response = agent.chat("æŸ¥è¯¢ç”¨æˆ·è¡¨æœ‰å¤šå°‘æ¡è®°å½•")
```

### å®æˆ˜æ¡ˆä¾‹

#### æ¡ˆä¾‹1ï¼šæ™ºèƒ½å®¢æœç³»ç»Ÿ

```python
from moltbot import Agent, Tool
from moltbot.knowledge import KnowledgeBase

# 1. åˆ›å»ºçŸ¥è¯†åº“
kb = KnowledgeBase()
kb.add_documents([
    {"content": "é€€æ¬¾æµç¨‹ï¼šè¿›å…¥è®¢å•è¯¦æƒ… â†’ ç”³è¯·é€€æ¬¾ â†’ ç­‰å¾…å®¡æ ¸"},
    {"content": "é…é€æ—¶é—´ï¼šä¸€èˆ¬2-3ä¸ªå·¥ä½œæ—¥"},
    {"content": "è”ç³»æ–¹å¼ï¼šå®¢æœç”µè¯400-123-4567"}
])

# 2. åˆ›å»ºå®¢æœAgent
customer_service = Agent(
    name="æ™ºèƒ½å®¢æœ",
    llm=llm,
    instructions="""
    ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¢æœäººå‘˜ã€‚
    - ä½¿ç”¨ç¤¼è²Œã€å‹å¥½çš„è¯­è¨€
    - ä¼˜å…ˆä»çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ç­”æ¡ˆ
    - æ— æ³•è§£å†³æ—¶å¼•å¯¼ç”¨æˆ·è”ç³»äººå·¥å®¢æœ
    """,
    knowledge_base=kb
)

# 3. æ·»åŠ è®¢å•æŸ¥è¯¢å·¥å…·
def check_order(order_id: str) -> dict:
    """æŸ¥è¯¢è®¢å•çŠ¶æ€"""
    # å®é™…æŸ¥è¯¢æ•°æ®åº“
    return {
        "order_id": order_id,
        "status": "å·²å‘è´§",
        "estimated_arrival": "2024-03-15"
    }

customer_service.add_tool(Tool(
    name="check_order",
    description="æŸ¥è¯¢è®¢å•çŠ¶æ€",
    function=check_order
))

# 4. æœåŠ¡
print(customer_service.chat("æˆ‘çš„è®¢å•12345ä»€ä¹ˆæ—¶å€™åˆ°ï¼Ÿ"))
# è¾“å‡ºï¼šæ‚¨çš„è®¢å•12345å·²å‘è´§ï¼Œé¢„è®¡3æœˆ15æ—¥é€è¾¾

print(customer_service.chat("å¦‚ä½•ç”³è¯·é€€æ¬¾ï¼Ÿ"))
# è¾“å‡ºï¼šé€€æ¬¾æµç¨‹å¾ˆç®€å•ï¼šè¿›å…¥è®¢å•è¯¦æƒ… â†’ ç”³è¯·é€€æ¬¾ â†’ ç­‰å¾…å®¡æ ¸
```

#### æ¡ˆä¾‹2ï¼šä»£ç åŠ©æ‰‹

```python
from moltbot import Agent
import subprocess

class CodeAssistant(Agent):
    """ä»£ç åŠ©æ‰‹Agent"""

    def __init__(self, llm):
        super().__init__(
            name="ä»£ç åŠ©æ‰‹",
            llm=llm,
            instructions="ä½ æ˜¯ä¸“ä¸šçš„ç¼–ç¨‹åŠ©æ‰‹ï¼Œæ“…é•¿ä»£ç ç”Ÿæˆå’Œè°ƒè¯•"
        )

        # æ·»åŠ ä»£ç æ‰§è¡Œå·¥å…·
        self.add_tool(Tool(
            name="execute_python",
            description="æ‰§è¡ŒPythonä»£ç ",
            function=self._execute_python
        ))

        # æ·»åŠ ä»£ç æœç´¢å·¥å…·
        self.add_tool(Tool(
            name="search_code",
            description="æœç´¢ä»£ç ç¤ºä¾‹",
            function=self._search_code
        ))

    def _execute_python(self, code: str) -> str:
        """æ‰§è¡ŒPythonä»£ç """
        try:
            result = subprocess.run(
                ["python", "-c", code],
                capture_output=True,
                text=True,
                timeout=10
            )
            return result.stdout if result.returncode == 0 else result.stderr
        except Exception as e:
            return str(e)

    def _search_code(self, query: str) -> str:
        """æœç´¢ä»£ç ç¤ºä¾‹"""
        # å®é™…å®ç°å¯ä»¥è°ƒç”¨GitHub APIæˆ–æœ¬åœ°ä»£ç åº“
        return f"æ‰¾åˆ°ç›¸å…³ä»£ç ï¼š{query}"

# ä½¿ç”¨
assistant = CodeAssistant(llm)
print(assistant.chat("å†™ä¸€ä¸ªPythonå¿«æ’å¹¶æ‰§è¡Œæµ‹è¯•"))
```

### Moltbot vs LangChain

**ä½•æ—¶é€‰æ‹©Moltbot**ï¼š
```python
# âœ… é€‰æ‹©Moltbotçš„åœºæ™¯ï¼š
- å¿«é€ŸåŸå‹ï¼ˆå°æ—¶çº§ vs å¤©çº§ï¼‰
- ç®€å•åˆ°ä¸­ç­‰å¤æ‚åº¦çš„Agent
- å›¢é˜ŸæŠ€æœ¯æ ˆè¾ƒæ–°
- éœ€è¦å¿«é€Ÿè¿­ä»£

# âœ… é€‰æ‹©LangChainçš„åœºæ™¯ï¼š
- å¤æ‚çš„å·¥ä½œæµ
- éœ€è¦æ·±åº¦å®šåˆ¶
- å·²æœ‰LangChainç”Ÿæ€
- éœ€è¦ç¤¾åŒºæ”¯æŒ
```

**ä»£ç å¯¹æ¯”**ï¼š

```python
# Moltbotï¼šç®€æ´
agent = Agent(name="åŠ©æ‰‹", llm=llm)
agent.add_tool(tool)
response = agent.chat("é—®é¢˜")

# LangChainï¼šè¯¦ç»†
from langchain.agents import initialize_agent, AgentType
from langchain.tools import Tool

tools = [tool]
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.OPENAI_FUNCTIONS,
    verbose=True
)
response = agent.run("é—®é¢˜")
```

### æœ€ä½³å®è·µ

```python
# 1. æ¸…æ™°çš„æŒ‡ä»¤
agent = Agent(
    name="æ•°æ®åˆ†æåŠ©æ‰‹",
    instructions="""
    è§’è‰²å®šä½ï¼šä½ æ˜¯ä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆ
    å·¥ä½œæµç¨‹ï¼š
      1. ç†è§£ç”¨æˆ·éœ€æ±‚
      2. é€‰æ‹©åˆé€‚çš„åˆ†æå·¥å…·
      3. æ‰§è¡Œåˆ†æå¹¶ç”ŸæˆæŠ¥å‘Š
    è¾“å‡ºæ ¼å¼ï¼šä½¿ç”¨Markdownæ ¼å¼
    æ³¨æ„äº‹é¡¹ï¼šæ•°æ®éšç§ç¬¬ä¸€
    """
)

# 2. åˆç†çš„å·¥å…·å®šä¹‰
tool = Tool(
    name="calculate",
    description="æ‰§è¡Œæ•°å­¦è®¡ç®—ï¼Œè¾“å…¥æ ¼å¼ï¼šæ•°å­¦è¡¨è¾¾å¼å­—ç¬¦ä¸²",
    function=calculate
)

# 3. é”™è¯¯å¤„ç†
try:
    response = agent.chat("å¤æ‚é—®é¢˜", timeout=30)
except TimeoutError:
    # è¶…æ—¶å¤„ç†
    response = "æŠ±æ­‰ï¼Œå¤„ç†è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•"
except Exception as e:
    # å…¶ä»–é”™è¯¯
    response = f"å‘ç”Ÿé”™è¯¯ï¼š{str(e)}"

# 4. æˆæœ¬ç›‘æ§
from moltbot import CostTracker

tracker = CostTracker()
agent = Agent(name="åŠ©æ‰‹", llm=llm, cost_tracker=tracker)

# æŸ¥çœ‹æˆæœ¬
print(f"æ€»æˆæœ¬ï¼š${tracker.total_cost():.4f}")
```

### æ€§èƒ½ä¼˜åŒ–

```python
# 1. å¯ç”¨ç¼“å­˜
from moltbot import Cache

cache = Cache(max_size=1000, ttl=3600)
agent = Agent(name="åŠ©æ‰‹", llm=llm, cache=cache)

# 2. æ‰¹é‡å¤„ç†
questions = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
responses = agent.batch_chat(questions)

# 3. æµå¼è¾“å‡º
for chunk in agent.stream_chat("é•¿é—®é¢˜"):
    print(chunk, end="", flush=True)
```

### å®Œæ•´éƒ¨ç½²æŒ‡å—

æœ¬èŠ‚å°†å¸¦ä½ ä»é›¶å¼€å§‹ï¼Œå®Œæˆ Moltbot åº”ç”¨çš„å®Œæ•´éƒ¨ç½²æµç¨‹ã€‚

#### éƒ¨ç½²æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Moltbot åº”ç”¨éƒ¨ç½²æ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚   å®¢æˆ·ç«¯      â”‚ â†â†’  â”‚  API Gateway  â”‚                 â”‚
â”‚  â”‚ (Web/Mobile) â”‚      â”‚   (Nginx)    â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                â†“                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚         åº”ç”¨æœåŠ¡å™¨ (Gunicorn + Uvicorn)      â”‚        â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚        â”‚
â”‚  â”‚  â”‚   Moltbot Agent åº”ç”¨                 â”‚  â”‚        â”‚
â”‚  â”‚  â”‚  - FastAPI REST API                  â”‚  â”‚        â”‚
â”‚  â”‚  â”‚  - WebSocket (å®æ—¶é€šä¿¡)              â”‚  â”‚        â”‚
â”‚  â”‚  â”‚  - ä»»åŠ¡é˜Ÿåˆ— (Celery/Redis)           â”‚  â”‚        â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚           â†“                â†“             â†“              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Redis      â”‚  â”‚ PostgreSQLâ”‚  â”‚ å‘é‡æ•°æ®åº“   â”‚      â”‚
â”‚  â”‚  (ç¼“å­˜/é˜Ÿåˆ—)  â”‚  â”‚  (æ•°æ®)   â”‚  â”‚  (Chroma)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚           ç›‘æ§å’Œæ—¥å¿—                          â”‚        â”‚
â”‚  â”‚  - Prometheus (æŒ‡æ ‡)                         â”‚        â”‚
â”‚  â”‚  - Grafana (å¯è§†åŒ–)                          â”‚        â”‚
â”‚  â”‚  - ELK Stack (æ—¥å¿—)                          â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### æœ¬åœ°å¼€å‘ç¯å¢ƒæ­å»º

**é¡¹ç›®ç»“æ„**ï¼š

```bash
moltbot-production-app/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI åº”ç”¨å…¥å£
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base.py          # Agent åŸºç±»
â”‚   â”‚   â”œâ”€â”€ customer_service.py
â”‚   â”‚   â””â”€â”€ code_assistant.py
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ routes.py        # API è·¯ç”±
â”‚   â”‚   â””â”€â”€ schemas.py       # Pydantic æ¨¡å‹
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py        # é…ç½®ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ security.py      # å®‰å…¨è®¤è¯
â”‚   â”‚   â””â”€â”€ logger.py        # æ—¥å¿—é…ç½®
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_agents.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ start.sh             # å¯åŠ¨è„šæœ¬
â”‚   â””â”€â”€ deploy.sh            # éƒ¨ç½²è„šæœ¬
â”œâ”€â”€ deployments/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ kubernetes/          # K8s é…ç½®
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

**é…ç½®æ–‡ä»¶ (`app/core/config.py`)**ï¼š

```python
from pydantic_settings import BaseSettings
from typing import Optional
import os

class Settings(BaseSettings):
    """åº”ç”¨é…ç½®"""

    # åº”ç”¨ä¿¡æ¯
    APP_NAME: str = "Moltbot Production App"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = False

    # API é…ç½®
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 8000
    API_PREFIX: str = "/api/v1"

    # CORS
    CORS_ORIGINS: list = ["http://localhost:3000", "https://yourdomain.com"]

    # LLM é…ç½®
    LLM_PROVIDER: str = "openai"  # openai, claude, ollama
    OPENAI_API_KEY: str
    ANTHROPIC_API_KEY: Optional[str] = None
    MODEL_NAME: str = "gpt-4-turbo-preview"
    TEMPERATURE: float = 0.7
    MAX_TOKENS: int = 2000

    # æ•°æ®åº“é…ç½®
    DATABASE_URL: str = "postgresql://user:password@localhost/moltbot"
    REDIS_URL: str = "redis://localhost:6379/0"

    # å‘é‡æ•°æ®åº“
    CHROMA_PERSIST_DIR: str = "./data/chroma"

    # å®‰å…¨é…ç½®
    SECRET_KEY: str
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 30
    ALGORITHM: str = "HS256"

    # ä»»åŠ¡é˜Ÿåˆ—
    CELERY_BROKER_URL: str = "redis://localhost:6379/1"
    CELERY_RESULT_BACKEND: str = "redis://localhost:6379/2"

    # ç›‘æ§é…ç½®
    ENABLE_METRICS: bool = True
    SENTRY_DSN: Optional[str] = None

    # é™æµé…ç½®
    RATE_LIMIT_PER_MINUTE: int = 60

    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
```

**ä¸»åº”ç”¨ (`app/main.py`)**ï¼š

```python
from fastapi import FastAPI, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import JSONResponse
from contextlib import asynccontextmanager
import logging
from prometheus_client import make_asgi_app
from slowapi import Limiter
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded

from app.core.config import settings
from app.core.logger import setup_logger
from app.api.routes import api_router
from app.agents.customer_service import customer_service_agent

# æ—¥å¿—é…ç½®
logger = setup_logger(__name__)

# é™æµå™¨
limiter = Limiter(key_func=get_remote_address)

@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    logger.info("ğŸš€ åº”ç”¨å¯åŠ¨ä¸­...")

    # åˆå§‹åŒ– Agent
    logger.info("åŠ è½½ Moltbot Agents...")
    await customer_service_agent.initialize()

    logger.info("âœ… åº”ç”¨å¯åŠ¨å®Œæˆ")
    yield

    # æ¸…ç†èµ„æº
    logger.info("ğŸ›‘ åº”ç”¨å…³é—­ä¸­...")
    await customer_service_agent.cleanup()
    logger.info("âœ… åº”ç”¨å·²å…³é—­")

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    debug=settings.DEBUG,
    lifespan=lifespan
)

# CORS ä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Gzip å‹ç¼©
app.add_middleware(GZipMiddleware, minimum_size=1000)

# é™æµå™¨
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, lambda req, exc: JSONResponse(
    status_code=status.HTTP_429_TOO_MANY_REQUESTS,
    content={"detail": "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•"}
))

# Prometheus æŒ‡æ ‡
metrics_app = make_asgi_app()
app.mount("/metrics", metrics_app)

# æ³¨å†Œè·¯ç”±
app.include_router(api_router, prefix=settings.API_PREFIX)

# å¥åº·æ£€æŸ¥
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "version": settings.APP_VERSION,
        "agents": {
            "customer_service": "ready"
        }
    }

# æ ¹è·¯å¾„
@app.get("/")
async def root():
    return {
        "message": "Moltbot Production API",
        "version": settings.APP_VERSION,
        "docs": "/docs"
    }

# å…¨å±€å¼‚å¸¸å¤„ç†
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    logger.error(f"å…¨å±€å¼‚å¸¸: {exc}", exc_info=True)
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={"detail": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯"}
    )
```

**API è·¯ç”± (`app/api/routes.py`)**ï¼š

```python
from fastapi import APIRouter, Depends, HTTPException, status
from slowapi import Limiter
from slowapi.util import get_remote_address
from typing import List

from app.api.schemas import (
    ChatRequest,
    ChatResponse,
    AgentInfo,
    HealthResponse
)
from app.agents.base import get_agent
from app.agents.customer_service import customer_service_agent
from app.core.security import get_current_user
from app.core.logger import logger

router = APIRouter()
limiter = Limiter(key_func=get_remote_address)

@router.post("/chat", response_model=ChatResponse)
@limiter.limit("60/minute")
async def chat(
    request: ChatRequest,
    current_user: dict = Depends(get_current_user)
):
    """
    ä¸ Agent å¯¹è¯

    - **agent_id**: Agent ID
    - **message**: ç”¨æˆ·æ¶ˆæ¯
    - **session_id**: ä¼šè¯ IDï¼ˆå¯é€‰ï¼‰
    """
    try:
        logger.info(f"ç”¨æˆ· {current_user['username']} å‘é€æ¶ˆæ¯åˆ° {request.agent_id}")

        # è·å–å¯¹åº”çš„ Agent
        agent = get_agent(request.agent_id)

        # æ‰§è¡Œå¯¹è¯
        response = await agent.chat_async(
            message=request.message,
            session_id=request.session_id,
            user_id=current_user["user_id"]
        )

        return ChatResponse(
            agent_id=request.agent_id,
            response=response["message"],
            session_id=response["session_id"],
            timestamp=response["timestamp"]
        )

    except Exception as e:
        logger.error(f"èŠå¤©é”™è¯¯: {e}", exc_info=True)
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="å¤„ç†è¯·æ±‚æ—¶å‡ºé”™"
        )

@router.get("/agents", response_model=List[AgentInfo])
async def list_agents():
    """è·å–å¯ç”¨çš„ Agent åˆ—è¡¨"""
    return [
        {
            "id": "customer_service",
            "name": "æ™ºèƒ½å®¢æœ",
            "description": "ä¸“ä¸šçš„å®¢æˆ·æœåŠ¡åŠ©æ‰‹",
            "capabilities": ["è®¢å•æŸ¥è¯¢", "é€€æ¬¾å¤„ç†", "äº§å“å’¨è¯¢"]
        },
        {
            "id": "code_assistant",
            "name": "ä»£ç åŠ©æ‰‹",
            "description": "ç¼–ç¨‹å¼€å‘åŠ©æ‰‹",
            "capabilities": ["ä»£ç ç”Ÿæˆ", "Bugä¿®å¤", "ä»£ç ä¼˜åŒ–"]
        }
    ]

@router.get("/agents/{agent_id}", response_model=AgentInfo)
async def get_agent_info(agent_id: str):
    """è·å– Agent è¯¦ç»†ä¿¡æ¯"""
    agents = {
        "customer_service": {
            "id": "customer_service",
            "name": "æ™ºèƒ½å®¢æœ",
            "description": "ä¸“ä¸šçš„å®¢æˆ·æœåŠ¡åŠ©æ‰‹",
            "capabilities": ["è®¢å•æŸ¥è¯¢", "é€€æ¬¾å¤„ç†", "äº§å“å’¨è¯¢"]
        },
        "code_assistant": {
            "id": "code_assistant",
            "name": "ä»£ç åŠ©æ‰‹",
            "description": "ç¼–ç¨‹å¼€å‘åŠ©æ‰‹",
            "capabilities": ["ä»£ç ç”Ÿæˆ", "Bugä¿®å¤", "ä»£ç ä¼˜åŒ–"]
        }
    }

    if agent_id not in agents:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"Agent {agent_id} ä¸å­˜åœ¨"
        )

    return agents[agent_id]
```

**Pydantic æ¨¡å‹ (`app/api/schemas.py`)**ï¼š

```python
from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime

class ChatRequest(BaseModel):
    """èŠå¤©è¯·æ±‚"""
    agent_id: str = Field(..., description="Agent ID")
    message: str = Field(..., min_length=1, max_length=2000, description="ç”¨æˆ·æ¶ˆæ¯")
    session_id: Optional[str] = Field(None, description="ä¼šè¯ ID")

class ChatResponse(BaseModel):
    """èŠå¤©å“åº”"""
    agent_id: str
    response: str
    session_id: str
    timestamp: datetime

class AgentInfo(BaseModel):
    """Agent ä¿¡æ¯"""
    id: str
    name: str
    description: str
    capabilities: List[str]

class HealthResponse(BaseModel):
    """å¥åº·æ£€æŸ¥å“åº”"""
    status: str
    version: str
    agents: dict
```

#### Docker å®¹å™¨åŒ–éƒ¨ç½²

**Dockerfile**ï¼š

```dockerfile
# å¤šé˜¶æ®µæ„å»º - ç”Ÿäº§ä¼˜åŒ–
FROM python:3.11-slim as builder

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir --upgrade pip && \
    pip install --no-cache-dir -r requirements.txt

# ç”Ÿäº§é•œåƒ
FROM python:3.11-slim

# åˆ›å»ºé root ç”¨æˆ·
RUN useradd -m -u 1000 appuser

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…è¿è¡Œæ—¶ä¾èµ–
RUN apt-get update && apt-get install -y \
    curl \
    && rm -rf /var/lib/apt/lists/*

# ä» builder å¤åˆ¶è™šæ‹Ÿç¯å¢ƒ
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY --from=builder /usr/local/bin /usr/local/bin

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY --chown=appuser:appuser . .

# åˆ‡æ¢åˆ°é root ç”¨æˆ·
USER appuser

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

**docker-compose.ymlï¼ˆæœ¬åœ°å¼€å‘ï¼‰**ï¼š

```yaml
version: '3.8'

services:
  # FastAPI åº”ç”¨
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: moltbot-api
    ports:
      - "8000:8000"
    environment:
      - DEBUG=${DEBUG:-False}
      - DATABASE_URL=postgresql://moltbot:${POSTGRES_PASSWORD}@postgres:5432/moltbot
      - REDIS_URL=redis://redis:6379/0
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./app:/app/app
      - ./data:/app/data
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    networks:
      - moltbot-network

  # PostgreSQL æ•°æ®åº“
  postgres:
    image: postgres:16-alpine
    container_name: moltbot-postgres
    environment:
      - POSTGRES_DB=moltbot
      - POSTGRES_USER=moltbot
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-changeme}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - moltbot-network

  # Redis ç¼“å­˜
  redis:
    image: redis:7-alpine
    container_name: moltbot-redis
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    restart: unless-stopped
    networks:
      - moltbot-network

  # Prometheus ç›‘æ§
  prometheus:
    image: prom/prometheus:latest
    container_name: moltbot-prometheus
    volumes:
      - ./deployments/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    ports:
      - "9090:9090"
    restart: unless-stopped
    networks:
      - moltbot-network

  # Grafana å¯è§†åŒ–
  grafana:
    image: grafana/grafana:latest
    container_name: moltbot-grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD:-admin}
    volumes:
      - grafana_data:/var/lib/grafana
      - ./deployments/grafana/dashboards:/etc/grafana/provisioning/dashboards
    ports:
      - "3001:3000"
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - moltbot-network

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  moltbot-network:
    driver: bridge
```

**Prometheus é…ç½® (`deployments/prometheus.yml`)**ï¼š

```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'moltbot-api'
    static_configs:
      - targets: ['api:8000']
    metrics_path: '/metrics'
```

**å¯åŠ¨è„šæœ¬ (`scripts/start.sh`)**ï¼š

```bash
#!/bin/bash

set -e

echo "ğŸš€ å¯åŠ¨ Moltbot åº”ç”¨..."

# æ£€æŸ¥ç¯å¢ƒå˜é‡
if [ ! -f .env ]; then
    echo "âŒ .env æ–‡ä»¶ä¸å­˜åœ¨"
    echo "è¯·å¤åˆ¶ .env.example åˆ° .env å¹¶é…ç½®ç¯å¢ƒå˜é‡"
    exit 1
fi

# æ„å»ºå¹¶å¯åŠ¨
echo "ğŸ“¦ æ„å»º Docker é•œåƒ..."
docker-compose build

echo "ğŸ”„ å¯åŠ¨æœåŠ¡..."
docker-compose up -d

echo "â³ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 10

# å¥åº·æ£€æŸ¥
echo "ğŸ” å¥åº·æ£€æŸ¥..."
if curl -f http://localhost:8000/health; then
    echo "âœ… åº”ç”¨å¯åŠ¨æˆåŠŸï¼"
    echo ""
    echo "ğŸ“Š æœåŠ¡åœ°å€ï¼š"
    echo "  - API: http://localhost:8000"
    echo "  - æ–‡æ¡£: http://localhost:8000/docs"
    echo "  - Prometheus: http://localhost:9090"
    echo "  - Grafana: http://localhost:3001"
else
    echo "âŒ åº”ç”¨å¯åŠ¨å¤±è´¥"
    docker-compose logs
    exit 1
fi
```

#### äº‘æœåŠ¡éƒ¨ç½²

æœ¬èŠ‚ä»‹ç»å¦‚ä½•åœ¨ä¸‰å¤§äº‘å¹³å°éƒ¨ç½² Moltbot åº”ç”¨ã€‚

##### é€‰é¡¹ 1ï¼šAWS éƒ¨ç½²

**æ¶æ„**ï¼š
- EC2 / ECSï¼ˆåº”ç”¨æœåŠ¡å™¨ï¼‰
- RDS PostgreSQLï¼ˆæ•°æ®åº“ï¼‰
- ElastiCache Redisï¼ˆç¼“å­˜ï¼‰
- Application Load Balancerï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
- CloudWatchï¼ˆç›‘æ§ï¼‰

**ä½¿ç”¨ ECS Fargate éƒ¨ç½²**ï¼š

```yaml
# aws/ecs-task-definition.json
{
  "family": "moltbot-task",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "2048",
  "memory": "4096",
  "executionRoleArn": "arn:aws:iam::ACCOUNT_ID:role/ecsTaskExecutionRole",
  "containerDefinitions": [
    {
      "name": "moltbot-api",
      "image": "YOUR_ECR_REPO/moltbot:latest",
      "portMappings": [
        {
          "containerPort": 8000,
          "protocol": "tcp"
        }
      ],
      "environment": [
        {
          "name": "DATABASE_URL",
          "value": "postgresql://user:pass@YOUR_RDS_ENDPOINT/moltbot"
        },
        {
          "name": "REDIS_URL",
          "value": "redis://YOUR_ELASTICACHE_ENDPOINT:6379"
        }
      ],
      "secrets": [
        {
          "name": "OPENAI_API_KEY",
          "valueFrom": "arn:aws:secretsmanager:region:account:secret:openai-key"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/moltbot",
          "awslogs-region": "us-east-1",
          "awslogs-stream-prefix": "ecs"
        }
      ],
      "healthCheck": {
        "command": ["CMD-SHELL", "curl -f http://localhost:8000/health || exit 1"],
        "interval": 30,
        "timeout": 5,
        "retries": 3
      }
    }
  ]
}
```

**éƒ¨ç½²è„šæœ¬ (`aws/deploy.sh`)**ï¼š

```bash
#!/bin/bash

set -e

AWS_REGION="us-east-1"
ECR_REPO="YOUR_ECR_REPO"
ECS_CLUSTER="moltbot-cluster"
ECS_SERVICE="moltbot-service"

echo "ğŸ”¨ æ„å»º Docker é•œåƒ..."
docker build -t moltbot .

echo "ğŸ·ï¸ æ‰“æ ‡ç­¾..."
docker tag moltbot:latest $ECR_REPO:latest

echo "ğŸ” ç™»å½• AWS ECR..."
aws ecr get-login-password --region $AWS_REGION | \
  docker login --username AWS --password-stdin $ECR_REPO

echo "ğŸ“¤ æ¨é€é•œåƒ..."
docker push $ECR_REPO:latest

echo "ğŸ”„ æ›´æ–° ECS æœåŠ¡..."
aws ecs update-service \
  --cluster $ECS_CLUSTER \
  --service $ECS_SERVICE \
  --force-new-deployment \
  --region $AWS_REGION

echo "â³ ç­‰å¾…éƒ¨ç½²å®Œæˆ..."
aws ecs wait services-stable \
  --cluster $ECS_CLUSTER \
  --services $ECS_SERVICE \
  --region $AWS_REGION

echo "âœ… éƒ¨ç½²å®Œæˆï¼"
```

##### é€‰é¡¹ 2ï¼šGoogle Cloud Platform (GCP)

**ä½¿ç”¨ Cloud Run**ï¼š

```yaml
# gcp/cloudbuild.yaml
steps:
  # æ„å»º Docker é•œåƒ
  - name: 'gcr.io/cloud-builders/docker'
    args: ['build', '-t', 'gcr.io/$PROJECT_ID/moltbot:latest', '.']

  # æ¨é€åˆ° Container Registry
  - name: 'gcr.io/cloud-builders/docker'
    args: ['push', 'gcr.io/$PROJECT_ID/moltbot:latest']

  # éƒ¨ç½²åˆ° Cloud Run
  - name: 'gcr.io/cloud-builders/gcloud'
    args:
      - 'run'
      - 'deploy'
      - 'moltbot'
      - '--image'
      - 'gcr.io/$PROJECT_ID/moltbot:latest'
      - '--platform'
      - 'managed'
      - '--region'
      - 'us-central1'
      - '--allow-unauthenticated'
      - '--memory'
      - '4Gi'
      - '--cpu'
      - '2'
      - '--set-env-vars'
      - 'DATABASE_URL=${_DATABASE_URL},REDIS_URL=${_REDIS_URL}'
      - '--set-secrets'
      - 'OPENAI_API_KEY=openai-key:latest'
```

**éƒ¨ç½²å‘½ä»¤**ï¼š

```bash
# å¯ç”¨å¿…è¦çš„ API
gcloud services enable \
  cloudbuild.googleapis.com \
  run.googleapis.com \
  secretmanager.googleapis.com

# åˆ›å»º Secret
echo "your-api-key" | \
  gcloud secrets create openai-key --data-file=-

# è§¦å‘æ„å»º
gcloud builds submit --config gcp/cloudbuild.yaml
```

##### é€‰é¡¹ 3ï¼šAzure Container Instances

```bash
# åˆ›å»ºèµ„æºç»„
az group create --name moltbot-rg --location eastus

# åˆ›å»ºå®¹å™¨æ³¨å†Œè¡¨
az acr create --resource-group moltbot-rg --name moltbotRegistry --sku Basic

# ç™»å½• ACR
az acr login --name moltbotRegistry

# æ„å»ºå¹¶æ¨é€
az acr build --registry moltbotRegistry --image moltbot:latest .

# éƒ¨ç½²åˆ° Container Instances
az container create \
  --resource-group moltbot-rg \
  --name moltbot-api \
  --image moltbotRegistry.azurecr.io/moltbot:latest \
  --cpu 2 \
  --memory 4 \
  --ports 8000 \
  --environment-variables \
    DATABASE_URL=$DATABASE_URL \
    REDIS_URL=$REDIS_URL \
  --secure-environment-variables \
    OPENAI_API_KEY=$OPENAI_API_KEY
```

#### CI/CD è‡ªåŠ¨åŒ–

**GitHub Actions å·¥ä½œæµ** (`.github/workflows/deploy.yml`):

```yaml
name: Build and Deploy

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  # æµ‹è¯•
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install pytest pytest-cov

      - name: Run tests
        run: |
          pytest tests/ --cov=app --cov-report=xml

      - name: Upload coverage
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml

  # æ„å»ºå’Œæ¨é€é•œåƒ
  build:
    needs: test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=sha,prefix={{branch}}-
            type=semver,pattern={{version}}

      - name: Build and push
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

  # éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v4

      - name: Deploy to AWS ECS
        uses: aws-actions/amazon-ecs-deploy-task-definition@v1
        with:
          task-definition: aws/ecs-task-definition.json
          service: moltbot-service
          cluster: moltbot-cluster
          wait-for-service-stability: true

      - name: Notify Slack
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒå®Œæˆï¼'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
        if: always()
```

**GitLab CI/CD** (`.gitlab-ci.yml`):

```yaml
stages:
  - test
  - build
  - deploy

variables:
  DOCKER_IMAGE: $CI_REGISTRY_IMAGE:$CI_COMMIT_SHORT_SHA

# æµ‹è¯•
test:
  stage: test
  image: python:3.11
  script:
    - pip install -r requirements.txt
    - pip install pytest
    - pytest tests/
  coverage: '/TOTAL.*\s+(\d+%)$/'

# æ„å»º
build:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  script:
    - docker login -u $CI_REGISTRY_USER -p $CI_REGISTRY_PASSWORD $CI_REGISTRY
    - docker build -t $DOCKER_IMAGE .
    - docker push $DOCKER_IMAGE

# éƒ¨ç½²åˆ°ç”Ÿäº§
deploy:
  stage: deploy
  image: amazon/aws-cli
  script:
    - aws ecs update-service --cluster moltbot --service moltbot-api --force-new-deployment
  only:
    - main
```

#### ç›‘æ§å’Œæ—¥å¿—

**æ—¥å¿—é…ç½® (`app/core/logger.py`)**ï¼š

```python
import logging
import sys
from pathlib import Path
from loguru import logger as loguru_logger

class InterceptHandler(logging.Handler):
    """å°†æ ‡å‡† logging è½¬å‘åˆ° loguru"""

    def emit(self, record):
        try:
            level = loguru_logger.level(record.levelname).name
        except ValueError:
            level = record.levelno

        frame, depth = logging.currentframe(), 2
        while frame.f_code.co_filename == logging.__file__:
            frame = frame.f_back
            depth += 1

        loguru_logger.opt(depth=depth, exception=record.exc_info).log(
            level, record.getMessage()
        )

def setup_logger(name: str = "moltbot"):
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""

    # ç§»é™¤é»˜è®¤ handler
    loguru_logger.remove()

    # æ§åˆ¶å°è¾“å‡ºï¼ˆå¸¦é¢œè‰²ï¼‰
    loguru_logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        level="INFO",
        colorize=True
    )

    # æ–‡ä»¶è¾“å‡ºï¼ˆæŒ‰æ—¥æœŸè½®è½¬ï¼‰
    loguru_logger.add(
        "logs/moltbot_{time:YYYY-MM-DD}.log",
        rotation="00:00",  # æ¯å¤©åˆå¤œè½®è½¬
        retention="30 days",  # ä¿ç•™ 30 å¤©
        compression="zip",  # å‹ç¼©æ—§æ—¥å¿—
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        level="DEBUG"
    )

    # é”™è¯¯æ—¥å¿—å•ç‹¬è®°å½•
    loguru_logger.add(
        "logs/errors.log",
        rotation="10 MB",
        retention="90 days",
        level="ERROR"
    )

    # æ‹¦æˆªæ ‡å‡† logging
    logging.basicConfig(handlers=[InterceptHandler()], level=0)

    return loguru_logger

logger = setup_logger()
```

**Prometheus æŒ‡æ ‡ (`app/core/metrics.py`)**ï¼š

```python
from prometheus_client import Counter, Histogram, Gauge, Info
import time
from functools import wraps

# å®šä¹‰æŒ‡æ ‡
request_count = Counter(
    'moltbot_requests_total',
    'Total requests',
    ['method', 'endpoint', 'status']
)

request_duration = Histogram(
    'moltbot_request_duration_seconds',
    'Request duration',
    ['method', 'endpoint']
)

agent_chat_count = Counter(
    'moltbot_agent_chats_total',
    'Total agent chats',
    ['agent_id']
)

agent_chat_duration = Histogram(
    'moltbot_agent_chat_duration_seconds',
    'Agent chat duration',
    ['agent_id']
)

active_sessions = Gauge(
    'moltbot_active_sessions',
    'Active sessions',
    ['agent_id']
)

app_info = Info(
    'moltbot_app',
    'Moltbot application info'
)

def track_time(metric: Histogram, *labels):
    """è£…é¥°å™¨ï¼šè·Ÿè¸ªå‡½æ•°æ‰§è¡Œæ—¶é—´"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                metric.labels(*labels).observe(duration)

        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                return result
            finally:
                duration = time.time() - start_time
                metric.labels(*labels).observe(duration)

        return async_wrapper if hasattr(func, '__async__') else sync_wrapper
    return decorator
```

**ä½¿ç”¨ç¤ºä¾‹**ï¼š

```python
from app.core.metrics import agent_chat_count, agent_chat_duration, track_time
from app.core.logger import logger

class CustomerServiceAgent:
    @track_time(agent_chat_duration, 'customer_service')
    async def chat(self, message: str):
        agent_chat_count.labels('customer_service').inc()

        logger.info(f"å¤„ç†æ¶ˆæ¯: {message}")

        # ä¸šåŠ¡é€»è¾‘
        response = await self._process_message(message)

        logger.info(f"å“åº”: {response}")
        return response
```

**Grafana ä»ªè¡¨æ¿é…ç½®**ï¼š

```json
{
  "dashboard": {
    "title": "Moltbot ç›‘æ§",
    "panels": [
      {
        "title": "è¯·æ±‚é€Ÿç‡",
        "targets": [
          {
            "expr": "rate(moltbot_requests_total[5m])"
          }
        ]
      },
      {
        "title": "å“åº”æ—¶é—´",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, moltbot_request_duration_seconds_bucket)"
          }
        ]
      },
      {
        "title": "Agent å¯¹è¯æ¬¡æ•°",
        "targets": [
          {
            "expr": "sum by (agent_id) (moltbot_agent_chats_total)"
          }
        ]
      },
      {
        "title": "æ´»è·ƒä¼šè¯æ•°",
        "targets": [
          {
            "expr": "sum by (agent_id) (moltbot_active_sessions)"
          }
        ]
      }
    ]
  }
}
```

#### å®‰å…¨æœ€ä½³å®è·µ

```python
# app/core/security.py

from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from jose import JWTError, jwt
from passlib.context import CryptContext
from datetime import datetime, timedelta
from typing import Optional

from app.core.config import settings

# å¯†ç å“ˆå¸Œ
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

# JWT Bearer
security = HTTPBearer()

def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """åˆ›å»ºè®¿é—®ä»¤ç‰Œ"""
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, settings.SECRET_KEY, algorithm=settings.ALGORITHM)
    return encoded_jwt

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """éªŒè¯ JWT ä»¤ç‰Œ"""
    try:
        payload = jwt.decode(
            credentials.credentials,
            settings.SECRET_KEY,
            algorithms=[settings.ALGORITHM]
        )
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED,
                detail="æ— æ•ˆçš„è®¤è¯å‡­æ®"
            )
        return {"username": username, "user_id": payload.get("user_id")}
    except JWTError:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="æ— æ•ˆçš„è®¤è¯å‡­æ®"
        )

def get_current_user(current_user: dict = Depends(verify_token)):
    """è·å–å½“å‰ç”¨æˆ·ï¼ˆä¾èµ–æ³¨å…¥ï¼‰"""
    return current_user

def hash_password(password: str) -> str:
    """å“ˆå¸Œå¯†ç """
    return pwd_context.hash(password)

def verify_password(plain_password: str, hashed_password: str) -> bool:
    """éªŒè¯å¯†ç """
    return pwd_context.verify(plain_password, hashed_password)
```

#### éƒ¨ç½²æ£€æŸ¥æ¸…å•

åœ¨éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒå‰ï¼Œè¯·ç¡®è®¤ä»¥ä¸‹é¡¹ç›®ï¼š

**ç¯å¢ƒé…ç½®**ï¼š
- [ ] æ‰€æœ‰ç¯å¢ƒå˜é‡å·²æ­£ç¡®é…ç½®
- [ ] `.env` æ–‡ä»¶å·²ä»ç‰ˆæœ¬æ§åˆ¶ä¸­æ’é™¤
- [ ] æ•æ„Ÿä¿¡æ¯ä½¿ç”¨ Secret Manager å­˜å‚¨
- [ ] æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²æ­£ç¡®
- [ ] API å¯†é’¥æœ‰æ•ˆä¸”æœ‰è¶³å¤Ÿé…é¢

**å®‰å…¨æ£€æŸ¥**ï¼š
- [ ] å¯ç”¨ HTTPSï¼ˆSSL è¯ä¹¦ï¼‰
- [ ] é…ç½® CORS ç™½åå•
- [ ] å¯ç”¨é€Ÿç‡é™åˆ¶
- [ ] å®æ–½è¾“å…¥éªŒè¯
- [ ] ä½¿ç”¨å¼ºå¯†ç å’Œ JWT
- [ ] å®šæœŸæ›´æ–°ä¾èµ–åŒ…

**æ€§èƒ½ä¼˜åŒ–**ï¼š
- [ ] å¯ç”¨ Redis ç¼“å­˜
- [ ] é…ç½®æ•°æ®åº“è¿æ¥æ± 
- [ ] å¯ç”¨ Gzip å‹ç¼©
- [ ] ä½¿ç”¨ CDNï¼ˆå¦‚éœ€è¦ï¼‰
- [ ] å¼‚æ­¥å¤„ç†é•¿æ—¶é—´ä»»åŠ¡

**ç›‘æ§å’Œæ—¥å¿—**ï¼š
- [ ] Prometheus æŒ‡æ ‡æ­£å¸¸é‡‡é›†
- [ ] æ—¥å¿—æ­£ç¡®è¾“å‡ºå’Œè½®è½¬
- [ ] é…ç½®å‘Šè­¦è§„åˆ™
- [ ] Grafana ä»ªè¡¨æ¿é…ç½®
- [ ] Sentry é”™è¯¯è¿½è¸ªï¼ˆå¯é€‰ï¼‰

**é«˜å¯ç”¨æ€§**ï¼š
- [ ] é…ç½®è´Ÿè½½å‡è¡¡
- [ ] æ•°æ®åº“å¤‡ä»½ç­–ç•¥
- [ ] è‡ªåŠ¨æ‰©ç¼©å®¹é…ç½®
- [ ] å¥åº·æ£€æŸ¥ç«¯ç‚¹æ­£å¸¸
- [ ] ä¼˜é›…å…³é—­æœºåˆ¶

**æµ‹è¯•**ï¼š
- [ ] å•å…ƒæµ‹è¯•é€šè¿‡
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] è´Ÿè½½æµ‹è¯•å®Œæˆ
- [ ] ç¾éš¾æ¢å¤æ¼”ç»ƒ

---

**ä¸‹ä¸€æ­¥**ï¼šåœ¨ 7.5 èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†å­¦ä¹ å®Œæ•´çš„ç«¯åˆ°ç«¯å®æˆ˜é¡¹ç›®ï¼Œå°†ä»¥ä¸Šæ‰€æœ‰å†…å®¹æ•´åˆåˆ°ä¸€èµ·ã€‚

---

### ç«¯åˆ°ç«¯å®æˆ˜é¡¹ç›®ï¼šä¼ä¸šçº§æ™ºèƒ½å®¢æœç³»ç»Ÿ

æœ¬èŠ‚å°†å¸¦ä½ ä»é›¶åˆ°ä¸€å®Œæˆä¸€ä¸ªå®Œæ•´çš„**ä¼ä¸šçº§æ™ºèƒ½å®¢æœç³»ç»Ÿ**ï¼Œæ¶µç›–å¼€å‘ã€æµ‹è¯•ã€éƒ¨ç½²å…¨æµç¨‹ã€‚

#### é¡¹ç›®æ¦‚è¿°

**åŠŸèƒ½ç‰¹æ€§**ï¼š
- ğŸ¤– å¤šè½®å¯¹è¯æ™ºèƒ½å®¢æœï¼ˆæ”¯æŒä¸Šä¸‹æ–‡è®°å¿†ï¼‰
- ğŸ“š RAG çŸ¥è¯†åº“ï¼ˆä¼ä¸šäº§å“æ–‡æ¡£ï¼‰
- ğŸ” å·¥å…·è°ƒç”¨ï¼ˆè®¢å•æŸ¥è¯¢ã€é€€æ¬¾å¤„ç†ã€ç‰©æµè·Ÿè¸ªï¼‰
- ğŸ’¬ å¤šæ¸ é“æ”¯æŒï¼ˆWebã€å¾®ä¿¡ã€APIï¼‰
- ğŸ“Š å®æ—¶ç›‘æ§å’Œåˆ†æ
- ğŸ” ä¼ä¸šçº§å®‰å…¨è®¤è¯

**æŠ€æœ¯æ ˆ**ï¼š
- åç«¯ï¼šFastAPI + Moltbot + PostgreSQL + Redis
- å‰ç«¯ï¼šVue3 + ElementPlus
- éƒ¨ç½²ï¼šDocker + Nginx + Cloud Run
- ç›‘æ§ï¼šPrometheus + Grafana + Sentry

#### é¡¹ç›®åˆå§‹åŒ–

**åˆ›å»ºé¡¹ç›®**ï¼š

```bash
# é¡¹ç›®åç§°
PROJECT_NAME="enterprise-cs-bot"

# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir -p $PROJECT_NAME/{app,tests,deployments,docs}
cd $PROJECT_NAME

# åˆå§‹åŒ– Git
git init

# åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒ
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# åˆ›å»ºåŸºæœ¬æ–‡ä»¶ç»“æ„
touch requirements.txt
touch README.md
touch .env.example
touch .gitignore
touch docker-compose.yml
```

**`.gitignore`**ï¼š

```gitignore
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/

# ç¯å¢ƒå˜é‡
.env
.env.local

# IDE
.vscode/
.idea/
*.swp
*.swo

# æ—¥å¿—
logs/
*.log

# æ•°æ®åº“
*.db
*.sqlite3

# å‘é‡æ•°æ®åº“
data/chroma/

# Docker
.dockerignore

# æµ‹è¯•
.pytest_cache/
.coverage
htmlcov/

# MacOS
.DS_Store
```

**`requirements.txt`**ï¼š

```txt
# FastAPI æ ¸å¿ƒ
fastapi==0.109.0
uvicorn[standard]==0.27.0
pydantic==2.5.3
pydantic-settings==2.1.0

# ASGI æœåŠ¡å™¨
gunicorn==21.2.0

# å®‰å…¨è®¤è¯
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.6

# æ•°æ®åº“
sqlalchemy==2.0.25
asyncpg==0.29.0
alembic==1.13.1

# Redis
redis==5.0.1
hiredis==2.3.2

# Moltbot
moltbot==0.3.0

# LangChainï¼ˆå¯é€‰ï¼Œç”¨äºé«˜çº§åŠŸèƒ½ï¼‰
langchain==0.1.0
langchain-openai==0.0.5
langchain-community==0.0.16

# å‘é‡æ•°æ®åº“
chromadb==0.4.22

# å·¥å…·åº“
httpx==0.26.0
aiofiles==23.2.1
python-dotenv==1.0.0

# ä»»åŠ¡é˜Ÿåˆ—
celery==5.3.4

# ç›‘æ§å’Œæ—¥å¿—
prometheus-client==0.19.0
loguru==0.7.2
sentry-sdk==1.40.0

# é™æµ
slowapi==0.1.9

# CORS
python-multipart==0.0.6

# æµ‹è¯•
pytest==7.4.4
pytest-asyncio==0.23.3
pytest-cov==4.1.0
httpx==0.26.0

# ä»£ç è´¨é‡
black==24.1.1
flake8==7.0.0
mypy==1.8.0
```

**`README.md`**ï¼š

```markdown
# ä¼ä¸šçº§æ™ºèƒ½å®¢æœç³»ç»Ÿ

åŸºäº Moltbot çš„ä¼ä¸šçº§æ™ºèƒ½å®¢æœè§£å†³æ–¹æ¡ˆã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ¤– å¤šè½®å¯¹è¯æ™ºèƒ½å®¢æœ
- ğŸ“š RAG çŸ¥è¯†åº“æ£€ç´¢
- ğŸ” å·¥å…·è°ƒç”¨å’Œè‡ªåŠ¨åŒ–
- ğŸ’¬ å¤šæ¸ é“æ¥å…¥
- ğŸ“Š å®æ—¶ç›‘æ§åˆ†æ
- ğŸ” ä¼ä¸šçº§å®‰å…¨

## å¿«é€Ÿå¼€å§‹

\`\`\`bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env æ–‡ä»¶ï¼Œå¡«å…¥ä½ çš„é…ç½®

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# è®¿é—®æ–‡æ¡£
open http://localhost:8000/docs
\`\`\`

## é¡¹ç›®ç»“æ„

\`\`\`
â”œâ”€â”€ app/                 # åº”ç”¨ä»£ç 
â”œâ”€â”€ tests/              # æµ‹è¯•ä»£ç 
â”œâ”€â”€ deployments/        # éƒ¨ç½²é…ç½®
â”œâ”€â”€ docs/              # æ–‡æ¡£
â””â”€â”€ scripts/           # è„šæœ¬
\`\`\`

## å¼€å‘æŒ‡å—

è¯¦è§ [docs/DEVELOPMENT.md](docs/DEVELOPMENT.md)

## éƒ¨ç½²æŒ‡å—

è¯¦è§ [docs/DEPLOYMENT.md](docs/DEPLOYMENT.md)

## License

MIT
```

#### åç«¯å¼€å‘

**é…ç½®ç®¡ç†** (`app/core/config.py`):

```python
from pydantic_settings import BaseSettings
from typing import List, Optional
import os

class Settings(BaseSettings):
    """åº”ç”¨é…ç½®"""

    # åº”ç”¨ä¿¡æ¯
    APP_NAME: str = "Enterprise Customer Service Bot"
    APP_VERSION: str = "1.0.0"
    DEBUG: bool = False
    ENVIRONMENT: str = "development"  # development, staging, production

    # API é…ç½®
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 8000
    API_PREFIX: str = "/api/v1"

    # CORS
    CORS_ORIGINS: List[str] = [
        "http://localhost:3000",
        "http://localhost:8080",
        "https://yourdomain.com"
    ]

    # LLM é…ç½®
    LLM_PROVIDER: str = "openai"
    OPENAI_API_KEY: str
    OPENAI_MODEL: str = "gpt-4-turbo-preview"
    OPENAI_TEMPERATURE: float = 0.7
    OPENAI_MAX_TOKENS: int = 2000

    # æ•°æ®åº“é…ç½®
    DATABASE_URL: str
    DATABASE_POOL_SIZE: int = 20
    DATABASE_MAX_OVERFLOW: int = 10

    # Redis é…ç½®
    REDIS_URL: str
    REDIS_MAX_CONNECTIONS: int = 50

    # å‘é‡æ•°æ®åº“
    CHROMA_PERSIST_DIR: str = "./data/chroma"
    EMBEDDING_MODEL: str = "text-embedding-3-small"

    # JWT é…ç½®
    SECRET_KEY: str
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 60
    REFRESH_TOKEN_EXPIRE_DAYS: int = 7

    # æ–‡ä»¶ä¸Šä¼ 
    MAX_UPLOAD_SIZE: int = 10 * 1024 * 1024  # 10MB
    ALLOWED_FILE_TYPES: List[str] = [".pdf", ".txt", ".md", ".docx"]

    # é™æµé…ç½®
    RATE_LIMIT_PER_MINUTE: int = 100
    BURST_RATE_LIMIT: int = 200

    # ç›‘æ§é…ç½®
    ENABLE_SENTRY: bool = True
    SENTRY_DSN: Optional[str] = None
    ENABLE_PROMETHEUS: bool = True

    # é‚®ä»¶é…ç½®ï¼ˆç”¨äºé€šçŸ¥ï¼‰
    SMTP_HOST: Optional[str] = None
    SMTP_PORT: int = 587
    SMTP_USER: Optional[str] = None
    SMTP_PASSWORD: Optional[str] = None

    class Config:
        env_file = ".env"
        case_sensitive = True

settings = Settings()
```

**æ•°æ®åº“æ¨¡å‹** (`app/models/models.py`):

```python
from sqlalchemy import Column, Integer, String, Text, DateTime, ForeignKey, Boolean
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import relationship
from datetime import datetime

Base = declarative_base()

class User(Base):
    """ç”¨æˆ·è¡¨"""
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    username = Column(String(50), unique=True, index=True, nullable=False)
    email = Column(String(100), unique=True, index=True, nullable=False)
    hashed_password = Column(String(200), nullable=False)
    is_active = Column(Boolean, default=True)
    is_admin = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # å…³ç³»
    conversations = relationship("Conversation", back_populates="user")

class Conversation(Base):
    """å¯¹è¯ä¼šè¯è¡¨"""
    __tablename__ = "conversations"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), nullable=False)
    session_id = Column(String(100), unique=True, index=True, nullable=False)
    title = Column(String(200))
    status = Column(String(20), default="active")  # active, closed, archived
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)

    # å…³ç³»
    user = relationship("User", back_populates="conversations")
    messages = relationship("Message", back_populates="conversation")

class Message(Base):
    """æ¶ˆæ¯è¡¨"""
    __tablename__ = "messages"

    id = Column(Integer, primary_key=True, index=True)
    conversation_id = Column(Integer, ForeignKey("conversations.id"), nullable=False)
    role = Column(String(20), nullable=False)  # user, assistant, system
    content = Column(Text, nullable=False)
    metadata = Column(Text)  # JSON å­—ç¬¦ä¸²ï¼Œå­˜å‚¨é¢å¤–ä¿¡æ¯
    created_at = Column(DateTime, default=datetime.utcnow)

    # å…³ç³»
    conversation = relationship("Conversation", back_populates="messages")

class Document(Base):
    """æ–‡æ¡£è¡¨ï¼ˆçŸ¥è¯†åº“ï¼‰"""
    __tablename__ = "documents"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String(200), nullable=False)
    content = Column(Text, nullable=False)
    category = Column(String(50))
    tags = Column(String(200))  # é€—å·åˆ†éš”çš„æ ‡ç­¾
    file_path = Column(String(500))
    is_active = Column(Boolean, default=True)
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
```

**æ•°æ®åº“è¿æ¥** (`app/core/database.py`):

```python
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker
from typing import AsyncGenerator
from app.core.config import settings
from app.models.models import Base

# åˆ›å»ºå¼‚æ­¥å¼•æ“
engine = create_async_engine(
    settings.DATABASE_URL,
    echo=settings.DEBUG,
    pool_size=settings.DATABASE_POOL_SIZE,
    max_overflow=settings.DATABASE_MAX_OVERFLOW
)

# åˆ›å»ºä¼šè¯å·¥å‚
async_session_maker = sessionmaker(
    engine,
    class_=AsyncSession,
    expire_on_commit=False
)

async def get_db() -> AsyncGenerator[AsyncSession, None]:
    """è·å–æ•°æ®åº“ä¼šè¯ï¼ˆä¾èµ–æ³¨å…¥ï¼‰"""
    async with async_session_maker() as session:
        try:
            yield session
            await session.commit()
        except Exception:
            await session.rollback()
            raise
        finally:
            await session.close()

async def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
    async with engine.begin() as conn:
        await conn.run_sync(Base.metadata.create_all)
```

**å®¢æœ Agent å®ç°** (`app/agents/customer_service.py`):

```python
from moltbot import Agent, Tool
from moltbot.memory import ConversationMemory
from moltbot.llm import OpenAILLM
from typing import Optional, Dict, Any
from app.core.config import settings
from app.core.logger import logger
from app.core.database import async_session_maker
from app.models.models import Message, Conversation
from sqlalchemy import select
import json

class CustomerServiceAgent:
    """æ™ºèƒ½å®¢æœ Agent"""

    def __init__(self):
        self.llm = OpenAILLM(
            api_key=settings.OPENAI_API_KEY,
            model=settings.OPENAI_MODEL,
            temperature=settings.OPENAI_TEMPERATURE,
            max_tokens=settings.OPENAI_MAX_TOKENS
        )

        # åˆ›å»ºè®°å¿†ç³»ç»Ÿ
        self.memory = ConversationMemory(
            max_history=20,
            persist=True,
            storage_path=settings.CHROMA_PERSIST_DIR
        )

        # åˆ›å»º Agent
        self.agent = Agent(
            name="æ™ºèƒ½å®¢æœ",
            llm=self.llm,
            instructions=self._get_instructions(),
            memory=self.memory
        )

        # æ·»åŠ å·¥å…·
        self._register_tools()

    def _get_instructions(self) -> str:
        """è·å– Agent æŒ‡ä»¤"""
        return """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ™ºèƒ½å®¢æœåŠ©æ‰‹ï¼Œåä¸º"å°æ™º"ã€‚

        ## è§’è‰²å®šä½
        - å‹å¥½ã€ä¸“ä¸šã€è€å¿ƒçš„å®¢æœä»£è¡¨
        - ä»£è¡¨å…¬å¸å“ç‰Œå½¢è±¡
        - è‡´åŠ›äºæä¾›å“è¶Šçš„å®¢æˆ·æœåŠ¡

        ## å·¥ä½œåŸåˆ™
        1. **å‹å¥½çƒ­æƒ…**ï¼šä½¿ç”¨ç¤¼è²Œã€çƒ­æƒ…çš„è¯­è¨€
        2. **ä¸“ä¸šå‡†ç¡®**ï¼šæä¾›å‡†ç¡®çš„ä¿¡æ¯å’Œè§£å†³æ–¹æ¡ˆ
        3. **é«˜æ•ˆå¿«é€Ÿ**ï¼šå¿«é€Ÿå“åº”ï¼Œä¸æµªè´¹å®¢æˆ·æ—¶é—´
        4. **åŒç†å¿ƒ**ï¼šç†è§£å®¢æˆ·çš„æƒ…ç»ªå’Œéœ€æ±‚
        5. **è¯šå®é€æ˜**ï¼šä¸ç¡®å®šçš„ä¿¡æ¯å¦è¯šå‘ŠçŸ¥

        ## å¯¹è¯æµç¨‹
        1. é—®å€™å’Œäº†è§£éœ€æ±‚
        2. åˆ†æé—®é¢˜ç±»å‹
        3. ä½¿ç”¨å·¥å…·æŸ¥è¯¢ä¿¡æ¯
        4. æä¾›è§£å†³æ–¹æ¡ˆ
        5. ç¡®è®¤æ»¡æ„åº¦
        6. è®°å½•åé¦ˆ

        ## è¯­è¨€é£æ ¼
        - ä½¿ç”¨ç®€æ´ã€æ¸…æ™°çš„è¯­è¨€
        - é¿å…æŠ€æœ¯æœ¯è¯­
        - é€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·ï¼ˆä¿æŒä¸“ä¸šï¼‰
        - ä¸»åŠ¨æä¾›å¸®åŠ©

        ## é™åˆ¶
        - ä¸é€éœ²å…¬å¸å†…éƒ¨ä¿¡æ¯
        - ä¸åšå‡ºæ— æ³•å…‘ç°çš„æ‰¿è¯º
        - é‡åˆ°æ— æ³•è§£å†³çš„é—®é¢˜ï¼Œå¼•å¯¼è”ç³»äººå·¥å®¢æœ
        """

    def _register_tools(self):
        """æ³¨å†Œå·¥å…·"""

        # å·¥å…· 1ï¼šæŸ¥è¯¢è®¢å•
        self.agent.add_tool(Tool(
            name="query_order",
            description="æŸ¥è¯¢è®¢å•ä¿¡æ¯ï¼ŒåŒ…æ‹¬è®¢å•çŠ¶æ€ã€ç‰©æµä¿¡æ¯ç­‰",
            function=self._query_order
        ))

        # å·¥å…· 2ï¼šå¤„ç†é€€æ¬¾
        self.agent.add_tool(Tool(
            name="process_refund",
            description="å¤„ç†é€€æ¬¾ç”³è¯·",
            function=self._process_refund
        ))

        # å·¥å…· 3ï¼šæŸ¥è¯¢äº§å“ä¿¡æ¯
        self.agent.add_tool(Tool(
            name="query_product",
            description="æŸ¥è¯¢äº§å“ä¿¡æ¯ï¼ŒåŒ…æ‹¬ä»·æ ¼ã€åº“å­˜ã€è§„æ ¼ç­‰",
            function=self._query_product
        ))

        # å·¥å…· 4ï¼šæœç´¢çŸ¥è¯†åº“
        self.agent.add_tool(Tool(
            name="search_knowledge",
            description="æœç´¢å…¬å¸çŸ¥è¯†åº“ï¼ŒæŸ¥æ‰¾å¸¸è§é—®é¢˜è§£ç­”",
            function=self._search_knowledge
        ))

    async def _query_order(self, order_id: str) -> Dict[str, Any]:
        """æŸ¥è¯¢è®¢å•ä¿¡æ¯"""
        logger.info(f"æŸ¥è¯¢è®¢å•: {order_id}")

        # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
        # å®é™…åº”ç”¨ä¸­ä»æ•°æ®åº“æŸ¥è¯¢
        order_info = {
            "order_id": order_id,
            "status": "å·²å‘è´§",
            "products": [
                {"name": "å•†å“A", "quantity": 2, "price": 99.00},
                {"name": "å•†å“B", "quantity": 1, "price": 199.00}
            ],
            "total": 397.00,
            "shipping_address": "åŒ—äº¬å¸‚æœé˜³åŒºxxx",
            "tracking_number": "SF1234567890",
            "estimated_delivery": "2024-03-20"
        }

        return order_info

    async def _process_refund(self, order_id: str, reason: str) -> Dict[str, Any]:
        """å¤„ç†é€€æ¬¾ç”³è¯·"""
        logger.info(f"å¤„ç†é€€æ¬¾ç”³è¯·: è®¢å•={order_id}, åŸå› ={reason}")

        # æ¨¡æ‹Ÿé€€æ¬¾å¤„ç†
        refund_result = {
            "success": True,
            "refund_id": f"REF{order_id}",
            "amount": 397.00,
            "estimated_time": "3-5ä¸ªå·¥ä½œæ—¥",
            "message": "é€€æ¬¾ç”³è¯·å·²æäº¤ï¼Œå®¡æ ¸é€šè¿‡åå°†åŸè·¯è¿”å›"
        }

        return refund_result

    async def _query_product(self, product_name: str) -> Dict[str, Any]:
        """æŸ¥è¯¢äº§å“ä¿¡æ¯"""
        logger.info(f"æŸ¥è¯¢äº§å“: {product_name}")

        # æ¨¡æ‹Ÿäº§å“æŸ¥è¯¢
        product_info = {
            "name": product_name,
            "price": 199.00,
            "stock": 150,
            "description": "è¿™æ˜¯ä¸€æ¬¾ä¼˜è´¨äº§å“...",
            "specifications": {
                "color": "å¤šç§é¢œè‰²å¯é€‰",
                "size": "S/M/L/XL",
                "material": "ä¼˜è´¨é¢æ–™"
            },
            "reviews": {
                "average_rating": 4.8,
                "total_reviews": 1250
            }
        }

        return product_info

    async def _search_knowledge(self, query: str) -> Dict[str, Any]:
        """æœç´¢çŸ¥è¯†åº“"""
        logger.info(f"æœç´¢çŸ¥è¯†åº“: {query}")

        # å®é™…åº”ç”¨ä¸­ä½¿ç”¨å‘é‡æœç´¢
        # è¿™é‡Œç®€åŒ–ä¸ºè¿”å›ç›¸å…³æ–‡æ¡£
        knowledge = {
            "query": query,
            "results": [
                {
                    "title": "é€€æ¬¾æ”¿ç­–",
                    "content": "è®¢å•ç­¾æ”¶å7å¤©å†…å¯ç”³è¯·æ— ç†ç”±é€€æ¬¾...",
                    "relevance": 0.95
                },
                {
                    "title": "ç‰©æµé…é€è¯´æ˜",
                    "content": "å…¨å›½åŒ…é‚®ï¼Œ2-3ä¸ªå·¥ä½œæ—¥é€è¾¾...",
                    "relevance": 0.87
                }
            ]
        }

        return knowledge

    async def chat(
        self,
        message: str,
        user_id: int,
        session_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""

        logger.info(f"ç”¨æˆ· {user_id} å‘é€æ¶ˆæ¯: {message}")

        try:
            # è°ƒç”¨ Moltbot Agent
            response = await self.agent.chat_async(
                message=message,
                session_id=session_id or f"session_{user_id}"
            )

            # ä¿å­˜åˆ°æ•°æ®åº“
            await self._save_conversation(
                user_id=user_id,
                session_id=session_id or f"session_{user_id}",
                user_message=message,
                assistant_message=response
            )

            return {
                "success": True,
                "response": response,
                "session_id": session_id or f"session_{user_id}"
            }

        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}", exc_info=True)
            return {
                "success": False,
                "error": "æŠ±æ­‰ï¼Œç³»ç»Ÿå‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•"
            }

    async def _save_conversation(
        self,
        user_id: int,
        session_id: str,
        user_message: str,
        assistant_message: str
    ):
        """ä¿å­˜å¯¹è¯åˆ°æ•°æ®åº“"""
        async with async_session_maker() as session:
            # æŸ¥æ‰¾æˆ–åˆ›å»ºä¼šè¯
            result = await session.execute(
                select(Conversation).filter_by(session_id=session_id)
            )
            conversation = result.scalar_one_or_none()

            if not conversation:
                conversation = Conversation(
                    user_id=user_id,
                    session_id=session_id,
                    title=user_message[:50]  # ä½¿ç”¨ç¬¬ä¸€æ¡æ¶ˆæ¯ä½œä¸ºæ ‡é¢˜
                )
                session.add(conversation)
                await session.flush()

            # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
            user_msg = Message(
                conversation_id=conversation.id,
                role="user",
                content=user_message
            )
            session.add(user_msg)

            # ä¿å­˜åŠ©æ‰‹æ¶ˆæ¯
            assistant_msg = Message(
                conversation_id=conversation.id,
                role="assistant",
                content=assistant_message
            )
            session.add(assistant_msg)

            await session.commit()

    async def get_conversation_history(
        self,
        session_id: str,
        limit: int = 50
    ) -> list:
        """è·å–å¯¹è¯å†å²"""
        async with async_session_maker() as session:
            result = await session.execute(
                select(Message)
                .join(Conversation)
                .filter(Conversation.session_id == session_id)
                .order_by(Message.created_at)
                .limit(limit)
            )
            messages = result.scalars().all()

            return [
                {
                    "role": msg.role,
                    "content": msg.content,
                    "timestamp": msg.created_at.isoformat()
                }
                for msg in messages
            ]

    async def clear_session(self, session_id: str):
        """æ¸…é™¤ä¼šè¯"""
        # æ¸…é™¤è®°å¿†
        await self.memory.clear_session(session_id)

        logger.info(f"ä¼šè¯ {session_id} å·²æ¸…é™¤")

# åˆ›å»ºå…¨å±€ Agent å®ä¾‹
customer_service_agent = CustomerServiceAgent()
```

#### å‰ç«¯å¼€å‘ï¼ˆVue3ï¼‰

**å‰ç«¯é¡¹ç›®ç»“æ„**ï¼š

```bash
# åˆ›å»ºå‰ç«¯é¡¹ç›®
npm create vue@latest frontend
cd frontend

# å®‰è£…ä¾èµ–
npm install axios element-plus @element-plus/icons-vue
```

**ä¸»ç»„ä»¶ (`frontend/src/views/CustomerService.vue`)**ï¼š

```vue
<template>
  <div class="customer-service-container">
    <el-container>
      <!-- å¤´éƒ¨ -->
      <el-header class="chat-header">
        <div class="header-content">
          <el-avatar :size="40" src="https://cube.elemecdn.com/0/88/03b0d39583f48206768a7534e55bcpng.png" />
          <div class="header-info">
            <h3>æ™ºèƒ½å®¢æœå°æ™º</h3>
            <el-tag type="success" size="small">åœ¨çº¿</el-tag>
          </div>
        </div>
        <el-button circle @click="clearChat">
          <el-icon><Refresh /></el-icon>
        </el-button>
      </el-header>

      <!-- èŠå¤©åŒºåŸŸ -->
      <el-main class="chat-main">
        <div ref="messagesContainer" class="messages-container">
          <div
            v-for="(msg, index) in messages"
            :key="index"
            :class="['message', msg.role]"
          >
            <el-avatar v-if="msg.role === 'assistant'" :size="32" />
            <div class="message-content">
              <div class="message-bubble">{{ msg.content }}</div>
              <div class="message-time">{{ formatTime(msg.timestamp) }}</div>
            </div>
            <el-avatar v-if="msg.role === 'user'" :size="32" />
          </div>

          <!-- åŠ è½½ä¸­ -->
          <div v-if="loading" class="message assistant">
            <el-avatar :size="32" />
            <div class="message-content">
              <div class="message-bubble loading">
                <span class="dot"></span>
                <span class="dot"></span>
                <span class="dot"></span>
              </div>
            </div>
          </div>
        </div>
      </el-main>

      <!-- è¾“å…¥åŒºåŸŸ -->
      <el-footer class="chat-footer">
        <el-input
          v-model="userInput"
          type="textarea"
          :rows="2"
          placeholder="è¾“å…¥æ‚¨çš„é—®é¢˜..."
          @keydown.enter.prevent="sendMessage"
          :disabled="loading"
        />
        <el-button
          type="primary"
          :icon="Promotion"
          @click="sendMessage"
          :loading="loading"
          class="send-button"
        >
          å‘é€
        </el-button>
      </el-footer>
    </el-container>
  </div>
</template>

<script setup>
import { ref, nextTick, onMounted } from 'vue'
import { ElMessage } from 'element-plus'
import { Refresh, Promotion } from '@element-plus/icons-vue'
import axios from 'axios'
import { formatDistanceToNow } from 'date-fns'
import { zhCN } from 'date-fns/locale'

const API_BASE = import.meta.env.VITE_API_BASE || 'http://localhost:8000/api/v1'

// çŠ¶æ€
const messages = ref([])
const userInput = ref('')
const loading = ref(false)
const sessionId = ref(null)
const messagesContainer = ref(null)

// å‘é€æ¶ˆæ¯
const sendMessage = async () => {
  if (!userInput.value.trim() || loading.value) return

  const userMessage = userInput.value
  userInput.value = ''

  // æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
  messages.value.push({
    role: 'user',
    content: userMessage,
    timestamp: new Date()
  })

  // æ»šåŠ¨åˆ°åº•éƒ¨
  await scrollToBottom()

  // æ˜¾ç¤ºåŠ è½½çŠ¶æ€
  loading.value = true

  try {
    const response = await axios.post(`${API_BASE}/chat`, {
      agent_id: 'customer_service',
      message: userMessage,
      session_id: sessionId.value
    })

    // æ·»åŠ åŠ©æ‰‹å›å¤
    messages.value.push({
      role: 'assistant',
      content: response.data.response,
      timestamp: new Date()
    })

    // æ›´æ–°ä¼šè¯ ID
    sessionId.value = response.data.session_id

  } catch (error) {
    console.error('å‘é€æ¶ˆæ¯å¤±è´¥:', error)
    ElMessage.error('å‘é€å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•')

    messages.value.push({
      role: 'assistant',
      content: 'æŠ±æ­‰ï¼Œç½‘ç»œè¿æ¥å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚',
      timestamp: new Date()
    })
  } finally {
    loading.value = false
    await scrollToBottom()
  }
}

// æ»šåŠ¨åˆ°åº•éƒ¨
const scrollToBottom = async () => {
  await nextTick()
  if (messagesContainer.value) {
    messagesContainer.value.scrollTop = messagesContainer.value.scrollHeight
  }
}

// æ ¼å¼åŒ–æ—¶é—´
const formatTime = (timestamp) => {
  return formatDistanceToNow(new Date(timestamp), {
    addSuffix: true,
    locale: zhCN
  })
}

// æ¸…é™¤å¯¹è¯
const clearChat = () => {
  messages.value = []
  sessionId.value = null
  ElMessage.success('å¯¹è¯å·²æ¸…é™¤')
}

// åˆå§‹åŒ–
onMounted(() => {
  // æ¬¢è¿æ¶ˆæ¯
  messages.value.push({
    role: 'assistant',
    content: 'æ‚¨å¥½ï¼æˆ‘æ˜¯æ™ºèƒ½å®¢æœå°æ™ºï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨ï¼Ÿ',
    timestamp: new Date()
  })
})
</script>

<style scoped>
.customer-service-container {
  height: 100vh;
  background: #f5f7fa;
}

.chat-header {
  background: white;
  border-bottom: 1px solid #e4e7ed;
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 0 20px;
}

.header-content {
  display: flex;
  align-items: center;
  gap: 15px;
}

.header-info h3 {
  margin: 0;
  font-size: 18px;
}

.chat-main {
  padding: 20px;
  overflow-y: auto;
}

.messages-container {
  max-height: calc(100vh - 200px);
  overflow-y: auto;
}

.message {
  display: flex;
  gap: 10px;
  margin-bottom: 20px;
  align-items: flex-start;
}

.message.user {
  flex-direction: row-reverse;
}

.message-content {
  max-width: 60%;
}

.message-bubble {
  padding: 12px 16px;
  border-radius: 12px;
  background: white;
  box-shadow: 0 2px 8px rgba(0,0,0,0.05);
  line-height: 1.6;
}

.message.user .message-bubble {
  background: #409eff;
  color: white;
}

.message.assistant .message-bubble {
  background: white;
  color: #333;
}

.message-time {
  font-size: 12px;
  color: #909399;
  margin-top: 5px;
  text-align: right;
}

.message-bubble.loading {
  display: flex;
  gap: 5px;
  padding: 15px 20px;
}

.dot {
  width: 8px;
  height: 8px;
  background: #409eff;
  border-radius: 50%;
  animation: bounce 1.4s infinite ease-in-out both;
}

.dot:nth-child(1) { animation-delay: -0.32s; }
.dot:nth-child(2) { animation-delay: -0.16s; }

@keyframes bounce {
  0%, 80%, 100% { transform: scale(0); }
  40% { transform: scale(1); }
}

.chat-footer {
  background: white;
  border-top: 1px solid #e4e7ed;
  padding: 15px 20px;
  display: flex;
  gap: 10px;
}

.send-button {
  align-self: flex-end;
}
</style>
```

#### éƒ¨ç½²ä¸Šçº¿

**å®Œæ•´çš„éƒ¨ç½²æµç¨‹åœ¨å‰é¢ 7.4.10 èŠ‚å·²è¯¦ç»†è®²è§£ï¼Œè¿™é‡Œæä¾›å¿«é€Ÿéƒ¨ç½²å‘½ä»¤**ï¼š

```bash
# 1. æ„å»ºå’Œå¯åŠ¨ï¼ˆæœ¬åœ°å¼€å‘ï¼‰
docker-compose up -d

# 2. è¿è¡Œæµ‹è¯•
pytest tests/ -v

# 3. æ„å»ºç”Ÿäº§é•œåƒ
docker build -t enterprise-cs-bot:latest .

# 4. æ¨é€åˆ°é•œåƒä»“åº“
docker tag enterprise-cs-bot:latest YOUR_REGISTRY/enterprise-cs-bot:latest
docker push YOUR_REGISTRY/enterprise-cs-bot:latest

# 5. éƒ¨ç½²åˆ°äº‘å¹³å°
# ä½¿ç”¨ GitHub Actions è‡ªåŠ¨éƒ¨ç½²ï¼ˆè§ .github/workflows/deploy.ymlï¼‰
# æˆ–æ‰‹åŠ¨éƒ¨ç½²åˆ°äº‘å¹³å°ï¼ˆè§ 7.4.10.3 èŠ‚ï¼‰

# 6. éªŒè¯éƒ¨ç½²
curl https://your-domain.com/health
```

#### ç›‘æ§å’Œç»´æŠ¤

**å…³é”®æŒ‡æ ‡ç›‘æ§**ï¼š
- è¯·æ±‚æˆåŠŸç‡ï¼ˆç›®æ ‡ï¼š> 99.9%ï¼‰
- å¹³å‡å“åº”æ—¶é—´ï¼ˆç›®æ ‡ï¼š< 2ç§’ï¼‰
- å¹¶å‘ä¼šè¯æ•°
- Agent å·¥å…·è°ƒç”¨æˆåŠŸç‡
- ç”¨æˆ·æ»¡æ„åº¦è¯„åˆ†

**æ—¥å¸¸ç»´æŠ¤**ï¼š
- æ¯æ—¥æŸ¥çœ‹æ—¥å¿—å’Œé”™è¯¯æŠ¥å‘Š
- æ¯å‘¨åˆ†æç”¨æˆ·åé¦ˆ
- æ¯æœˆæ›´æ–°çŸ¥è¯†åº“å†…å®¹
- å®šæœŸä¼˜åŒ– Prompt å’Œå·¥å…·

---

**é¡¹ç›®æ€»ç»“**ï¼š

é€šè¿‡è¿™ä¸ªå®Œæ•´çš„å®æˆ˜é¡¹ç›®ï¼Œä½ å·²ç»æŒæ¡äº†ï¼š

âœ… ä»é›¶åˆ°ä¸€æ„å»ºä¼ä¸šçº§ AI åº”ç”¨
âœ… Moltbot Agent å¼€å‘çš„æœ€ä½³å®è·µ
âœ… å‰åç«¯åˆ†ç¦»æ¶æ„è®¾è®¡
âœ… Docker å®¹å™¨åŒ–éƒ¨ç½²
âœ… CI/CD è‡ªåŠ¨åŒ–æµç¨‹
âœ… ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ
âœ… å®‰å…¨è®¤è¯å’Œæƒé™ç®¡ç†

**ä¸‹ä¸€æ­¥å»ºè®®**ï¼š

1. æ ¹æ®å®é™…ä¸šåŠ¡éœ€æ±‚å®šåˆ¶åŠŸèƒ½
2. æ·»åŠ æ›´å¤š Agent å·¥å…·
3. é›†æˆæ›´å¤šæ¸ é“ï¼ˆå¾®ä¿¡ã€é’‰é’‰ç­‰ï¼‰
4. ä¼˜åŒ–æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒ
5. å»ºç«‹å®Œå–„çš„æµ‹è¯•ä½“ç³»

---

## MCP (Model Context Protocol)

### ä»€ä¹ˆæ˜¯MCPï¼Ÿ

**MCP** æ˜¯ä¸€ä¸ªå¼€æ”¾åè®®ï¼Œè®©AIåº”ç”¨èƒ½å¤Ÿè½»æ¾è¿æ¥åˆ°å¤–éƒ¨æ•°æ®æºå’Œå·¥å…·ã€‚

```
ä¼ ç»Ÿæ–¹å¼ vs MCPï¼š

ä¼ ç»Ÿæ–¹å¼ï¼š
  âŒ æ¯ä¸ªæ•°æ®æºéœ€è¦è‡ªå®šä¹‰é›†æˆ
  âŒ ä»£ç é‡å¤ï¼Œç»´æŠ¤å›°éš¾
  âŒ ç¼ºä¹ç»Ÿä¸€æ ‡å‡†

MCPæ–¹å¼ï¼š
  âœ… ç»Ÿä¸€çš„æ¥å£æ ‡å‡†
  âœ… å³æ’å³ç”¨
  âœ… ç¤¾åŒºç”Ÿæ€å…±äº«
```

### MCPæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MCP æ¶æ„                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                          â”‚
â”‚  AI Application                          â”‚
â”‚    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚  â”‚ MCP Client  â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚    â†“                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ MCP Server  â”‚ â† â†’ â”‚ Data Source  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                          â”‚
â”‚  ç¤ºä¾‹MCP Serversï¼š                       â”‚
â”‚  - æ–‡ä»¶ç³»ç»Ÿè®¿é—®                          â”‚
â”‚  - æ•°æ®åº“æŸ¥è¯¢                            â”‚
â”‚  - Gitæ“ä½œ                              â”‚
â”‚  - Slacké›†æˆ                             â”‚
â”‚  - Google Drive                         â”‚
â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ä½¿ç”¨MCP

**å®‰è£…MCP SDK**ï¼š
```bash
pip install mcp
```

**åˆ›å»ºMCP Server**ï¼š
```python
# my_mcp_server.py
from mcp.server import Server
from mcp.types import Tool, TextContent
import subprocess

app = Server("my-tools")

@app.tool()
def execute_command(command: str) -> str:
    """æ‰§è¡Œshellå‘½ä»¤"""
    try:
        result = subprocess.run(
            command,
            shell=True,
            capture_output=True,
            text=True
        )
        return f"è¾“å‡ºï¼š\n{result.stdout}\né”™è¯¯ï¼š\n{result.stderr}"
    except Exception as e:
        return f"é”™è¯¯ï¼š{str(e)}"

@app.tool()
def read_file(file_path: str) -> str:
    """è¯»å–æ–‡ä»¶å†…å®¹"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return f.read()
    except Exception as e:
        return f"é”™è¯¯ï¼š{str(e)}"

# å¯åŠ¨æœåŠ¡å™¨
if __name__ == "__main__":
    app.run()
```

**LangChainä¸­ä½¿ç”¨MCP**ï¼š
```python
from langchain_mcp import MCPToolkit

# è¿æ¥åˆ°MCPæœåŠ¡å™¨
toolkit = MCPToolkit(
    server_url="http://localhost:3000"
)

# è·å–å·¥å…·
tools = toolkit.get_tools()

# åˆ›å»ºAgent
from langchain.agents import create_openai_functions_agent
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-3.5-turbo")
agent = create_openai_functions_agent(llm, tools, prompt)

# ä½¿ç”¨
from langchain.agents import AgentExecutor

executor = AgentExecutor(agent=agent, tools=tools)
result = executor.invoke({"input": "è¯»å–config.pyæ–‡ä»¶"})
```

### å¸¸ç”¨MCP Servers

```bash
# 1. æ–‡ä»¶ç³»ç»ŸæœåŠ¡å™¨
mcp-server-filesystem /path/to/directory

# 2. GitæœåŠ¡å™¨
mcp-server-git

# 3. PostgreSQLæœåŠ¡å™¨
mcp-server-postgres

# 4. SlackæœåŠ¡å™¨
mcp-server-slack

# 5. Google DriveæœåŠ¡å™¨
mcp-server-gdrive
```

---

## LangGraphï¼šå¤æ‚Agentæ¡†æ¶ {#langgraphå¤æ‚agentæ¡†æ¶}

### ä¸ºä»€ä¹ˆéœ€è¦LangGraphï¼Ÿ

**ä¼ ç»ŸAgentçš„å±€é™**ï¼š
```python
# ä¼ ç»ŸReAct Agent
- çº¿æ€§æ¨ç†
- éš¾ä»¥å¤„ç†å¾ªç¯
- çŠ¶æ€ç®¡ç†ç®€å•
- éš¾ä»¥å®ç°å¤æ‚é€»è¾‘

# LangGraph
- å›¾çŠ¶çŠ¶æ€æœº
- æ”¯æŒå¾ªç¯å’Œæ¡ä»¶åˆ†æ”¯
- çµæ´»çš„çŠ¶æ€ç®¡ç†
- å¯è§†åŒ–å·¥ä½œæµ
```

### LangGraphæ ¸å¿ƒæ¦‚å¿µ

```
LangGraph = Graph + State

Graphï¼ˆå›¾ï¼‰ï¼š
  - Nodeï¼ˆèŠ‚ç‚¹ï¼‰ï¼šæ‰§è¡Œæ“ä½œçš„å‡½æ•°
  - Edgeï¼ˆè¾¹ï¼‰ï¼šèŠ‚ç‚¹ä¹‹é—´çš„è½¬æ¢
  - Conditional Edgeï¼ˆæ¡ä»¶è¾¹ï¼‰ï¼šåŸºäºæ¡ä»¶çš„åˆ†æ”¯

Stateï¼ˆçŠ¶æ€ï¼‰ï¼š
  - åœ¨èŠ‚ç‚¹ä¹‹é—´ä¼ é€’çš„æ•°æ®
  - å¯ä»¥è¢«ä»»æ„èŠ‚ç‚¹ä¿®æ”¹
  - ç±»å‹å®‰å…¨ï¼ˆTypedDictï¼‰
```

### æ„å»ºç¬¬ä¸€ä¸ªLangGraph

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict

# 1. å®šä¹‰çŠ¶æ€
class AgentState(TypedDict):
    query: str
    response: str
    needs_search: bool
    search_results: str

# 2. å®šä¹‰èŠ‚ç‚¹
def decide_search(state: AgentState) -> AgentState:
    """å†³å®šæ˜¯å¦éœ€è¦æœç´¢"""
    query = state["query"]

    # ç®€å•è§„åˆ™ï¼šå¦‚æœåŒ…å«"æœç´¢"åˆ™æœç´¢
    state["needs_search"] = "æœç´¢" in query

    return state

def search(state: AgentState) -> AgentState:
    """æ‰§è¡Œæœç´¢"""
    # å®é™…çš„æœç´¢é€»è¾‘
    state["search_results"] = f"æœç´¢'{state['query']}'çš„ç»“æœ..."
    return state

def generate_response(state: AgentState) -> AgentState:
    """ç”Ÿæˆå›å¤"""
    if state["needs_search"]:
        state["response"] = f"åŸºäºæœç´¢ç»“æœï¼š{state['search_results']}"
    else:
        state["response"] = f"ç›´æ¥å›ç­”ï¼š{state['query']}"

    return state

# 3. æ„å»ºå›¾
workflow = StateGraph(AgentState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("decide_search", decide_search)
workflow.add_node("search", search)
workflow.add_node("generate", generate_response)

# è®¾ç½®å…¥å£
workflow.set_entry_point("decide_search")

# æ·»åŠ è¾¹
workflow.add_conditional_edges(
    "decide_search",
    lambda x: "search" if x["needs_search"] else "generate",
    {
        "search": "search",
        "generate": "generate"
    }
)

workflow.add_edge("search", "generate")
workflow.add_edge("generate", END)

# 4. ç¼–è¯‘å›¾
app = workflow.compile()

# 5. è¿è¡Œ
result = app.invoke({
    "query": "æœç´¢Pythonæ•™ç¨‹",
    "response": "",
    "needs_search": False,
    "search_results": ""
})

print(result["response"])
```

### å¤æ‚ç¤ºä¾‹ï¼šå®¢æœAgent

```python
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from typing import Literal

class CustomerServiceState(TypedDict):
    user_input: str
    intent: str  # faq, complaint, tech_support
    response: str
    satisfaction: int

llm = ChatOpenAI(model="gpt-3.5-turbo")

def classify_intent(state: CustomerServiceState) -> CustomerServiceState:
    """åˆ†ç±»ç”¨æˆ·æ„å›¾"""
    prompt = f"åˆ†ç±»ä»¥ä¸‹ç”¨æˆ·æ„å›¾ï¼ˆfaq/complaint/tech_supportï¼‰ï¼š{state['user_input']}"
    response = llm.invoke(prompt)
    state["intent"] = response.content.strip().lower()
    return state

def handle_faq(state: CustomerServiceState) -> CustomerServiceState:
    """å¤„ç†FAQ"""
    response = llm.invoke(f"å›ç­”è¿™ä¸ªFAQé—®é¢˜ï¼š{state['user_input']}")
    state["response"] = response.content
    return state

def handle_complaint(state: CustomerServiceState) -> CustomerServiceState:
    """å¤„ç†æŠ•è¯‰"""
    response = llm.invoke(f"ç¤¼è²Œåœ°å¤„ç†è¿™ä¸ªæŠ•è¯‰ï¼š{state['user_input']}")
    state["response"] = response.content
    return state

def handle_tech_support(state: CustomerServiceState) -> CustomerServiceState:
    """å¤„ç†æŠ€æœ¯æ”¯æŒ"""
    response = llm.invoke(f"æä¾›æŠ€æœ¯æ”¯æŒï¼š{state['user_input']}")
    state["response"] = response.content
    return state

def route_intent(state: CustomerServiceState) -> Literal["faq", "complaint", "tech_support"]:
    """è·¯ç”±åˆ°ä¸åŒå¤„ç†æµç¨‹"""
    return state["intent"]

# æ„å»ºå›¾
workflow = StateGraph(CustomerServiceState)

workflow.add_node("classify", classify_intent)
workflow.add_node("faq", handle_faq)
workflow.add_node("complaint", handle_complaint)
workflow.add_node("tech_support", handle_tech_support)

workflow.set_entry_point("classify")

workflow.add_conditional_edges(
    "classify",
    route_intent,
    {
        "faq": "faq",
        "complaint": "complaint",
        "tech_support": "tech_support"
    }
)

workflow.add_edge("faq", END)
workflow.add_edge("complaint", END)
workflow.add_edge("tech_support", END)

# ç¼–è¯‘
app = workflow.compile()

# ä½¿ç”¨
result = app.invoke({
    "user_input": "æˆ‘çš„äº§å“æœ‰è´¨é‡é—®é¢˜",
    "intent": "",
    "response": "",
    "satisfaction": 0
})

print(result["response"])
```

### å¯è§†åŒ–LangGraph

```python
# ç”Ÿæˆå¯è§†åŒ–å›¾
from IPython.display import Image, display

try:
    display(Image(app.get_graph().draw_mermaid_png()))
except Exception:
    pass
```

### LangGraph å¸¸è§æ¨¡å¼ {#langgraph-å¸¸è§æ¨¡å¼}

#### æ¨¡å¼1ï¼šå¾ªç¯æ¨¡å¼ï¼ˆLoop Patternï¼‰

å¤„ç†éœ€è¦å¤šæ¬¡è¿­ä»£æ‰èƒ½å®Œæˆçš„ä»»åŠ¡ã€‚

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Literal

class IterationState(TypedDict):
    content: str
    feedback: str
    iteration: int
    approved: bool
    final_output: str

def generate_content(state: IterationState) -> IterationState:
    """ç”Ÿæˆå†…å®¹"""
    if state["iteration"] == 0:
        state["content"] = "åˆç¨¿å†…å®¹..."
    else:
        # æ ¹æ®åé¦ˆæ”¹è¿›
        prompt = f"åŸå†…å®¹ï¼š{state['content']}\nåé¦ˆï¼š{state['feedback']}\nè¯·æ”¹è¿›"
        state["content"] = llm.invoke(prompt).content

    state["iteration"] += 1
    return state

def review_content(state: IterationState) -> IterationState:
    """è¯„å®¡å†…å®¹"""
    prompt = f"""
    è¯„å®¡ä»¥ä¸‹å†…å®¹ï¼ˆæœ€å¤šè¿­ä»£{state['iteration']}æ¬¡ï¼‰ï¼š
    {state['content']}

    å¦‚æœæ»¡æ„ï¼Œå›å¤"APPROVED"ã€‚
    å¦‚æœéœ€è¦æ”¹è¿›ï¼Œæä¾›å…·ä½“å»ºè®®ã€‚
    """

    response = llm.invoke(prompt).content

    if "APPROVED" in response:
        state["approved"] = True
        state["final_output"] = state["content"]
    else:
        state["feedback"] = response
        state["approved"] = False

    return state

def should_continue(state: IterationState) -> Literal["continue", "end"]:
    """å†³å®šæ˜¯å¦ç»§ç»­è¿­ä»£"""
    if state["approved"]:
        return "end"
    if state["iteration"] >= 3:  # æœ€å¤šè¿­ä»£3æ¬¡
        state["final_output"] = state["content"]
        return "end"
    return "continue"

# æ„å»ºå¾ªç¯å›¾
workflow = StateGraph(IterationState)
workflow.add_node("generate", generate_content)
workflow.add_node("review", review_content)

workflow.set_entry_point("generate")
workflow.add_edge("generate", "review")

workflow.add_conditional_edges(
    "review",
    should_continue,
    {
        "continue": "generate",  # å¾ªç¯å› generate
        "end": END
    }
)

app = workflow.compile()

# è¿è¡Œ
result = app.invoke({
    "content": "",
    "feedback": "",
    "iteration": 0,
    "approved": False,
    "final_output": ""
})

print(f"æœ€ç»ˆè¾“å‡ºï¼š{result['final_output']}")
print(f"è¿­ä»£æ¬¡æ•°ï¼š{result['iteration']}")
```

#### æ¨¡å¼2ï¼šå¹¶è¡Œæ¨¡å¼ï¼ˆParallel Patternï¼‰

å¤šä¸ªä»»åŠ¡å¹¶è¡Œæ‰§è¡Œï¼Œç„¶åèšåˆç»“æœã€‚

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, List
import asyncio

class ParallelState(TypedDict):
    topic: str
    technical_analysis: str
    market_analysis: str
    user_analysis: str
    final_report: str

async def technical_analysis(state: ParallelState) -> ParallelState:
    """æŠ€æœ¯åˆ†æ"""
    prompt = f"ä»æŠ€æœ¯è§’åº¦åˆ†æï¼š{state['topic']}"
    # æ¨¡æ‹Ÿå¼‚æ­¥æ“ä½œ
    await asyncio.sleep(1)
    state["technical_analysis"] = llm.invoke(prompt).content
    return state

async def market_analysis(state: ParallelState) -> ParallelState:
    """å¸‚åœºåˆ†æ"""
    prompt = f"ä»å¸‚åœºè§’åº¦åˆ†æï¼š{state['topic']}"
    await asyncio.sleep(1)
    state["market_analysis"] = llm.invoke(prompt).content
    return state

async def user_analysis(state: ParallelState) -> ParallelState:
    """ç”¨æˆ·åˆ†æ"""
    prompt = f"ä»ç”¨æˆ·è§’åº¦åˆ†æï¼š{state['topic']}"
    await asyncio.sleep(1)
    state["user_analysis"] = llm.invoke(prompt).content
    return state

def synthesize(state: ParallelState) -> ParallelState:
    """ç»¼åˆåˆ†æç»“æœ"""
    prompt = f"""
    ç»¼åˆä»¥ä¸‹ä¸‰ä¸ªåˆ†æè§’åº¦ï¼Œç”Ÿæˆå®Œæ•´æŠ¥å‘Šï¼š

    æŠ€æœ¯è§’åº¦ï¼š{state['technical_analysis']}

    å¸‚åœºè§’åº¦ï¼š{state['market_analysis']}

    ç”¨æˆ·è§’åº¦ï¼š{state['user_analysis']}

    è¯·æä¾›ï¼š
    1. ç»¼åˆè¯„ä¼°
    2. æœºä¼šä¸é£é™©
    3. å»ºè®®è¡ŒåŠ¨
    """

    state["final_report"] = llm.invoke(prompt).content
    return state

# æ³¨æ„ï¼šLangGraph æœ¬èº«æ˜¯ä¸²è¡Œçš„ï¼ŒçœŸæ­£çš„å¹¶è¡Œéœ€è¦åœ¨èŠ‚ç‚¹å†…å®ç°
# æˆ–è€…ä½¿ç”¨ astream events å’Œå¼‚æ­¥è°ƒç”¨

def parallel_analysis_node(state: ParallelState) -> ParallelState:
    """åœ¨èŠ‚ç‚¹å†…å®ç°å¹¶è¡Œ"""
    async def _parallel():
        results = await asyncio.gather(
            technical_analysis(state.copy()),
            market_analysis(state.copy()),
            user_analysis(state.copy())
        )
        return results

    # è¿è¡Œå¹¶è¡Œä»»åŠ¡
    results = asyncio.run(_parallel())

    state["technical_analysis"] = results[0]["technical_analysis"]
    state["market_analysis"] = results[1]["market_analysis"]
    state["user_analysis"] = results[2]["user_analysis"]

    return state

# æ„å»ºå›¾
workflow = StateGraph(ParallelState)
workflow.add_node("parallel_analysis", parallel_analysis_node)
workflow.add_node("synthesize", synthesize)

workflow.set_entry_point("parallel_analysis")
workflow.add_edge("parallel_analysis", "synthesize")
workflow.add_edge("synthesize", END)

app = workflow.compile()

result = app.invoke({
    "topic": "å¼€å‘AIç¼–ç¨‹åŠ©æ‰‹",
    "technical_analysis": "",
    "market_analysis": "",
    "user_analysis": "",
    "final_report": ""
})

print(result["final_report"])
```

#### æ¨¡å¼3ï¼šåˆ†æ”¯èšåˆæ¨¡å¼ï¼ˆFork-Join Patternï¼‰

å…ˆåˆ†æ”¯å¤„ç†ï¼Œå†åˆå¹¶ç»“æœã€‚

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Literal

class ForkJoinState(TypedDict):
    document: str
    grammar_errors: list
    style_issues: list
    factual_errors: list
    combined_feedback: str
    revised_document: str

def grammar_check(state: ForkJoinState) -> ForkJoinState:
    """è¯­æ³•æ£€æŸ¥"""
    prompt = f"æ£€æŸ¥è¯­æ³•é”™è¯¯ï¼š{state['document']}"
    response = llm.invoke(prompt)
    state["grammar_errors"] = ["è¯­æ³•é”™è¯¯1", "è¯­æ³•é”™è¯¯2"]  # å®é™…ä» response è§£æ
    return state

def style_check(state: ForkJoinState) -> ForkJoinState:
    """é£æ ¼æ£€æŸ¥"""
    prompt = f"æ£€æŸ¥é£æ ¼é—®é¢˜ï¼š{state['document']}"
    response = llm.invoke(prompt)
    state["style_issues"] = ["é£æ ¼é—®é¢˜1", "é£æ ¼é—®é¢˜2"]
    return state

def fact_check(state: ForkJoinState) -> ForkJoinState:
    """äº‹å®æ£€æŸ¥"""
    prompt = f"æ£€æŸ¥äº‹å®é”™è¯¯ï¼š{state['document']}"
    response = llm.invoke(prompt)
    state["factual_errors"] = ["äº‹å®é”™è¯¯1"]
    return state

def aggregate_feedback(state: ForkJoinState) -> ForkJoinState:
    """èšåˆæ‰€æœ‰åé¦ˆ"""
    all_issues = []
    all_issues.extend(state["grammar_errors"])
    all_issues.extend(state["style_issues"])
    all_issues.extend(state["factual_errors"])

    state["combined_feedback"] = "\n".join(all_issues)
    return state

def revise_document(state: ForkJoinState) -> ForkJoinState:
    """æ ¹æ®åé¦ˆä¿®è®¢æ–‡æ¡£"""
    prompt = f"""
    åŸæ–‡æ¡£ï¼š
    {state['document']}

    åé¦ˆï¼š
    {state['combined_feedback']}

    è¯·æ ¹æ®åé¦ˆä¿®è®¢æ–‡æ¡£ã€‚
    """

    state["revised_document"] = llm.invoke(prompt).content
    return state

def check_quality(state: ForkJoinState) -> Literal["revise", "finish"]:
    """æ£€æŸ¥ä¿®è®¢åçš„è´¨é‡"""
    # ç®€åŒ–ç‰ˆï¼šå¦‚æœæœ‰é”™è¯¯å°±ç»§ç»­ä¿®è®¢
    if len(state["grammar_errors"]) > 0 or len(state["factual_errors"]) > 0:
        return "revise"
    return "finish"

# æ„å»ºå›¾
workflow = StateGraph(ForkJoinState)

# æ·»åŠ åˆ†æ”¯èŠ‚ç‚¹
workflow.add_node("grammar_check", grammar_check)
workflow.add_node("style_check", style_check)
workflow.add_node("fact_check", fact_check)

# æ·»åŠ èšåˆèŠ‚ç‚¹
workflow.add_node("aggregate", aggregate_feedback)
workflow.add_node("revise", revise_document)

# å…¥å£ç‚¹ï¼ˆé€‰æ‹©ä¸€ä¸ªåˆ†æ”¯èµ·ç‚¹ï¼‰
workflow.set_entry_point("grammar_check")

# æ·»åŠ åˆ†æ”¯è¾¹ï¼ˆæ¯ä¸ªæ£€æŸ¥åéƒ½åˆ°èšåˆï¼‰
workflow.add_edge("grammar_check", "aggregate")
workflow.add_edge("style_check", "aggregate")
workflow.add_edge("fact_check", "aggregate")

# æ³¨æ„ï¼šè¿™ä¸ªç®€åŒ–ç‰ˆæœ¬æ²¡æœ‰çœŸæ­£çš„å¹¶è¡Œ
# å®é™…éœ€è¦ä½¿ç”¨ Send é¢å¤–è§¦å‘å…¶ä»–åˆ†æ”¯

workflow.add_edge("aggregate", "revise")

workflow.add_conditional_edges(
    "revise",
    check_quality,
    {
        "revise": "grammar_check",  # é‡æ–°æ£€æŸ¥
        "finish": END
    }
)

app = workflow.compile()
```

#### æ¨¡å¼4ï¼šä»£ç†åè°ƒæ¨¡å¼ï¼ˆAgent Coordinationï¼‰

å¤šä¸ª Agent åä½œå®Œæˆå¤æ‚ä»»åŠ¡ã€‚

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, Literal

class MultiAgentState(TypedDict):
    task: str
    researcher_output: str
    writer_output: str
    reviewer_output: str
    final_output: str
    current_agent: str

def researcher_agent(state: MultiAgentState) -> MultiAgentState:
    """ç ”ç©¶ Agentï¼šæ”¶é›†ä¿¡æ¯"""
    prompt = f"ç ”ç©¶ä»»åŠ¡ï¼š{state['task']}\n\næ”¶é›†ç›¸å…³ä¿¡æ¯å’Œæ•°æ®ã€‚"
    state["researcher_output"] = llm.invoke(prompt).content
    state["current_agent"] = "researcher"
    return state

def writer_agent(state: MultiAgentState) -> MultiAgentState:
    """å†™ä½œ Agentï¼šåŸºäºç ”ç©¶å†…å®¹æ’°å†™"""
    prompt = f"""
    åŸºäºä»¥ä¸‹ç ”ç©¶å†…å®¹æ’°å†™æ–‡ç« ï¼š

    ç ”ç©¶å†…å®¹ï¼š
    {state['researcher_output']}

    ä»»åŠ¡ï¼š{state['task']}
    """
    state["writer_output"] = llm.invoke(prompt).content
    state["current_agent"] = "writer"
    return state

def reviewer_agent(state: MultiAgentState) -> MultiAgentState:
    """è¯„å®¡ Agentï¼šå®¡æ ¸å¹¶ç»™å‡ºåé¦ˆ"""
    prompt = f"""
    è¯„å®¡ä»¥ä¸‹æ–‡ç« ï¼š

    {state['writer_output']}

    ç»™å‡ºè¯„åˆ†ï¼ˆ1-10ï¼‰å’Œæ”¹è¿›å»ºè®®ã€‚
    å¦‚æœä½äº8åˆ†ï¼Œæä¾›å…·ä½“ä¿®æ”¹æ„è§ã€‚
    """
    state["reviewer_output"] = llm.invoke(prompt).content
    state["current_agent"] = "reviewer"
    return state

def should_revise(state: MultiAgentState) -> Literal["revise", "finish"]:
    """å†³å®šæ˜¯å¦éœ€è¦ä¿®è®¢"""
    review = state["reviewer_output"]
    # ç®€åŒ–åˆ¤æ–­
    if "8" in review or "9" in review or "10" in review:
        return "finish"
    return "revise"

def revise_agent(state: MultiAgentState) -> MultiAgentState:
    """ä¿®è®¢ Agentï¼šæ ¹æ®è¯„å®¡æ„è§ä¿®æ”¹"""
    prompt = f"""
    åŸæ–‡ç« ï¼š
    {state['writer_output']}

    è¯„å®¡æ„è§ï¼š
    {state['reviewer_output']}

    è¯·æ ¹æ®æ„è§ä¿®æ”¹æ–‡ç« ã€‚
    """

    revised = llm.invoke(prompt).content
    state["writer_output"] = revised
    state["final_output"] = revised
    return state

# æ„å»ºå¤š Agent å·¥ä½œæµ
workflow = StateGraph(MultiAgentState)

workflow.add_node("researcher", researcher_agent)
workflow.add_node("writer", writer_agent)
workflow.add_node("reviewer", reviewer_agent)
workflow.add_node("revise", revise_agent)

workflow.set_entry_point("researcher")

# é¡ºåºæ‰§è¡Œï¼šresearcher â†’ writer â†’ reviewer
workflow.add_edge("researcher", "writer")
workflow.add_edge("writer", "reviewer")

# æ¡ä»¶åˆ†æ”¯ï¼šå¦‚æœéœ€è¦ä¿®è®¢åˆ™å›åˆ° writer
workflow.add_conditional_edges(
    "reviewer",
    should_revise,
    {
        "revise": "revise",
        "finish": END
    }
)

# ä¿®è®¢åé‡æ–°è¯„å®¡
workflow.add_edge("revise", "reviewer")

app = workflow.compile()

# è¿è¡Œ
result = app.invoke({
    "task": "å†™ä¸€ç¯‡å…³äºAIæœªæ¥çš„æ–‡ç« ",
    "researcher_output": "",
    "writer_output": "",
    "reviewer_output": "",
    "final_output": "",
    "current_agent": ""
})

print(result["final_output"])
```

### LangGraph vs Prompt Chaining å¯¹æ¯” {#langgraph-vs-prompt-chaining-å¯¹æ¯”}

| ç‰¹æ€§ | LangGraph | Prompt Chaining |
|------|-----------|----------------|
| **å¤æ‚åº¦** | é«˜ï¼Œæ”¯æŒå¤æ‚çŠ¶æ€æœº | ä¸­ï¼Œçº¿æ€§æˆ–ç®€å•åˆ†æ”¯ |
| **çµæ´»æ€§** | éå¸¸çµæ´»ï¼Œæ”¯æŒå¾ªç¯ã€æ¡ä»¶åˆ†æ”¯ | ç›¸å¯¹å›ºå®šï¼Œä¸»è¦æ˜¯é¡ºåºæ‰§è¡Œ |
| **çŠ¶æ€ç®¡ç†** | å†…ç½®çŠ¶æ€ç®¡ç†ï¼Œåœ¨èŠ‚ç‚¹é—´ä¼ é€’ | éœ€è¦æ‰‹åŠ¨ä¼ é€’ä¸Šä¸‹æ–‡ |
| **å¯è§†åŒ–** | æ”¯æŒç”Ÿæˆæµç¨‹å›¾ | æ— å†…ç½®å¯è§†åŒ– |
| **è°ƒè¯•** | å¯ä»¥è¿½è¸ªæ¯ä¸€æ­¥çš„çŠ¶æ€å˜åŒ– | éœ€è¦æ‰‹åŠ¨æ‰“å°ä¸­é—´ç»“æœ |
| **å­¦ä¹ æ›²çº¿** | é™¡å³­ï¼Œéœ€è¦ç†è§£å›¾æ¦‚å¿µ | å¹³ç¼“ï¼Œå®¹æ˜“ä¸Šæ‰‹ |
| **é€‚ç”¨åœºæ™¯** | å¤æ‚ Agent ç³»ç»Ÿã€å¤šæ­¥å†³ç­– | ç®€å•å¤šæ­¥ä»»åŠ¡ã€å†…å®¹ç”Ÿæˆæµæ°´çº¿ |

### LangGraph æœ€ä½³å®è·µ

#### âœ… DOï¼ˆæ¨èåšæ³•ï¼‰

1. **æ˜ç¡®å®šä¹‰çŠ¶æ€ç»“æ„**
```python
from typing import TypedDict

class AgentState(TypedDict):
    # æ˜ç¡®æ¯ä¸ªå­—æ®µçš„ç±»å‹
    query: str
    search_results: List[str]
    answer: str
    confidence: float
    iteration_count: int
```

2. **ä¿æŒèŠ‚ç‚¹å‡½æ•°ç®€å•**
```python
# å¥½çš„åšæ³•ï¼šæ¯ä¸ªèŠ‚ç‚¹åªåšä¸€ä»¶äº‹
def search_node(state: AgentState) -> AgentState:
    """åªè´Ÿè´£æœç´¢"""
    state["search_results"] = search_api(state["query"])
    return state

def rank_node(state: AgentState) -> AgentState:
    """åªè´Ÿè´£æ’åº"""
    state["search_results"] = rank_results(state["search_results"])
    return state
```

3. **ä½¿ç”¨æ¡ä»¶è¾¹å®ç°å¤æ‚é€»è¾‘**
```python
def route_condition(state: AgentState) -> Literal["a", "b", "c"]:
    """æ¸…æ™°çš„è·¯ç”±é€»è¾‘"""
    score = state["confidence"]

    if score > 0.9:
        return "a"  # é«˜ç½®ä¿¡åº¦ç›´æ¥è¾“å‡º
    elif score > 0.5:
        return "b"  # ä¸­ç­‰ç½®ä¿¡åº¦éœ€è¦éªŒè¯
    else:
        return "c"  # ä½ç½®ä¿¡åº¦é‡æ–°æœç´¢
```

4. **æ·»åŠ é”™è¯¯å¤„ç†**
```python
def safe_node(state: AgentState) -> AgentState:
    """å¸¦é”™è¯¯å¤„ç†çš„èŠ‚ç‚¹"""
    try:
        result = risky_operation(state)
        state["result"] = result
        state["error"] = None
    except Exception as e:
        state["error"] = str(e)
        state["retry_count"] = state.get("retry_count", 0) + 1

    return state
```

5. **è®°å½•ä¸­é—´ç»“æœ**
```python
from langgraph.checkpoint.memory import MemorySaver

# æ·»åŠ æ£€æŸ¥ç‚¹ï¼Œå¯ä»¥ä¿å­˜å’Œæ¢å¤çŠ¶æ€
memory = MemorySaver()
app = workflow.compile(checkpointer=memory)

# è¿è¡Œæ—¶å¯ä»¥æŒ‡å®š thread_id
config = {"configurable": {"thread_id": "session-123"}}
result = app.invoke(initial_state, config=config)

# å¯ä»¥æŸ¥çœ‹å†å²çŠ¶æ€
for state in app.get_state_history(config):
    print(state)
```

#### âŒ DON'Tï¼ˆé¿å…åšæ³•ï¼‰

1. **ä¸è¦åœ¨èŠ‚ç‚¹ä¸­æ‰§è¡Œé•¿æ—¶é—´ä»»åŠ¡**
```python
# ä¸å¥½çš„åšæ³•ï¼šåœ¨èŠ‚ç‚¹ä¸­ä¸‹è½½å¤§æ–‡ä»¶
def download_node(state: AgentState) -> AgentState:
    # å¯èƒ½ä¼šé˜»å¡å¾ˆé•¿æ—¶é—´
    large_file = download_huge_file()
    return state

# å¥½çš„åšæ³•ï¼šè¿”å›ä»»åŠ¡IDï¼Œå¼‚æ­¥å¤„ç†
def initiate_download(state: AgentState) -> AgentState:
    task_id = start_async_download(state["url"])
    state["task_id"] = task_id
    state["status"] = "downloading"
    return state
```

2. **ä¸è¦åœ¨èŠ‚ç‚¹ä¸­ç›´æ¥ä¿®æ”¹å¤–éƒ¨çŠ¶æ€**
```python
# ä¸å¥½çš„åšæ³•ï¼šç›´æ¥å†™æ•°æ®åº“
def save_node(state: AgentState) -> AgentState:
    database.save(state["data"])  # å‰¯ä½œç”¨
    return state

# å¥½çš„åšæ³•ï¼šåœ¨ä¸“é—¨çš„èŠ‚ç‚¹ä¸­å¤„ç†
def prepare_data_node(state: AgentState) -> AgentState:
    state["prepared_data"] = prepare_for_db(state["data"])
    return state

def save_node(state: AgentState) -> AgentState:
    # è¿™ä¸ªèŠ‚ç‚¹å”¯ä¸€çš„ä½œç”¨å°±æ˜¯ä¿å­˜
    database.save(state["prepared_data"])
    return state
```

3. **ä¸è¦åˆ›å»ºè¿‡å¤§çš„çŠ¶æ€å¯¹è±¡**
```python
# ä¸å¥½çš„åšæ³•ï¼šçŠ¶æ€åŒ…å«å¤§é‡æ•°æ®
class AgentState(TypedDict):
    entire_document: str  # å¯èƒ½å¾ˆé•¿
    all_search_results: List[str]  # å¯èƒ½æœ‰å‡ ç™¾æ¡
    complete_history: List[dict]  # å®Œæ•´å†å²è®°å½•

# å¥½çš„åšæ³•ï¼šåªä¿å­˜å¿…è¦ä¿¡æ¯
class AgentState(TypedDict):
    document_id: str  # ç”¨IDå¼•ç”¨
    top_k_results: List[str]  # åªä¿ç•™å‰Kä¸ªç»“æœ
    current_step: int  # å½“å‰æ­¥éª¤
    summary: str  # ç®€è¦æ€»ç»“
```

### å®æˆ˜é¡¹ç›®ï¼šæ™ºèƒ½å†…å®¹ç”Ÿæˆç³»ç»Ÿ {#å®æˆ˜é¡¹ç›®æ™ºèƒ½å†…å®¹ç”Ÿæˆç³»ç»Ÿ}

ç»“åˆ LangGraph å’Œ Prompt Chaining æ„å»ºå®Œæ•´ç³»ç»Ÿã€‚

```python
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from typing import TypedDict, Literal
import operator
from typing import Annotated

# 1. å®šä¹‰çŠ¶æ€
class ContentGenState(TypedDict):
    topic: str
    target_audience: str
    content_type: str  # blog, tutorial, guide
    research_data: str
    outline: str
    draft: str
    feedback: str
    final_content: str
    quality_score: float
    iteration: int

# 2. å®šä¹‰å·¥å…·å‡½æ•°
def web_search(query: str) -> str:
    """æ¨¡æ‹Ÿç½‘ç»œæœç´¢"""
    return f"å…³äº'{query}'çš„æœç´¢ç»“æœï¼š..."

def seo_analysis(content: str) -> dict:
    """SEO åˆ†æ"""
    return {
        "score": 0.75,
        "keywords": ["AI", "LangGraph", "Python"],
        "suggestions": ["æ·»åŠ æ›´å¤šç¤ºä¾‹", "ä¼˜åŒ–æ ‡é¢˜"]
    }

# 3. å®šä¹‰èŠ‚ç‚¹
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.7)

def research(state: ContentGenState) -> ContentGenState:
    """ç ”ç©¶é˜¶æ®µï¼šæ”¶é›†ä¿¡æ¯"""
    # ä½¿ç”¨ Prompt Chainingï¼šåˆ†æ­¥ç ”ç©¶
    search_prompt = f"æœç´¢å…³äº'{state['topic']}'çš„æœ€æ–°ä¿¡æ¯"
    search_results = web_search(search_prompt)

    analyze_prompt = f"""
    åˆ†æä»¥ä¸‹æœç´¢ç»“æœï¼Œæå–å…³é”®ä¿¡æ¯ï¼š
    {search_results}

    é’ˆå¯¹{state['target_audience']}å—ä¼—ã€‚
    """

    analysis = llm.invoke(analyze_prompt).content
    state["research_data"] = analysis
    return state

def outline(state: ContentGenState) -> ContentGenState:
    """å¤§çº²é˜¶æ®µï¼šåˆ›å»ºç»“æ„"""
    prompt = f"""
    åŸºäº{state['content_type']}æ ¼å¼ï¼Œåˆ›å»ºå¤§çº²ï¼š

    ä¸»é¢˜ï¼š{state['topic']}
    ç ”ç©¶æ•°æ®ï¼š{state['research_data']}

    å¤§çº²åº”åŒ…å«ï¼š
    - å¼•è¨€
    - ä¸»è¦ç« èŠ‚ï¼ˆ3-5ä¸ªï¼‰
    - æ¯ç« èŠ‚çš„å…³é”®ç‚¹
    - ç»“è®º
    """

    state["outline"] = llm.invoke(prompt).content
    return state

def draft_content(state: ContentGenState) -> ContentGenState:
    """èµ·è‰é˜¶æ®µï¼šæ’°å†™å†…å®¹"""
    prompt = f"""
    åŸºäºä»¥ä¸‹å¤§çº²æ’°å†™å†…å®¹ï¼š

    {state['outline']}

    è¦æ±‚ï¼š
    - é’ˆå¯¹{state['target_audience']}
    - ä¸“ä¸šä¸”æ˜“æ‡‚
    - åŒ…å«ä»£ç ç¤ºä¾‹ï¼ˆå¦‚é€‚ç”¨ï¼‰
    - 1000-1500å­—
    """

    state["draft"] = llm.invoke(prompt).content
    return state

def quality_check(state: ContentGenState) -> ContentGenState:
    """è´¨é‡æ£€æŸ¥"""
    # SEO åˆ†æ
    seo_result = seo_analysis(state["draft"])
    state["quality_score"] = seo_result["score"]

    # AI è¯„å®¡
    review_prompt = f"""
    è¯„å®¡ä»¥ä¸‹å†…å®¹è´¨é‡ï¼š

    {state['draft']}

    ä»ä»¥ä¸‹æ–¹é¢è¯„åˆ†ï¼ˆ1-10ï¼‰ï¼š
    1. å†…å®¹å‡†ç¡®æ€§
    2. ç»“æ„å®Œæ•´æ€§
    3. è¯­è¨€æµç•…æ€§
    4. SEO ä¼˜åŒ–
    5. å—ä¼—é€‚é…åº¦

    ç»™å‡ºæ€»åˆ†å’Œæ”¹è¿›å»ºè®®ã€‚
    """

    review = llm.invoke(review_prompt).content
    state["feedback"] = review
    return state

def should_improve(state: ContentGenState) -> Literal["improve", "finish"]:
    """å†³å®šæ˜¯å¦éœ€è¦æ”¹è¿›"""
    state["iteration"] += 1

    if state["quality_score"] >= 0.8:
        return "finish"
    if state["iteration"] >= 2:  # æœ€å¤šè¿­ä»£2æ¬¡
        return "finish"
    return "improve"

def improve_content(state: ContentGenState) -> ContentGenState:
    """æ”¹è¿›å†…å®¹"""
    prompt = f"""
    åŸå†…å®¹ï¼š
    {state['draft']}

    åé¦ˆï¼š
    {state['feedback']}

    è¯·æ ¹æ®åé¦ˆæ”¹è¿›å†…å®¹ã€‚
    """

    improved = llm.invoke(prompt).content
    state["draft"] = improved
    return state

def finalize(state: ContentGenState) -> ContentGenState:
    """æœ€ç»ˆå¤„ç†"""
    state["final_content"] = state["draft"]
    return state

# 4. æ„å»ºå›¾
workflow = StateGraph(ContentGenState)

workflow.add_node("research", research)
workflow.add_node("outline", outline)
workflow.add_node("draft", draft_content)
workflow.add_node("quality_check", quality_check)
workflow.add_node("improve", improve_content)
workflow.add_node("finalize", finalize)

workflow.set_entry_point("research")

# ä¸»æµç¨‹
workflow.add_edge("research", "outline")
workflow.add_edge("outline", "draft")
workflow.add_edge("draft", "quality_check")

# æ¡ä»¶åˆ†æ”¯ï¼šè´¨é‡æ£€æŸ¥åå†³å®šæ˜¯å¦æ”¹è¿›
workflow.add_conditional_edges(
    "quality_check",
    should_improve,
    {
        "improve": "improve",
        "finish": "finalize"
    }
)

# æ”¹è¿›åé‡æ–°æ£€æŸ¥
workflow.add_edge("improve", "quality_check")
workflow.add_edge("finalize", END)

# 5. ç¼–è¯‘å¹¶è¿è¡Œ
app = workflow.compile()

result = app.invoke({
    "topic": "ä½¿ç”¨ LangGraph æ„å»º AI Agent",
    "target_audience": "Python å¼€å‘è€…",
    "content_type": "tutorial",
    "research_data": "",
    "outline": "",
    "draft": "",
    "feedback": "",
    "final_content": "",
    "quality_score": 0.0,
    "iteration": 0
})

print("=== æœ€ç»ˆç”Ÿæˆçš„å†…å®¹ ===")
print(result["final_content"])
print(f"\nè´¨é‡è¯„åˆ†ï¼š{result['quality_score']}")
print(f"è¿­ä»£æ¬¡æ•°ï¼š{result['iteration']}")
```

### å°ç»“

**LangGraph æ ¸å¿ƒè¦ç‚¹**ï¼š
- ğŸ¯ **å›¾çŠ¶æ€ç»´**ï¼šå°†å¤æ‚æµç¨‹å»ºæ¨¡ä¸ºçŠ¶æ€å›¾
- ğŸ”„ **çŠ¶æ€ç®¡ç†**ï¼šæ˜ç¡®å®šä¹‰åœ¨èŠ‚ç‚¹é—´ä¼ é€’çš„æ•°æ®ç»“æ„
- ğŸ”€ **æ¡ä»¶åˆ†æ”¯**ï¼šä½¿ç”¨ conditional_edges å®ç°å¤æ‚é€»è¾‘
- ğŸ“Š **å¯è§†åŒ–**ï¼šåˆ©ç”¨ graph.draw_mermaid_png() å¯è§†åŒ–æµç¨‹
- ğŸ’¾ **æ£€æŸ¥ç‚¹**ï¼šä½¿ç”¨ checkpointer ä¿å­˜ä¸­é—´çŠ¶æ€
- ğŸ› ï¸ **æœ€ä½³å®è·µ**ï¼šä¿æŒèŠ‚ç‚¹ç®€å•ã€æ˜ç¡®çŠ¶æ€ç»“æ„ã€å¤„ç†é”™è¯¯

---

## AIåº”ç”¨è¯„ä¼°å’Œæµ‹è¯• {#aiåº”ç”¨è¯„ä¼°å’Œæµ‹è¯•}

### è¯„ä¼°ç»´åº¦

```
AIåº”ç”¨è¯„ä¼°æ¡†æ¶ï¼š

1. å‡†ç¡®æ€§ï¼ˆAccuracyï¼‰
   - ç­”æ¡ˆæ­£ç¡®ç‡
   - äº‹å®ä¸€è‡´æ€§
   - è®¡ç®—å‡†ç¡®æ€§

2. ç›¸å…³æ€§ï¼ˆRelevanceï¼‰
   - å›ç­”æ˜¯å¦åˆ‡é¢˜
   - æ˜¯å¦æ»¡è¶³ç”¨æˆ·éœ€æ±‚
   - ä¿¡æ¯å®Œæ•´åº¦

3. è´¨é‡ï¼ˆQualityï¼‰
   - è¯­è¨€æµç•…åº¦
   - é€»è¾‘è¿è´¯æ€§
   - å¯è¯»æ€§

4. å®‰å…¨æ€§ï¼ˆSafetyï¼‰
   - æœ‰å®³å†…å®¹è¿‡æ»¤
   - åè§æ£€æµ‹
   - éšç§ä¿æŠ¤

5. æ€§èƒ½ï¼ˆPerformanceï¼‰
   - å“åº”æ—¶é—´
   - ååé‡
   - èµ„æºæ¶ˆè€—

6. æˆæœ¬ï¼ˆCostï¼‰
   - Tokenæ¶ˆè€—
   - APIè´¹ç”¨
   - åŸºç¡€è®¾æ–½æˆæœ¬
```

### RAGè¯„ä¼°æ¡†æ¶

**ä½¿ç”¨Ragas**ï¼š

```bash
pip install ragas
```

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_relevancy,
    context_recall
)
from datasets import Dataset

# å‡†å¤‡æµ‹è¯•æ•°æ®
test_data = {
    "question": [
        "ä»€ä¹ˆæ˜¯Pythonè£…é¥°å™¨ï¼Ÿ",
        "å¦‚ä½•ä½¿ç”¨FastAPIï¼Ÿ"
    ],
    "answer": [
        "è£…é¥°å™¨æ˜¯Pythonçš„ä¸€ç§è®¾è®¡æ¨¡å¼...",
        "ä½¿ç”¨FastAPIéœ€è¦å…ˆå®‰è£…..."
    ],
    "contexts": [
        ["Pythonè£…é¥°å™¨æ˜¯ä¸€ç§...", "è£…é¥°å™¨å¯ä»¥ç”¨æ¥..."],
        ["FastAPIæ˜¯ç°ä»£Webæ¡†æ¶...", "å®‰è£…å‘½ä»¤æ˜¯..."]
    ],
    "ground_truth": [
        "è£…é¥°å™¨æ˜¯ä¸€ç§ä¿®æ”¹å‡½æ•°è¡Œä¸ºçš„å·¥å…·",
        "éœ€è¦ä½¿ç”¨pip install fastapiå®‰è£…"
    ]
}

dataset = Dataset.from_dict(test_data)

# è¯„ä¼°
result = evaluate(
    dataset=dataset,
    metrics=[
        faithfulness,       # å¿ å®åº¦
        answer_relevancy,   # ç­”æ¡ˆç›¸å…³æ€§
        context_relevancy,  # ä¸Šä¸‹æ–‡ç›¸å…³æ€§
        context_recall      # ä¸Šä¸‹æ–‡å¬å›ç‡
    ]
)

# æŸ¥çœ‹ç»“æœ
print(result)
# Output:
# {'faithfulness': 0.85, 'answer_relevancy': 0.92, ...}

# è½¬æ¢ä¸ºDataFrame
df = result.to_pandas()
print(df)
```

### è‡ªå®šä¹‰è¯„ä¼°

```python
class RAGEvaluator:
    """RAGç³»ç»Ÿè¯„ä¼°å™¨"""

    def __init__(self, rag_system, test_questions: list):
        self.rag = rag_system
        self.test_questions = test_questions
        self.llm = ChatOpenAI(model="gpt-3.5-turbo")

    def evaluate_accuracy(self, question: str, ground_truth: str) -> float:
        """è¯„ä¼°å‡†ç¡®æ€§"""
        response = self.rag.ask(question)
        answer = response["answer"]

        # ä½¿ç”¨LLMè¯„åˆ†
        prompt = f"""
        è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆçš„è´¨é‡ï¼ˆ0-10åˆ†ï¼‰ï¼š

        é—®é¢˜ï¼š{question}
        æ ‡å‡†ç­”æ¡ˆï¼š{ground_truth}
        å®é™…ç­”æ¡ˆï¼š{answer}

        è¯„åˆ†æ ‡å‡†ï¼š
        - å‡†ç¡®æ€§ï¼šæ˜¯å¦ä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´
        - å®Œæ•´æ€§ï¼šæ˜¯å¦åŒ…å«æ‰€æœ‰å…³é”®ä¿¡æ¯
        - æ­£ç¡®æ€§ï¼šæ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯

        åªè¿”å›0-10ä¹‹é—´çš„æ•°å­—åˆ†æ•°ã€‚
        """

        score = self.llm.invoke(prompt)
        return float(score.content) / 10

    def evaluate_retrieval(self, question: str, top_k: int = 3) -> dict:
        """è¯„ä¼°æ£€ç´¢è´¨é‡"""
        docs = self.rag.vectorstore.similarity_search(question, k=top_k)

        return {
            "retrieved_count": len(docs),
            "avg_relevance": self._avg_relevance(question, docs)
        }

    def _avg_relevance(self, question: str, docs: list) -> float:
        """è®¡ç®—å¹³å‡ç›¸å…³æ€§"""
        scores = []
        for doc in docs:
            prompt = f"""
            è¯„ä¼°æ–‡æ¡£ä¸é—®é¢˜çš„ç›¸å…³æ€§ï¼ˆ0-1ï¼‰ï¼š

            é—®é¢˜ï¼š{question}
            æ–‡æ¡£ï¼š{doc.page_content[:200]}

            åªè¿”å›0-1ä¹‹é—´çš„æ•°å­—ã€‚
            """
            score = self.llm.invoke(prompt)
            scores.append(float(score.content))

        return sum(scores) / len(scores)

    def run_evaluation(self) -> dict:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        results = {
            "total_questions": len(self.test_questions),
            "evaluated": 0,
            "avg_accuracy": 0,
            "avg_retrieval_relevance": 0
        }

        accuracy_scores = []
        relevance_scores = []

        for item in self.test_questions:
            acc = self.evaluate_accuracy(
                item["question"],
                item["ground_truth"]
            )
            acc_score = acc

            ret = self.evaluate_retrieval(item["question"])
            rel_score = ret["avg_relevance"]

            accuracy_scores.append(acc_score)
            relevance_scores.append(rel_score)

            results["evaluated"] += 1

        results["avg_accuracy"] = sum(accuracy_scores) / len(accuracy_scores)
        results["avg_retrieval_relevance"] = sum(relevance_scores) / len(relevance_scores)

        return results

# ä½¿ç”¨
evaluator = RAGEvaluator(rag_system, [
    {
        "question": "ä»€ä¹ˆæ˜¯Pythonè£…é¥°å™¨ï¼Ÿ",
        "ground_truth": "è£…é¥°å™¨æ˜¯ä¸€ç§ä¿®æ”¹å‡½æ•°è¡Œä¸ºçš„å·¥å…·"
    },
    # ... æ›´å¤šæµ‹è¯•æ•°æ®
])

results = evaluator.run_evaluation()
print(results)
# {'total_questions': 10, 'avg_accuracy': 0.85, 'avg_retrieval_relevance': 0.78}
```

### A/Bæµ‹è¯•

```python
def ab_test(model_a, model_b, test_cases: list):
    """A/Bæµ‹è¯•ä¸¤ä¸ªæ¨¡å‹"""

    results_a = []
    results_b = []

    for case in test_cases:
        response_a = model_a.invoke(case["input"])
        response_b = model_b.invoke(case["input"])

        # è¯„ä¼°
        score_a = evaluate_response(response_a, case)
        score_b = evaluate_response(response_b, case)

        results_a.append(score_a)
        results_b.append(score_b)

    # ç»Ÿè®¡
    avg_a = sum(results_a) / len(results_a)
    avg_b = sum(results_b) / len(results_b)

    return {
        "model_a_avg": avg_a,
        "model_b_avg": avg_b,
        "winner": "A" if avg_a > avg_b else "B"
    }
```

### æ€§èƒ½æµ‹è¯•

```python
import time
from statistics import mean

def performance_test(rag_system, test_questions: list):
    """æ€§èƒ½æµ‹è¯•"""

    latencies = []
    token_counts = []

    for question in test_questions:
        start = time.time()

        response = rag_system.ask(question)

        end = time.time()
        latency = end - start

        latencies.append(latency)
        # å‡è®¾ä½ æœ‰æ–¹æ³•è·å–tokenæ•°
        # token_counts.append(get_token_count(response))

    return {
        "avg_latency": mean(latencies),
        "p95_latency": sorted(latencies)[int(len(latencies) * 0.95)],
        "p99_latency": sorted(latencies)[int(len(latencies) * 0.99)],
        "min_latency": min(latencies),
        "max_latency": max(latencies)
    }

# ä½¿ç”¨
perf_results = performance_test(rag_system, test_questions)
print(f"å¹³å‡å»¶è¿Ÿï¼š{perf_results['avg_latency']:.2f}ç§’")
print(f"P95å»¶è¿Ÿï¼š{perf_results['p95_latency']:.2f}ç§’")
```

---

## æœ€ä½³å®è·µæ€»ç»“

### æ¨¡å‹é€‰æ‹©æ¸…å•

```
âœ“ æ˜ç¡®éœ€æ±‚
  - ä»»åŠ¡ç±»å‹ï¼ˆå¯¹è¯/åˆ†æ/ä»£ç ï¼‰
  - ä¸Šä¸‹æ–‡é•¿åº¦
  - å®æ—¶æ€§è¦æ±‚
  - é¢„ç®—é™åˆ¶

âœ“ è¯„ä¼°é€‰é¡¹
  - é—­æº vs å¼€æº
  - API vs æœ¬åœ°éƒ¨ç½²
  - æ€§èƒ½ vs æˆæœ¬

âœ“ æµ‹è¯•éªŒè¯
  - A/Bæµ‹è¯•
  - è¯„ä¼°æŒ‡æ ‡
  - ç”¨æˆ·åé¦ˆ
```

### ä¼˜åŒ–æŠ€å·§

```python
# 1. æç¤ºè¯ä¼˜åŒ–
- æ¸…æ™°å…·ä½“
- æä¾›ç¤ºä¾‹
- è§’è‰²è®¾å®š

# 2. æ£€ç´¢ä¼˜åŒ–
- è°ƒæ•´chunk_size
- ä½¿ç”¨æ··åˆæ£€ç´¢
- å®æ–½é‡æ’åº

# 3. æˆæœ¬ä¼˜åŒ–
- ä½¿ç”¨ç¼“å­˜
- æ‰¹é‡å¤„ç†
- é€‰æ‹©åˆé€‚æ¨¡å‹

# 4. æ€§èƒ½ä¼˜åŒ–
- å¼‚æ­¥è°ƒç”¨
- æµå¼è¾“å‡º
- è¿æ¥æ± 
```

### å®‰å…¨å»ºè®®

```
âœ“ APIå¯†é’¥ç®¡ç†
  - ä½¿ç”¨ç¯å¢ƒå˜é‡
  - å®šæœŸè½®æ¢
  - è®¿é—®æ§åˆ¶

âœ“ å†…å®¹è¿‡æ»¤
  - è¾“å…¥éªŒè¯
  - è¾“å‡ºå®¡æ ¸
  - æ•æ„Ÿä¿¡æ¯æ£€æµ‹

âœ“ é€Ÿç‡é™åˆ¶
  - é˜²æ­¢æ»¥ç”¨
  - æ§åˆ¶æˆæœ¬
  - ä¿æŠ¤æœåŠ¡
```

---

## æœ¬ç« å°ç»“

### æ ¸å¿ƒå†…å®¹

âœ… **LLMæ¨¡å‹é€‰æ‹©**ï¼š
- ä¸»æµæ¨¡å‹å¯¹æ¯”
- åœºæ™¯åŒ–é€‰æ‹©ç­–ç•¥
- æˆæœ¬æ€§èƒ½æƒè¡¡

âœ… **Claude API**ï¼š
- é•¿æ–‡æœ¬å¤„ç†ä¼˜åŠ¿
- å¤šæ¨¡æ€èƒ½åŠ›
- LangChainé›†æˆ

âœ… **å¼€æºæ¨¡å‹**ï¼š
- Ollamaæœ¬åœ°éƒ¨ç½²
- æˆæœ¬ä¼˜åŠ¿
- éšç§ä¿æŠ¤

âœ… **MCPåè®®**ï¼š
- ç»Ÿä¸€æ•°æ®æ¥å£
- å³æ’å³ç”¨
- ç”Ÿæ€å…±äº«

âœ… **LangGraph**ï¼š
- å¤æ‚Agentæ„å»º
- çŠ¶æ€å›¾ç®¡ç†
- å¯è§†åŒ–å·¥ä½œæµ

âœ… **è¯„ä¼°æµ‹è¯•**ï¼š
- Ragasæ¡†æ¶
- è‡ªå®šä¹‰è¯„ä¼°
- A/Bæµ‹è¯•

### è¿›é˜¶å­¦ä¹ è·¯å¾„

```
å½“å‰é˜¶æ®µï¼šAIåº”ç”¨å¼€å‘ âœ…
    â†“
è¿›é˜¶æ–¹å‘ï¼š
  â”œâ”€â”€ å¤šæ¨¡æ€AIï¼ˆå›¾åƒã€éŸ³é¢‘ã€è§†é¢‘ï¼‰
  â”œâ”€â”€ æ¨¡å‹å¾®è°ƒï¼ˆLoRAã€QLoRAï¼‰
  â”œâ”€â”€ ç”Ÿäº§éƒ¨ç½²ï¼ˆKubernetesã€ç›‘æ§ï¼‰
  â”œâ”€â”€ AIå®‰å…¨ï¼ˆå¯¹æŠ—æ”»å‡»ã€é˜²å¾¡ï¼‰
  â””â”€â”€ å‰æ²¿ç ”ç©¶ï¼ˆæœ€æ–°è®ºæ–‡ã€æŠ€æœ¯ï¼‰
```

---

## ç»ƒä¹ é¢˜

### ç»ƒä¹ 1ï¼šæ¨¡å‹å¯¹æ¯”

é€‰æ‹©åŒä¸€ä¸ªä»»åŠ¡ï¼ˆå¦‚RAGé—®ç­”ï¼‰ï¼Œä½¿ç”¨GPT-3.5ã€Claude Sonnetã€Llama 3åˆ†åˆ«å®ç°ï¼Œå¯¹æ¯”æ•ˆæœå’Œæˆæœ¬ã€‚

### ç»ƒä¹ 2ï¼šæ„å»ºMCP Server

åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„MCP Serverï¼Œé›†æˆä½ çš„æ•°æ®æºã€‚

### ç»ƒä¹ 3ï¼šLangGraphé¡¹ç›®

ä½¿ç”¨LangGraphå®ç°ä¸€ä¸ªå¤æ‚çš„å®¢æœç³»ç»Ÿï¼ŒåŒ…å«FAQã€æŠ•è¯‰ã€æŠ€æœ¯æ”¯æŒç­‰åˆ†æ”¯ã€‚

### ç»ƒä¹ 4ï¼šè¯„ä¼°æ¡†æ¶

ä¸ºä½ çš„RAGç³»ç»Ÿå»ºç«‹å®Œæ•´çš„è¯„ä¼°ä½“ç³»ï¼ŒåŒ…å«å‡†ç¡®æ€§ã€æ€§èƒ½ã€æˆæœ¬ç­‰ç»´åº¦ã€‚

---

**æ­å–œå®ŒæˆAIåº”ç”¨å¼€å‘å®Œå…¨æŒ‡å—ï¼** ğŸ‰

ä»åŸºç¡€åˆ°è¿›é˜¶ï¼Œä½ å·²ç»æŒæ¡äº†æ„å»ºç°ä»£AIåº”ç”¨çš„å…¨å¥—æŠ€èƒ½ã€‚

**ç»§ç»­ä¿æŒå­¦ä¹ ï¼Œæ¢ç´¢AIçš„æ— é™å¯èƒ½ï¼** ğŸš€
