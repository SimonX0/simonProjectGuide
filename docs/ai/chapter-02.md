# LangChainæ¡†æ¶

## æœ¬ç« å¯¼è¯»

åœ¨ç¬¬1ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•ç›´æ¥è°ƒç”¨OpenAI APIæ„å»ºç®€å•çš„AIåº”ç”¨ã€‚ä½†éšç€åº”ç”¨å˜å¾—å¤æ‚ï¼Œç›´æ¥ä½¿ç”¨APIä¼šé‡åˆ°å¾ˆå¤šæŒ‘æˆ˜ï¼šå¦‚ä½•ç®¡ç†å¯¹è¯å†å²ï¼Ÿå¦‚ä½•ä¸²è”å¤šä¸ªæ­¥éª¤ï¼Ÿå¦‚ä½•é›†æˆå¤–éƒ¨æ•°æ®ï¼Ÿ

**LangChain** æ˜¯ç›®å‰æœ€æµè¡Œçš„LLMåº”ç”¨å¼€å‘æ¡†æ¶ï¼Œå®ƒä¸ºæˆ‘ä»¬æä¾›äº†ä¸€å¥—å®Œæ•´çš„å·¥å…·é“¾ï¼Œå¤§å¤§ç®€åŒ–äº†LLMåº”ç”¨çš„å¼€å‘ã€‚

**2024-2026æ›´æ–°**ï¼š
- LangChain 0.3+ é‡æ–°è®¾è®¡çš„æ¶æ„
- LCELï¼ˆLangChain Expression Languageï¼‰æˆä¸ºæ ‡å‡†
- å†…ç½® LangGraph æ”¯æŒå¤æ‚ Agent
- æ”¹è¿›çš„ç±»å‹æç¤ºå’Œé”™è¯¯å¤„ç†
- æ›´å¥½çš„æµå¼è¾“å‡ºæ”¯æŒ

**å­¦ä¹ ç›®æ ‡**ï¼š
- ç†è§£LangChainçš„æ ¸å¿ƒæ¦‚å¿µå’Œæ¶æ„
- æŒæ¡Model I/Oï¼šæ¨¡å‹ã€æç¤ºè¯ã€è¾“å‡ºè§£æ
- å­¦ä¹ Chainsï¼šä¸²è”å¤šä¸ªæ“ä½œ
- å®ç°Memoryï¼šå¯¹è¯è®°å¿†ç®¡ç†
- äº†è§£LangGraphåŸºç¡€

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š60åˆ†é’Ÿ

---

## ä»€ä¹ˆæ˜¯LangChainï¼Ÿ

### LangChainç®€ä»‹

**LangChain** æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¼€å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚å®ƒæä¾›äº†ä¸€ç³»åˆ—æ¨¡å—åŒ–çš„ç»„ä»¶ï¼Œè®©ä½ å¯ä»¥è½»æ¾æ„å»ºå¤æ‚çš„AIåº”ç”¨ã€‚

**æ ¸å¿ƒä»·å€¼**ï¼š

```
ç›´æ¥ä½¿ç”¨APIçš„é—®é¢˜ï¼š
  âŒ æç¤ºè¯ç®¡ç†æ··ä¹±
  âŒ éš¾ä»¥ä¸²è”å¤šä¸ªæ­¥éª¤
  âŒ å¯¹è¯å†å²éœ€è¦æ‰‹åŠ¨ç»´æŠ¤
  âŒ éš¾ä»¥é›†æˆå¤–éƒ¨æ•°æ®
  âŒ ç¼ºä¹æ ‡å‡†åŒ–ç»„ä»¶

LangChainçš„è§£å†³æ–¹æ¡ˆï¼š
  âœ… æç¤ºè¯æ¨¡æ¿åŒ–ç®¡ç†
  âœ… Chainé“¾å¼è°ƒç”¨
  âœ… Memoryè‡ªåŠ¨ç®¡ç†
  âœ… Data Connectioné›†æˆ
  âœ… ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿ
```

### LangChainçš„å…­å¤§æ ¸å¿ƒæ¨¡å—

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LangChain æ¶æ„å›¾                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                              â”‚
â”‚  ğŸ“¦ Model I/O      æ¨¡å‹è¾“å…¥è¾“å‡º                â”‚
â”‚    â”œâ”€â”€ Models    LLMåŒ…è£…å’Œç»Ÿä¸€æ¥å£            â”‚
â”‚    â”œâ”€â”€ Prompts   æç¤ºè¯æ¨¡æ¿                   â”‚
â”‚    â””â”€â”€ Parsers   è¾“å‡ºè§£æå™¨                   â”‚
â”‚                                              â”‚
â”‚  ğŸ”— Chains        é“¾å¼è°ƒç”¨                    â”‚
â”‚    â”œâ”€â”€ LLMChain  åŸºç¡€é“¾                      â”‚
â”‚    â”œâ”€â”€ Sequential é¡ºåºé“¾                     â”‚
â”‚    â””â”€â”€ Router    è·¯ç”±é“¾                      â”‚
â”‚                                              â”‚
â”‚  ğŸ§  Memory        è®°å¿†ç®¡ç†                    â”‚
â”‚    â”œâ”€â”€ Buffer    ç¼“å†²è®°å¿†                    â”‚
â”‚    â”œâ”€â”€ Summary   æ‘˜è¦è®°å¿†                    â”‚
â”‚    â””â”€â”€ Knowledge çŸ¥è¯†å›¾è°±è®°å¿†                 â”‚
â”‚                                              â”‚
â”‚  ğŸ“š Retrieval     æ£€ç´¢ç”Ÿæˆ                    â”‚
â”‚    â”œâ”€â”€ Loaders   æ–‡æ¡£åŠ è½½å™¨                   â”‚
â”‚    â”œâ”€â”€ Splitters æ–‡æœ¬åˆ†å‰²å™¨                  â”‚
â”‚    â”œâ”€â”€ Embeddings å‘é‡åŒ–                    â”‚
â”‚    â””â”€â”€ VectorStores å‘é‡æ•°æ®åº“               â”‚
â”‚                                              â”‚
â”‚  ğŸ¤– Agents        æ™ºèƒ½ä»£ç†                    â”‚
â”‚    â”œâ”€â”€ Tools     å·¥å…·å®šä¹‰                    â”‚
â”‚    â”œâ”€â”€ Agent     ä»£ç†ç±»å‹                    â”‚
â”‚    â””â”€â”€ Executor  æ‰§è¡Œå™¨                      â”‚
â”‚                                              â”‚
â”‚  ğŸ”§ Chains        å·¥å…·å’Œå›è°ƒ                  â”‚
â”‚    â””â”€â”€ Callbacks è§‚å¯Ÿå’Œç›‘å¬                   â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å®‰è£…LangChain

```bash
# 2024-2026 æ¨èå®‰è£…æ–¹å¼

# æ ¸å¿ƒ - LangChain 0.3+
pip install langchain==0.3.0
pip install langchain-core==0.3.0
pip install langchain-openai==0.2.0

# ç¤¾åŒºæ‰©å±•
pip install langchain-community==0.3.0

# LangGraph - å¤æ‚Agentç¼–æ’
pip install langgraph==0.2.0

# å‘é‡æ•°æ®åº“
pip install chromadb==0.5.0
pip install faiss-cpu==1.8.0

# å®Œæ•´ä¾èµ–ï¼ˆä¸€é”®å®‰è£…ï¼‰
pip install langchain==0.3.0 \
            langchain-openai==0.2.0 \
            langchain-community==0.3.0 \
            langgraph==0.2.0 \
            chromadb==0.5.0

# éªŒè¯å®‰è£…
python -c "import langchain; print(langchain.__version__)"
# åº”è¾“å‡º: 0.3.0 æˆ–æ›´é«˜ç‰ˆæœ¬
```

**ç‰ˆæœ¬è¯´æ˜ï¼ˆ2024-2026ï¼‰**ï¼š
- **LangChain 0.3+**ï¼ˆ2024å¹´10æœˆå‘å¸ƒï¼‰- é‡å¤§æ¶æ„æ›´æ–°
  - å®Œå…¨åŸºäºLCELé‡å†™
  - ç§»é™¤æ‰€æœ‰åºŸå¼ƒçš„API
  - æ€§èƒ½æå‡30-50%
  - æ›´å¥½çš„Pythonç±»å‹æç¤º
  - å†…ç½®LangGraphæ”¯æŒ

---

## Model I/Oï¼šæ¨¡å‹è¾“å…¥è¾“å‡º

Model I/Oæ˜¯LangChainæœ€åŸºç¡€çš„æ¨¡å—ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š
- **Models**ï¼šä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’
- **Prompts**ï¼šç®¡ç†æç¤ºè¯
- **Output Parsers**ï¼šè§£ææ¨¡å‹è¾“å‡º

### Modelsï¼šæ¨¡å‹æ¥å£

LangChainæä¾›äº†ç»Ÿä¸€çš„æ¨¡å‹æ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from config import Config

# 2024-2026 æ¨èæ¨¡å‹é…ç½®

# 1. GPT-4o - å¤šæ¨¡æ€ã€é€Ÿåº¦å¿«
gpt4o = ChatOpenAI(
    model="gpt-4o",  # 2024å¹´æœ€æ–°å¤šæ¨¡æ€æ¨¡å‹
    api_key=Config.OPENAI_API_KEY,
    temperature=0.7,
    max_tokens=4096
)

# 2. Claude 3.5 Sonnet - ç»¼åˆèƒ½åŠ›æœ€å¼º
claude = ChatAnthropic(
    model="claude-3-5-sonnet-20241022",  # 2024å¹´æœ€å¼ºæ¨¡å‹
    api_key=Config.ANTHROPIC_API_KEY,
    temperature=0.7,
    max_tokens=8192  # Claudeæ”¯æŒæ›´é•¿è¾“å‡º
)

# 3. GPT-4o-mini - æ€§ä»·æ¯”æœ€é«˜
gpt4o_mini = ChatOpenAI(
    model="gpt-4o-mini",  # 2024å¹´æ€§ä»·æ¯”ä¹‹ç‹
    api_key=Config.OPENAI_API_KEY,
    temperature=0.7,
    max_tokens=16384
)

# ä½¿ç”¨ç¤ºä¾‹
message = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
response = gpt4o.invoke(message)
print(response.content)  # AIçš„å›å¤

# 2024-2026 æ¨¡å‹é€‰æ‹©å»ºè®®ï¼š
# - å¤æ‚ä»»åŠ¡/ä»£ç  â†’ Claude 3.5 Sonnet
# - å¤šæ¨¡æ€/å®æ—¶ â†’ GPT-4o
# - é«˜é¢‘ç®€å•ä»»åŠ¡ â†’ GPT-4o-mini
# - ä¸­æ–‡ä¼˜åŒ– â†’ Qwen 2.5 / DeepSeek-V3
```

**Messageç±»å‹**ï¼š

```python
from langchain_core.messages import (
    HumanMessage,      # äººç±»æ¶ˆæ¯
    AIMessage,         # AIæ¶ˆæ¯
    SystemMessage,     # ç³»ç»Ÿæ¶ˆæ¯
    ToolMessage        # å·¥å…·æ¶ˆæ¯
)

# æ„å»ºæ¶ˆæ¯åˆ—è¡¨
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonå¯¼å¸ˆ"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ")
]

response = chat.invoke(messages)
print(response.content)
```

### Promptsï¼šæç¤ºè¯æ¨¡æ¿

#### åŸºç¡€æç¤ºè¯æ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate

# æ–¹å¼1ï¼šä½¿ç”¨PromptTemplate
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate(
    template="è¯·è§£é‡Š{concept}è¿™ä¸ªæ¦‚å¿µï¼Œç”¨{style}çš„é£æ ¼",
    input_variables=["concept", "style"]
)

# å¡«å……æ¨¡æ¿
formatted = prompt.format(
    concept="æœºå™¨å­¦ä¹ ",
    style="é€šä¿—æ˜“æ‡‚"
)
print(formatted)
# è¾“å‡ºï¼šè¯·è§£é‡Šæœºå™¨å­¦ä¹ è¿™ä¸ªæ¦‚å¿µï¼Œç”¨é€šä¿—æ˜“æ‡‚çš„é£æ ¼
```

#### Chatæç¤ºè¯æ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate

# åˆ›å»ºæ¨¡æ¿
prompt_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}åŠ©æ‰‹ã€‚"),
    ("human", "{user_input}")
])

# å¡«å……å¹¶è°ƒç”¨
prompt = prompt_template.format_messages(
    role="Pythonç¼–ç¨‹",
    user_input="ä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼ï¼Ÿ"
)

response = chat.invoke(prompt)
print(response.content)
```

#### å®ç”¨çš„æç¤ºè¯æ¨¡æ¿

```python
# 1. ä»£ç ç”Ÿæˆæ¨¡æ¿
code_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{language}ç¼–ç¨‹ä¸“å®¶ã€‚"),
    ("human", """è¯·å¸®æˆ‘å®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š
    åŠŸèƒ½æè¿°ï¼š{task}
    è¦æ±‚ï¼š
    - ä»£ç è¦æœ‰æ³¨é‡Š
    - åŒ…å«ä½¿ç”¨ç¤ºä¾‹
    - è§£é‡Šæ—¶é—´å¤æ‚åº¦
    """)
])

# 2. æ–‡æ¡£åˆ†ææ¨¡æ¿
doc_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æå¸ˆã€‚"),
    ("human", """è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼š
    {document}

    ä»»åŠ¡ï¼š{task}
    è¾“å‡ºæ ¼å¼ï¼š{format}
    """)
])

# 3. è§’è‰²æ‰®æ¼”æ¨¡æ¿
role_prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ ç°åœ¨æ˜¯{role}ã€‚
    ä½ çš„ç‰¹ç‚¹æ˜¯ï¼š{characteristics}
    ä½ çš„è¯´è¯é£æ ¼ï¼š{style}"""),
    ("human", "{question}")
])
```

### Output Parsersï¼šè¾“å‡ºè§£æå™¨

è¾“å‡ºè§£æå™¨è®©LLMçš„è¾“å‡ºæ›´ç»“æ„åŒ–ã€‚

#### åŸºç¡€è§£æå™¨

```python
from langchain_core.output_parsers import StrOutputParser

# åˆ›å»ºè§£æå™¨
output_parser = StrOutputParser()

# ä½¿ç”¨é“¾å¼è°ƒç”¨
chain = chat | output_parser  # | æ˜¯ç®¡é“æ“ä½œç¬¦

response = chain.invoke("ä»€ä¹ˆæ˜¯Pythonï¼Ÿ")
print(response)  # ç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼Œæ— éœ€.content
```

#### ç»“æ„åŒ–è¾“å‡ºè§£æå™¨

```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate

# 1. å®šä¹‰æœŸæœ›çš„è¾“å‡ºæ ¼å¼
format_instructions = """
è¯·ä»¥JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
    "summary": "ç®€è¦æ€»ç»“",
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"],
    "difficulty": "éš¾åº¦ç­‰çº§(1-5)"
}
"""

# 2. åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt = PromptTemplate(
    template="""è¯·åˆ†æä»¥ä¸‹ä¸»é¢˜ï¼š{topic}

    {format_instructions}
    """,
    input_variables=["topic"],
    partial_variables={"format_instructions": format_instructions}
)

# 3. åˆ›å»ºè§£æå™¨å’Œé“¾
parser = JsonOutputParser()
chain = prompt | chat | parser

# 4. è°ƒç”¨
result = chain.invoke({"topic": "Pythonè£…é¥°å™¨"})
print(result)
# è¾“å‡ºï¼š{'summary': '...', 'key_points': [...], 'difficulty': 3}
```

#### Pydanticè§£æå™¨ï¼ˆæ¨èï¼‰

```python
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# 1. å®šä¹‰æ•°æ®æ¨¡å‹
class CodeAnalysis(BaseModel):
    """ä»£ç åˆ†æç»“æœ"""
    language: str = Field(description="ç¼–ç¨‹è¯­è¨€")
    time_complexity: str = Field(description="æ—¶é—´å¤æ‚åº¦")
    has_issues: bool = Field(description="æ˜¯å¦å­˜åœ¨é—®é¢˜")
    suggestions: list[str] = Field(description="æ”¹è¿›å»ºè®®")

# 2. åˆ›å»ºè§£æå™¨
parser = PydanticOutputParser(pydantic_object=CodeAnalysis)

# 3. è·å–æ ¼å¼è¯´æ˜
format_instructions = parser.get_format_instructions()

# 4. åˆ›å»ºæç¤ºè¯
prompt = PromptTemplate(
    template="""åˆ†æä»¥ä¸‹ä»£ç ï¼š
    {code}

    {format_instructions}
    """,
    input_variables=["code"],
    partial_variables={"format_instructions": format_instructions}
)

# 5. åˆ›å»ºé“¾
chain = prompt | chat | parser

# 6. è°ƒç”¨
result = chain.invoke({"code": "def foo(): return [i**2 for i in range(1000000)]"})
print(result)
# CodeAnalysis(language='Python', time_complexity='O(n)', has_issues=False, suggestions=[...])
```

---

## Chainsï¼šé“¾å¼è°ƒç”¨

Chainså…è®¸ä½ å°†å¤šä¸ªç»„ä»¶ä¸²è”èµ·æ¥ï¼Œæ„å»ºå¤æ‚çš„å·¥ä½œæµã€‚

### LLMChainï¼šæœ€åŸºæœ¬çš„é“¾

```python
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate

# åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt = PromptTemplate(
    input_variables=["product"],
    template="ä¸º{product}äº§å“å†™ä¸€ä¸ªå¸å¼•äººçš„å¹¿å‘Šè¯­ï¼Œä¸è¶…è¿‡20å­—ã€‚"
)

# åˆ›å»ºé“¾
chain = LLMChain(
    llm=chat,
    prompt=prompt
)

# è°ƒç”¨é“¾
result = chain.run(product="æ™ºèƒ½æ‰‹è¡¨")
print(result)  # "è…•é—´æ™ºæ…§ï¼Œå¥åº·éšè¡Œï¼"

# ä¹Ÿå¯ä»¥ç”¨invokeæ–¹å¼
result = chain.invoke({"product": "æ™ºèƒ½æ‰‹è¡¨"})
print(result['text'])
```

### Sequential Chainï¼šé¡ºåºé“¾

æŒ‰é¡ºåºæ‰§è¡Œå¤šä¸ªé“¾ã€‚

```python
from langchain.chains import SequentialChain

# Chain 1: ç”Ÿæˆæ•…äº‹å¤§çº²
outline_prompt = PromptTemplate(
    input_variables=["topic"],
    template="ä¸ºä¸»é¢˜'{topic}'åˆ›ä½œä¸€ä¸ª3ç« çš„æ•…äº‹å¤§çº²"
)
outline_chain = LLMChain(
    llm=chat,
    prompt=outline_prompt,
    output_key="outline"
)

# Chain 2: æ ¹æ®å¤§çº²å†™ç¬¬ä¸€ç« 
chapter_prompt = PromptTemplate(
    input_variables=["outline"],
    template="æ ¹æ®ä»¥ä¸‹å¤§çº²å†™ç¬¬ä¸€ç« ï¼ˆ500å­—ï¼‰ï¼š\n{outline}"
)
chapter_chain = LLMChain(
    llm=chat,
    prompt=chapter_prompt,
    output_key="chapter"
)

# Chain 3: ä¸ºç« èŠ‚èµ·æ ‡é¢˜
title_prompt = PromptTemplate(
    input_variables=["chapter"],
    template="ä¸ºä»¥ä¸‹ç« èŠ‚èµ·ä¸€ä¸ªå¸å¼•äººçš„æ ‡é¢˜ï¼š\n{chapter}"
)
title_chain = LLMChain(
    llm=chat,
    prompt=title_prompt,
    output_key="title"
)

# ç»„åˆæˆé¡ºåºé“¾
overall_chain = SequentialChain(
    chains=[outline_chain, chapter_chain, title_chain],
    input_variables=["topic"],
    output_variables=["outline", "chapter", "title"]
)

# æ‰§è¡Œ
result = overall_chain.invoke({"topic": "æ—¶é—´æ—…è¡Œ"})
print(f"å¤§çº²ï¼š\n{result['outline']}\n")
print(f"ç¬¬ä¸€ç« ï¼š\n{result['chapter']}\n")
print(f"æ ‡é¢˜ï¼š{result['title']}")
```

### ä½¿ç”¨LCELï¼ˆLangChain Expression Languageï¼‰

LCELæ˜¯LangChainæ¨èçš„æ–°è¯­æ³•ï¼Œæ›´ç®€æ´ä¼˜é›…ã€‚

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ |
prompt = ChatPromptTemplate.from_messages([
    ("human", "è®²ä¸€ä¸ªå…³äº{topic}çš„æ•…äº‹")
])

# åˆ›å»ºé“¾ï¼šprompt | model | parser
chain = prompt | chat | StrOutputParser()

# è°ƒç”¨
result = chain.invoke({"topic": "å‹‡æ•¢çš„å°å…”å­"})
print(result)
```

#### å¤æ‚çš„LCELé“¾

```python
# é“¾å¼è°ƒç”¨å¤šä¸ªç»„ä»¶
from langchain_core.runnables import RunnablePassthrough

# 1. å‡†å¤‡æ•°æ®
def prepare_data(inputs):
    topic = inputs["topic"]
    return {
        "topic": topic,
        "style": inputs.get("style", "å¹½é»˜"),
        "length": inputs.get("length", "short")
    }

# 2. åˆ›å»ºæç¤ºè¯
prompt = ChatPromptTemplate.from_template(
    "ç”¨{style}çš„é£æ ¼å†™ä¸€ä¸ª{length}çš„æ•…äº‹ï¼Œä¸»é¢˜æ˜¯{topic}"
)

# 3. æ„å»ºé“¾
chain = (
    RunnablePassthrough.assign(
        style=lambda x: x.get("style", "å¹½é»˜"),
        length=lambda x: x.get("length", "short")
    )
    | prompt
    | chat
    | StrOutputParser()
)

# è°ƒç”¨
result = chain.invoke({
    "topic": "AIå­¦ä¹ ",
    "style": "åŠ±å¿—",
    "length": "long"
})
```

### Router Chainï¼šè·¯ç”±é“¾

æ ¹æ®è¾“å…¥å†…å®¹åŠ¨æ€é€‰æ‹©ä¸åŒçš„å¤„ç†é“¾ã€‚

```python
from langchain.chains import RouterChain
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# å®šä¹‰å¤šä¸ªå¤„ç†é“¾
# Chain 1: ä»£ç ç›¸å…³
code_prompt = PromptTemplate(
    template="ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹ä¸“å®¶ã€‚{input}",
    input_variables=["input"]
)
code_chain = LLMChain(llm=chat, prompt=code_prompt)

# Chain 2: æ–‡æœ¬ç›¸å…³
text_prompt = PromptTemplate(
    template="ä½ æ˜¯ä¸€ä¸ªå†™ä½œåŠ©æ‰‹ã€‚{input}",
    input_variables=["input"]
)
text_chain = LLMChain(llm=chat, prompt=text_prompt)

# Chain 3: é€šç”¨
general_prompt = PromptTemplate(
    template="ä½ æ˜¯AIåŠ©æ‰‹ã€‚{input}",
    input_variables=["input"]
)
general_chain = LLMChain(llm=chat, prompt=general_prompt)

# è·¯ç”±é€»è¾‘
from langchain.chains.router import MultiPromptChain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE

# å®šä¹‰è·¯ç”±çš„ç›®æ ‡å’Œæè¿°
destinations = [
    "code: ç¼–ç¨‹å’Œä»£ç ç›¸å…³é—®é¢˜",
    "text: å†™ä½œå’Œæ–‡æœ¬ç›¸å…³é—®é¢˜",
    "general: å…¶ä»–é€šç”¨é—®é¢˜"
]

# åˆ›å»ºè·¯ç”±æç¤ºè¯
router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(
    destinations=destinations
)
router_prompt = PromptTemplate(
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser()
)

# åˆ›å»ºè·¯ç”±é“¾
router_chain = LLMRouterChain.from_llm(
    chat,
    router_prompt
)

# ç»„åˆ
chain = MultiPromptChain(
    router_chain=router_chain,
    destination_chains={
        "code": code_chain,
        "text": text_chain,
        "general": general_chain
    },
    default_chain=general_chain
)

# ä½¿ç”¨
result = chain.run("å¦‚ä½•åœ¨Pythonä¸­å®ç°å¿«é€Ÿæ’åºï¼Ÿ")
# ä¼šè‡ªåŠ¨è·¯ç”±åˆ°codeé“¾
```

**2024-2026æ›´æ–°ï¼šä½¿ç”¨RunnableLambdaå®ç°æ›´çµæ´»çš„è·¯ç”±**

```python
from langchain_core.runnables import RunnableLambda

# å®šä¹‰è·¯ç”±å‡½æ•°
def route_func(inputs):
    query = inputs["input"].lower()
    if "ä»£ç " in query or "ç¼–ç¨‹" in query:
        return "code"
    elif "æ–‡ç« " in query or "æ–‡æ¡ˆ" in query:
        return "text"
    else:
        return "general"

# åˆ›å»ºè·¯ç”±é“¾
from langchain_core.runnables import RunnableBranch

branch = RunnableBranch(
    (lambda x: "ä»£ç " in x["input"], code_chain),
    (lambda x: "æ–‡æœ¬" in x["input"], text_chain),
    general_chain
)

# ä½¿ç”¨LCELè¯­æ³•
chain = branch
result = chain.invoke({"input": "å†™Pythonä»£ç å®ç°å¿«é€Ÿæ’åº"})
```

---

## Memoryï¼šè®°å¿†ç®¡ç†

Memoryç»„ä»¶è®©AIèƒ½å¤Ÿè®°ä½å¯¹è¯å†å²ï¼Œå®ç°å¤šè½®å¯¹è¯ã€‚

### ConversationBufferMemoryï¼šç¼“å†²è®°å¿†

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory()

# åˆ›å»ºå¯¹è¯é“¾
conversation = ConversationChain(
    llm=chat,
    memory=memory,
    verbose=True  # æ‰“å°è¯¦ç»†ä¿¡æ¯
)

# ç¬¬ä¸€æ¬¡å¯¹è¯
response1 = conversation.predict(input="æˆ‘å«å°æ˜")
print(response1)
# AI: ä½ å¥½å°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚

# ç¬¬äºŒæ¬¡å¯¹è¯ï¼ˆAIè®°ä½äº†åå­—ï¼‰
response2 = conversation.predict(input="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
print(response2)
# AI: ä½ å«å°æ˜ã€‚

# æŸ¥çœ‹è®°å¿†å†å²
print(memory.load_memory_variables({}))
# {'history': 'Human: æˆ‘å«å°æ˜\nAI: ä½ å¥½å°æ˜ï¼...'}
```

### ConversationBufferWindowMemoryï¼šçª—å£è®°å¿†

åªä¿å­˜æœ€è¿‘Nè½®å¯¹è¯ï¼ŒèŠ‚çœTokenã€‚

```python
from langchain.memory import ConversationBufferWindowMemory

# åªä¿å­˜æœ€è¿‘3è½®å¯¹è¯
memory = ConversationBufferWindowMemory(k=3)

conversation = ConversationChain(
    llm=chat,
    memory=memory
)

# è¿›è¡Œå¤šè½®å¯¹è¯
conversation.predict(input="æˆ‘å–œæ¬¢Python")
conversation.predict(input="æˆ‘ä¹Ÿå–œæ¬¢JavaScript")
conversation.predict(input="æˆ‘æœ€å–œæ¬¢Rust")
conversation.predict(input="æˆ‘å–œæ¬¢ä»€ä¹ˆè¯­è¨€ï¼Ÿ")
# AIè®°å¾—æœ€è¿‘3è½®çš„å¯¹è¯ï¼Œä½†ä¸è®°å¾—æ›´æ—©çš„å†…å®¹
```

### ConversationSummaryMemoryï¼šæ‘˜è¦è®°å¿†

å¯¹é•¿å¯¹è¯è¿›è¡Œæ‘˜è¦ï¼ŒèŠ‚çœTokenã€‚

```python
from langchain.memory import ConversationSummaryMemory

# åˆ›å»ºæ‘˜è¦è®°å¿†
memory = ConversationSummaryMemory(llm=chat)

conversation = ConversationChain(
    llm=chat,
    memory=memory,
    verbose=True
)

# è¿›è¡Œå¤šè½®å¯¹è¯
conversation.predict(input="æˆ‘æ˜¯å°æ˜ï¼Œæ˜¯ä¸€åç¨‹åºå‘˜")
conversation.predict(input="æˆ‘ä¸»è¦ç”¨Pythonå¼€å‘")
conversation.predict(input="æˆ‘åœ¨æ­å·å·¥ä½œ")

# æŸ¥çœ‹æ‘˜è¦
print(memory.load_memory_variables({}))
# ç±»ä¼¼ï¼š'summary': 'The human introduces themselves as Xiao Ming...'
```

### åœ¨LCELä¸­ä½¿ç”¨Memory

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# åˆ›å»ºåŒ…å«å†å²æ¶ˆæ¯çš„æç¤ºè¯
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"),
    MessagesPlaceholder(variable_name="history"),  # å†å²æ¶ˆæ¯å ä½ç¬¦
    ("human", "{input}")
])

# åˆ›å»ºé“¾
chain = prompt | chat

# åˆå§‹åŒ–å†å²
history = []

# ç¬¬ä¸€è½®
inputs = {"input": "ä½ å¥½", "history": history}
response = chain.invoke(inputs)
print(response.content)

# ä¿å­˜åˆ°å†å²
history.append(HumanMessage(content="ä½ å¥½"))
history.append(AIMessage(content=response.content))

# ç¬¬äºŒè½®
inputs = {"input": "æˆ‘å«ä»€ä¹ˆï¼Ÿ", "history": history}
response = chain.invoke(inputs)
# AIå¯ä»¥å¼•ç”¨ä¹‹å‰çš„å¯¹è¯
```

### å®ç”¨çš„è®°å¿†ç®¡ç†æŠ€å·§

```python
# 1. ä¿å­˜å’ŒåŠ è½½è®°å¿†
import json

# ä¿å­˜
memory_dict = memory.save_context(
    {"input": "ç”¨æˆ·è¾“å…¥"},
    {"output": "AIå›å¤"}
)
with open("memory.json", "w") as f:
    json.dump(memory.load_memory_variables({}), f)

# åŠ è½½
with open("memory.json") as f:
    saved_memory = json.load(f)
    memory = ConversationBufferMemory()
    memory.chat_memory.add_user_message(saved_memory['history'])

# 2. æ¸…ç©ºè®°å¿†
memory.clear()

# 3. è·å–Tokenä½¿ç”¨é‡
from langchain.callbacks import get_openai_callback

with get_openai_callback() as cb:
    response = conversation.predict(input="ä½ å¥½")
    print(f"Total Tokens: {cb.total_tokens}")
    print(f"Total Cost: ${cb.total_cost}")
```

---

## ç»¼åˆå®æˆ˜ï¼šæ™ºèƒ½é—®ç­”ç³»ç»Ÿ

è®©æˆ‘ä»¬å°†æ‰€å­¦çŸ¥è¯†æ•´åˆèµ·æ¥ï¼Œæ„å»ºä¸€ä¸ªæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from config import Config

class SmartQA:
    """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""

    def __init__(self):
        # åˆå§‹åŒ–æ¨¡å‹
        self.llm = ChatOpenAI(
            model=Config.MODEL_NAME,
            api_key=Config.OPENAI_API_KEY,
            temperature=0.7
        )

        # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œåå«å°å¾ã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
- å›ç­”å‡†ç¡®ä¸“ä¸š
- ä½¿ç”¨emojiå¢åŠ äº²å’ŒåŠ›
- é‡è¦å†…å®¹ç”¨åˆ—è¡¨å‘ˆç°
- ä¸ç¡®å®šçš„é—®é¢˜ä¼šè¯šå®å‘ŠçŸ¥"""),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])

        # åˆ›å»ºè®°å¿†
        self.memory = ConversationBufferMemory(
            return_messages=True,
            memory_key="history"
        )

        # åˆ›å»ºé“¾
        self.chain = self.prompt | self.llm | StrOutputParser()

    def ask(self, question: str) -> str:
        """æé—®"""
        # è·å–å†å²
        history = self.memory.chat_memory.messages

        # è°ƒç”¨é“¾
        response = self.chain.invoke({
            "input": question,
            "history": history
        })

        # ä¿å­˜åˆ°è®°å¿†
        self.memory.chat_memory.add_user_message(question)
        self.memory.chat_memory.add_ai_message(response)

        return response

    def chat(self):
        """äº¤äº’å¼èŠå¤©"""
        print("ğŸ¤– æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼ˆè¾“å…¥'quit'é€€å‡ºï¼‰\n")

        while True:
            question = input("\nä½ ï¼š").strip()

            if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("å°å¾ï¼šå†è§ï¼ğŸ‘‹")
                break

            if not question:
                continue

            try:
                answer = self.ask(question)
                print(f"å°å¾ï¼š{answer}")
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼š{e}")

# ä½¿ç”¨
if __name__ == "__main__":
    qa = SmartQA()
    qa.chat()
```

---

## æœ€ä½³å®è·µ

### æç¤ºè¯æ¨¡æ¿ç®¡ç†

```python
# åˆ›å»ºtemplates.py
PROMPTS = {
    "code_assistant": ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯{language}ç¼–ç¨‹ä¸“å®¶"),
        ("human", "{question}")
    ]),

    "writer": ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸“ä¸šå†™æ‰‹ï¼Œé£æ ¼{style}"),
        ("human", "ä¸»é¢˜ï¼š{topic}ï¼Œå­—æ•°ï¼š{words}")
    ]),

    "analyst": ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯æ•°æ®åˆ†æå¸ˆ"),
        ("human", "åˆ†æä»¥ä¸‹æ•°æ®ï¼š\n{data}")
    ])
}

# ä½¿ç”¨
from templates import PROMPTS

chain = PROMPTS["code_assistant"] | chat | StrOutputParser()
```

### é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_core.exceptions import LangChainException

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def safe_chain_invoke(chain, inputs):
    """å¸¦é‡è¯•çš„é“¾è°ƒç”¨"""
    try:
        return chain.invoke(inputs)
    except LangChainException as e:
        print(f"âš ï¸ è°ƒç”¨å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        raise
```

### æˆæœ¬ä¼˜åŒ–

```python
# 1. ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
fast_llm = ChatOpenAI(model="gpt-3.5-turbo")
smart_llm = ChatOpenAI(model="gpt-4")

# ç®€å•ä»»åŠ¡ç”¨å¿«é€Ÿæ¨¡å‹
simple_chain = fast_llm

# å¤æ‚ä»»åŠ¡ç”¨æ™ºèƒ½æ¨¡å‹
complex_chain = smart_llm

# 2. ç¼“å­˜ç»“æœ
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())

# ç¬¬ä¸€æ¬¡è°ƒç”¨ä¼šè¯·æ±‚API
chain.invoke("Pythonæ˜¯ä»€ä¹ˆï¼Ÿ")

# ç¬¬äºŒæ¬¡ç›´æ¥ä»ç¼“å­˜è¯»å–
chain.invoke("Pythonæ˜¯ä»€ä¹ˆï¼Ÿ")

# 3. æµå¼è¾“å‡ºèŠ‚çœé¦–å­—èŠ‚æ—¶é—´
for chunk in chain.stream("è®²ä¸ªæ•…äº‹"):
    print(chunk.content, end="", flush=True)
```

---

## æœ¬ç« å°ç»“

### æ ¸å¿ƒæ¦‚å¿µå›é¡¾

âœ… **LangChainå…­å¤§æ¨¡å—**ï¼š
- Model I/Oã€Chainsã€Memory
- Retrievalã€Agentsã€Callbacks

âœ… **Model I/O**ï¼š
- Modelsï¼šç»Ÿä¸€çš„æ¨¡å‹æ¥å£
- Promptsï¼šæç¤ºè¯æ¨¡æ¿
- Output Parsersï¼šç»“æ„åŒ–è¾“å‡º

âœ… **Chains**ï¼š
- LLMChainï¼šåŸºç¡€é“¾
- Sequential Chainï¼šé¡ºåºé“¾
- Router Chainï¼šè·¯ç”±é“¾
- LCELï¼šæ–°çš„é“¾å¼è¯­æ³•

âœ… **Memory**ï¼š
- BufferMemoryï¼šå…¨éƒ¨è®°å¿†
- WindowMemoryï¼šçª—å£è®°å¿†
- SummaryMemoryï¼šæ‘˜è¦è®°å¿†

### ä½ å·²ç»å­¦ä¼š

- âœ… ä½¿ç”¨LangChainç»Ÿä¸€æ¥å£è°ƒç”¨LLM
- âœ… åˆ›å»ºå’Œç®¡ç†æç¤ºè¯æ¨¡æ¿
- âœ… è§£æå’Œç»“æ„åŒ–è¾“å‡º
- âœ… æ„å»ºå¤æ‚çš„é“¾å¼å·¥ä½œæµ
- âœ… å®ç°å¯¹è¯è®°å¿†ç®¡ç†
- âœ… æ„å»ºå®Œæ•´çš„é—®ç­”ç³»ç»Ÿ

### ä¸‹ä¸€æ­¥

æ­å–œå®Œæˆç¬¬2ç« ï¼ä½ å·²ç»æŒæ¡äº†LangChainçš„æ ¸å¿ƒç”¨æ³•ã€‚

**åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ **ï¼š
- Prompt Engineeringçš„æ ¸å¿ƒåŸåˆ™
- é«˜çº§æç¤ºè¯æŠ€å·§å’Œæ¨¡å¼
- Few-shot Learning
- æç¤ºè¯ä¼˜åŒ–æ–¹æ³•

**å‡†å¤‡å¥½äº†å—ï¼Ÿç»§ç»­ï¼** ğŸš€

---

## ç»ƒä¹ é¢˜

### åŸºç¡€ç»ƒä¹ 

**ç»ƒä¹ 1**ï¼šä½¿ç”¨LCELåˆ›å»ºä¸€ä¸ªé“¾ï¼Œå®ç°ï¼š
- æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
- ç¿»è¯‘æˆè‹±æ–‡
- æ€»ç»“è‹±æ–‡å†…å®¹
- è¿”å›ä¸­æ–‡æ‘˜è¦

**ç»ƒä¹ 2**ï¼šåˆ›å»ºä¸€ä¸ªå¸¦è®°å¿†çš„"ä»£ç è§£é‡Šå™¨"ï¼š
- å¯ä»¥è¿ç»­å¯¹è¯
- è®°ä½ç”¨æˆ·æåˆ°çš„ä»£ç ç‰‡æ®µ
- æä¾›é€æ­¥è§£é‡Š

**ç»ƒä¹ 3**ï¼šå®ç°Router Chainï¼Œæ ¹æ®é—®é¢˜ç±»å‹è‡ªåŠ¨é€‰æ‹©ï¼š
- ä»£ç é—®é¢˜ â†’ ä½¿ç”¨æŠ€æœ¯ä¸“å®¶è§’è‰²
- åˆ›æ„é—®é¢˜ â†’ ä½¿ç”¨åˆ›æ„å†™æ‰‹è§’è‰²
- å…¶ä»– â†’ ä½¿ç”¨é€šç”¨åŠ©æ‰‹è§’è‰²

### è¿›é˜¶ç»ƒä¹ 

**æŒ‘æˆ˜1**ï¼šæ„å»º"å­¦ä¹ åŠ©æ‰‹"ç³»ç»Ÿ
åŠŸèƒ½ï¼š
- è¿½è¸ªå­¦ä¹ è¿›åº¦
- æ ¹æ®ç”¨æˆ·æ°´å¹³è°ƒæ•´å›ç­”æ·±åº¦
- ç”Ÿæˆç»ƒä¹ é¢˜
- æä¾›å­¦ä¹ å»ºè®®

**æŒ‘æˆ˜2**ï¼šå®ç°"å¤šè½®å¯¹è¯æ–‡æ¡ˆç”Ÿæˆå™¨"
åœºæ™¯ï¼š
- ç¬¬ä¸€è½®ï¼šäº†è§£äº§å“ç‰¹ç‚¹
- ç¬¬äºŒè½®ï¼šç¡®å®šç›®æ ‡å—ä¼—
- ç¬¬ä¸‰è½®ï¼šé€‰æ‹©æ–‡æ¡ˆé£æ ¼
- ç¬¬å››è½®ï¼šç”Ÿæˆå¤šä¸ªç‰ˆæœ¬

---

## å¸¸è§é—®é¢˜ FAQ

### Q1: LangChain å’Œç›´æ¥è°ƒç”¨ OpenAI API æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ

**A:**

```python
# ç›´æ¥è°ƒç”¨APIçš„é—®é¢˜
âŒ éœ€è¦æ‰‹åŠ¨ç®¡ç†å¯¹è¯å†å²
âŒ æç¤ºè¯å†™æ­»åœ¨ä»£ç é‡Œ
âŒ éš¾ä»¥ä¸²è”å¤šä¸ªæ­¥éª¤
âŒ æ²¡æœ‰æ ‡å‡†åŒ–çš„ç»„ä»¶

# LangChainçš„ä¼˜åŠ¿
âœ… ç»Ÿä¸€çš„æ¨¡å‹æ¥å£ï¼ˆåˆ‡æ¢æ¨¡å‹åªéœ€æ”¹ä¸€è¡Œä»£ç ï¼‰
âœ… æç¤ºè¯æ¨¡æ¿åŒ–ç®¡ç†
âœ… Chainè‡ªåŠ¨ä¸²è”å¤šä¸ªæ“ä½œ
âœ… Memoryè‡ªåŠ¨ç®¡ç†å¯¹è¯å†å²
âœ… ä¸°å¯Œçš„é¢„åˆ¶ç»„ä»¶å’Œé›†æˆ

# å¯¹æ¯”ç¤ºä¾‹
# ç›´æ¥è°ƒç”¨
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[...]  # éœ€è¦æ‰‹åŠ¨ç»´æŠ¤
)

# LangChain
chain = prompt | chat | parser  # ç®€æ´ä¼˜é›…
response = chain.invoke({"topic": "..."})
```

### Q2: LCELï¼ˆLangChain Expression Languageï¼‰æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆè¦ç”¨ï¼Ÿ

**A:**

```python
# LCELæ˜¯LangChainæ¨èçš„æ–°çš„é“¾å¼è¯­æ³•
# ç‰¹ç‚¹ï¼š
âœ… ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ | è¿æ¥ç»„ä»¶
âœ… ä»£ç æ›´ç®€æ´æ˜“è¯»
âœ… è‡ªåŠ¨æ”¯æŒæµå¼è¾“å‡º
âœ… å†…ç½®å¼‚æ­¥æ”¯æŒ
âœ… æ›´å¥½çš„é”™è¯¯å¤„ç†

# ä¼ ç»Ÿå†™æ³•ï¼ˆæ—§ç‰ˆï¼‰
from langchain.chains import LLMChain
chain = LLMChain(llm=chat, prompt=prompt)
result = chain.run(topic="Python")

# LCELå†™æ³•ï¼ˆæ¨èï¼‰
chain = prompt | chat | StrOutputParser()
result = chain.invoke({"topic": "Python"})

# LCELçš„ä¼˜åŠ¿åœ¨äºï¼š
# 1. å£°æ˜å¼ï¼Œä»£ç å³æ–‡æ¡£
# 2. ç»„åˆæ€§å¼ºï¼Œå¯ä»¥ä»»æ„è¿æ¥ç»„ä»¶
# 3. è‡ªåŠ¨ä¼˜åŒ–ï¼Œæ€§èƒ½æ›´å¥½
```

### Q3: ä»€ä¹ˆæ—¶å€™ç”¨ ConversationBufferMemoryï¼Œä»€ä¹ˆæ—¶å€™ç”¨ ConversationSummaryMemoryï¼Ÿ

**A:**

```python
âœ… ConversationBufferMemory é€‚åˆï¼š
- å¯¹è¯è½®æ¬¡ä¸å¤šï¼ˆ< 10è½®ï¼‰
- éœ€è¦è®°ä½å®Œæ•´å¯¹è¯å†…å®¹
- Tokené¢„ç®—å……è¶³

# ç¤ºä¾‹åœºæ™¯
- å®¢æœå¯¹è¯ï¼ˆçŸ­æœŸï¼‰
- é—®ç­”ç³»ç»Ÿï¼ˆä¼šè¯çŸ­ï¼‰

âŒ ä¸é€‚åˆï¼š
- é•¿å¯¹è¯ï¼ˆä¼šè¶…å‡ºTokené™åˆ¶ï¼‰
- æˆæœ¬æ•æ„Ÿçš„åº”ç”¨

---

âœ… ConversationSummaryMemory é€‚åˆï¼š
- å¯¹è¯è½®æ¬¡å¾ˆå¤šï¼ˆ> 10è½®ï¼‰
- åªéœ€è¦å…³é”®ä¿¡æ¯ï¼Œä¸éœ€è¦å®Œæ•´å¯¹è¯
- æƒ³èŠ‚çœTokenæˆæœ¬

# ç¤ºä¾‹åœºæ™¯
- é•¿æœŸå­¦ä¹ åŠ©æ‰‹
- å¥åº·å’¨è¯¢ï¼ˆéœ€è¦è®°ä½å†å²ä½†ä¸éœ€è¦é€å­—è®°å½•ï¼‰

âŒ ä¸é€‚åˆï¼š
- éœ€è¦ç²¾ç¡®å¼•ç”¨ä¹‹å‰çš„å¯¹è¯
- å¯¹è¯ç»†èŠ‚å¾ˆé‡è¦

---

âœ… ConversationBufferWindowMemory é€‚åˆï¼š
- åªå…³å¿ƒæœ€è¿‘å‡ è½®å¯¹è¯
- å¯¹è¯å¾ˆé•¿ä½†å†å²ä¸é‡è¦
- å¹³è¡¡è®°å¿†å’Œæˆæœ¬

# ç¤ºä¾‹åœºæ™¯
- å®æ—¶èŠå¤©æœºå™¨äºº
- çŸ­æœŸä»»åŠ¡åŠ©æ‰‹
```

### Q4: å¦‚ä½•åœ¨ Chain ä¸­ä¼ é€’å¤šä¸ªå˜é‡ï¼Ÿ

**A:**

```python
# æ–¹æ³•1ï¼šä½¿ç”¨å­—å…¸ä¼ é€’å¤šä¸ªå˜é‡
prompt = ChatPromptTemplate.from_template(
    "ä½ æ˜¯{role}ï¼Œç”¨{style}çš„é£æ ¼è§£é‡Š{topic}"
)

chain = prompt | chat | StrOutputParser()

result = chain.invoke({
    "role": "Pythonä¸“å®¶",
    "style": "å¹½é»˜",
    "topic": "è£…é¥°å™¨"
})

# æ–¹æ³•2ï¼šä½¿ç”¨RunnablePassthroughå¤„ç†
from langchain_core.runnables import RunnablePassthrough

chain = (
    RunnablePassthrough.assign(
        style=lambda x: x.get("style", "ä¸“ä¸š"),
        role=lambda x: x.get("role", "åŠ©æ‰‹")
    )
    | prompt
    | chat
    | StrOutputParser()
)

# æ–¹æ³•3ï¼šä½¿ç”¨éƒ¨åˆ†å˜é‡ï¼ˆpartial_variablesï¼‰
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯{role}åŠ©æ‰‹"),
    ("human", "{input}")
])

# é¢„è®¾role
prompt_partial = prompt.partial(role="Python")

# ä½¿ç”¨æ—¶åªéœ€ä¼ input
chain = prompt_partial | chat
result = chain.invoke({"input": "ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ"})
```

### Q5: OutputParser æœ‰å“ªäº›ç±»å‹ï¼Ÿå¦‚ä½•é€‰æ‹©ï¼Ÿ

**A:**

```python
# 1. StrOutputParser - æœ€ç®€å•
from langchain_core.output_parsers import StrOutputParser

chain = chat | StrOutputParser()
# é€‚ç”¨ï¼šåªéœ€è¦çº¯æ–‡æœ¬è¾“å‡º

# 2. JsonOutputParser - JSONæ ¼å¼
from langchain_core.output_parsers import JsonOutputParser

chain = prompt | chat | JsonOutputParser()
# é€‚ç”¨ï¼šéœ€è¦ç»“æ„åŒ–æ•°æ®ï¼Œä½†ä¸éœ€è¦ç±»å‹éªŒè¯

# 3. PydanticOutputParser - æ¨èï¼
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel

class Response(BaseModel):
    summary: str
    score: int

parser = PydanticOutputParser(pydantic_object=Response)
chain = prompt | chat | parser
# é€‚ç”¨ï¼š
# - éœ€è¦ä¸¥æ ¼çš„æ•°æ®ç±»å‹éªŒè¯
# - éœ€è¦è‡ªåŠ¨ç”ŸæˆJSON Schema
# - éœ€è¦æ›´å¥½çš„é”™è¯¯æç¤º

# 4. CommaSeparatedListOutputParser
from langchain_core.output_parsers import CommaSeparatedListOutputParser

parser = CommaSeparatedListOutputParser()
chain = prompt | chat | parser
# é€‚ç”¨ï¼šéœ€è¦åˆ—è¡¨è¾“å‡ºï¼ˆå¦‚ï¼šè‹¹æœ,é¦™è•‰,æ©™å­ï¼‰

# é€‰æ‹©å»ºè®®ï¼š
# ç®€å•æ–‡æœ¬ â†’ StrOutputParser
# ç®€å•JSON â†’ JsonOutputParser
# ç”Ÿäº§ç¯å¢ƒ â†’ PydanticOutputParserï¼ˆç±»å‹å®‰å…¨ï¼‰
# åˆ—è¡¨æ•°æ® â†’ CommaSeparatedListOutputParser
```

### Q6: å¦‚ä½•å®ç°æ¡ä»¶åˆ†æ”¯çš„ Chainï¼Ÿ

**A:**

```python
from langchain_core.runnables import RunnableBranch

# å®šä¹‰ä¸åŒçš„åˆ†æ”¯
code_chain = ChatPromptTemplate.from_template("å†™{language}ä»£ç ï¼š{input}") | chat
text_chain = ChatPromptTemplate.from_template("å†™æ–‡æœ¬ï¼š{input}") | chat
general_chain = ChatPromptTemplate.from_template("å›ç­”ï¼š{input}") | chat

# å®šä¹‰è·¯ç”±é€»è¾‘
branch = RunnableBranch(
    (lambda x: "ä»£ç " in x["input"], code_chain),
    (lambda x: "æ–‡æœ¬" in x["input"], text_chain),
    general_chain  # é»˜è®¤åˆ†æ”¯
)

# ä½¿ç”¨
result = branch.invoke({"input": "å†™Pythonä»£ç å®ç°å¿«é€Ÿæ’åº"})
# ä¼šè·¯ç”±åˆ°code_chain

# æˆ–è€…ä½¿ç”¨è‡ªå®šä¹‰è·¯ç”±å‡½æ•°
def route_func(inputs):
    query = inputs["input"].lower()
    if "ä»£ç " in query or "ç¼–ç¨‹" in query:
        return "code"
    elif "æ–‡ç« " in query or "æ–‡æ¡ˆ" in query:
        return "text"
    else:
        return "general"

# ç»“åˆRunnableLambda
from langchain_core.runnables import RunnableLambda

router = RunnableLambda(route_func).bind(
    code=code_chain,
    text=text_chain,
    general=general_chain
)
```

### Q7: å¦‚ä½•è°ƒè¯• Chainï¼Ÿçœ‹åˆ°ä¸­é—´æ­¥éª¤ï¼Ÿ

**A:**

```python
# æ–¹æ³•1ï¼šä½¿ç”¨verbose=True
from langchain.chains import ConversationChain

conversation = ConversationChain(
    llm=chat,
    memory=memory,
    verbose=True  # æ‰“å°è¯¦ç»†æ—¥å¿—
)

# æ–¹æ³•2ï¼šä½¿ç”¨å›è°ƒå‡½æ•°
from langchain.callbacks import StdOutCallbackHandler

handler = StdOutCallbackHandler()
response = chain.invoke(
    {"input": "ä½ å¥½"},
    config={"callbacks": [handler]}
)

# æ–¹æ³•3ï¼šä½¿ç”¨RunnablePassthroughæŸ¥çœ‹ä¸­é—´å€¼
from langchain_core.runnables import RunnablePassthrough

debug_chain = (
    RunnablePassthrough.assign(
        prompt_content=lambda x: print(f"ğŸ“ Prompt: {x}")
    )
    | prompt
    | RunnablePassthrough.assign(
        llm_output=lambda x: print(f"ğŸ¤– LLM Output: {x}")
    )
    | StrOutputParser()
)

# æ–¹æ³•4ï¼šä½¿ç”¨debugå…¨å±€è°ƒè¯•
from langchain.globals import debug

debug = True  # å¼€å¯å…¨å±€è°ƒè¯•
result = chain.invoke({"input": "æµ‹è¯•"})
debug = False  # å…³é—­è°ƒè¯•

# æ–¹æ³•5ï¼šä½¿ç”¨get_graph()å¯è§†åŒ–
chain.get_graph().print_ascii()
# ä¼šæ‰“å°å‡ºChainçš„ç»“æ„å›¾
```

### Q8: Memory ä¼šå ç”¨å¾ˆå¤š Token å—ï¼Ÿå¦‚ä½•ä¼˜åŒ–ï¼Ÿ

**A:**

```python
# é—®é¢˜ï¼šMemoryä¼šç´¯ç§¯å†å²ï¼Œå¯¼è‡´Tokenè¶Šæ¥è¶Šå¤š

âœ… ä¼˜åŒ–ç­–ç•¥ï¼š

# 1. ä½¿ç”¨WindowMemoryé™åˆ¶é•¿åº¦
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(k=3)
# åªä¿ç•™æœ€è¿‘3è½®å¯¹è¯

# 2. ä½¿ç”¨SummaryMemoryå‹ç¼©å†å²
from langchain.memory import ConversationSummaryMemory

memory = ConversationSummaryMemory(llm=chat, max_token_limit=500)
# è¶…è¿‡500 tokenå°±è‡ªåŠ¨æ‘˜è¦

# 3. ç»“åˆä½¿ç”¨
from langchain.memory import CombinedMemory

buffer_memory = ConversationBufferWindowMemory(k=2)
summary_memory = ConversationSummaryMemory(llm=chat)

memory = CombinedMemory(
    memories=[buffer_memory, summary_memory]
)

# 4. å®šæœŸæ¸…ç†
# ä¿å­˜é‡è¦ä¿¡æ¯åˆ°è‡ªå®šä¹‰å­˜å‚¨
def compress_memory(memory):
    # æå–å…³é”®ä¿¡æ¯
    history = memory.load_memory_variables({})
    # å­˜å‚¨åˆ°æ•°æ®åº“/æ–‡ä»¶
    save_to_db(history)
    # æ¸…ç©ºMemory
    memory.clear()

# 5. ä½¿ç”¨TokenBufferè‡ªåŠ¨é™åˆ¶
from langchain.memory import ConversationTokenBufferMemory

memory = ConversationTokenBufferMemory(
    llm=chat,
    max_token_limit=1000  # è¶…è¿‡1000å°±è‡ªåŠ¨åˆ é™¤æ—§å†…å®¹
)

# é€‰æ‹©å»ºè®®ï¼š
- çŸ­å¯¹è¯ â†’ BufferMemory
- é•¿å¯¹è¯ â†’ SummaryMemory æˆ– WindowMemory
- æˆæœ¬æ•æ„Ÿ â†’ TokenBuffer æˆ–å®šæœŸæ¸…ç†
```

### Q9: å¦‚ä½•å¤„ç† API è°ƒç”¨å¤±è´¥çš„æƒ…å†µï¼Ÿ

**A:**

```python
# æ–¹æ³•1ï¼šä½¿ç”¨tenacityè‡ªåŠ¨é‡è¯•
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),  # æœ€å¤šé‡è¯•3æ¬¡
    wait=wait_exponential(min=1, max=10)  # æŒ‡æ•°é€€é¿
)
def invoke_with_retry(chain, inputs):
    return chain.invoke(inputs)

# æ–¹æ³•2ï¼šä½¿ç”¨try-exceptåŒ…è£¹
def safe_invoke(chain, inputs, fallback="æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†é—®é¢˜"):
    try:
        return chain.invoke(inputs)
    except Exception as e:
        print(f"é”™è¯¯ï¼š{e}")
        return fallback

# æ–¹æ³•3ï¼šä½¿ç”¨RunnableParallelçš„fallback
from langchain_core.runnables import RunnableParallel

chain_with_fallback = (
    prompt | chat
).with_fallbacks(
    [backup_llm],  # å¤‡ç”¨æ¨¡å‹
    exceptions_to_handle=(Exception,)
)

# æ–¹æ³•4ï¼šä½¿ç”¨å›è°ƒç›‘æ§é”™è¯¯
from langchain.callbacks import BaseCallbackHandler

class ErrorHandler(BaseCallbackHandler):
    def on_llm_error(self, error, **kwargs):
        print(f"LLMé”™è¯¯: {error}")
        # è®°å½•æ—¥å¿—ã€å‘é€é€šçŸ¥ç­‰

chain.invoke(
    {"input": "æµ‹è¯•"},
    config={"callbacks": [ErrorHandler()]}
)
```

### Q10: LangChain å’Œ LlamaIndex æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿå¦‚ä½•é€‰æ‹©ï¼Ÿ

**A:**

```python
LangChain vs LlamaIndex å¯¹æ¯”ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç‰¹æ€§      â”‚   LangChain      â”‚   LlamaIndex     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å®šä½        â”‚ å…¨èƒ½LLMåº”ç”¨æ¡†æ¶  â”‚ ä¸“æ³¨äºRAG/æ•°æ®ç´¢å¼•â”‚
â”‚ æ“…é•¿        â”‚ Agent/Chain      â”‚ æ–‡æ¡£æ£€ç´¢/é—®ç­”    â”‚
â”‚ å­¦ä¹ æ›²çº¿    â”‚ ä¸­ç­‰             â”‚ è¾ƒä½             â”‚
â”‚ çµæ´»æ€§      â”‚ å¾ˆé«˜             â”‚ ä¸­ç­‰             â”‚
â”‚ æ–‡æ¡£è´¨é‡    â”‚ ä¼˜ç§€             â”‚ ä¼˜ç§€             â”‚
â”‚ ç¤¾åŒº        â”‚ åºå¤§             â”‚ æ´»è·ƒ             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ… ä½¿ç”¨ LangChain çš„åœºæ™¯ï¼š
- éœ€è¦æ„å»ºAgent
- å¤æ‚çš„å·¥ä½œæµï¼ˆå¤šæ­¥Chainï¼‰
- éœ€è¦å¤šç§å·¥å…·é›†æˆ
- éœ€è¦çµæ´»æ§åˆ¶
- ä¼ä¸šçº§åº”ç”¨

ç¤ºä¾‹ï¼š
- AIå®¢æœç³»ç»Ÿ
- ä»£ç ç”Ÿæˆå·¥å…·
- è‡ªåŠ¨åŒ–Agent

âœ… ä½¿ç”¨ LlamaIndex çš„åœºæ™¯ï¼š
- ä¸“æ³¨äºæ–‡æ¡£é—®ç­”ï¼ˆRAGï¼‰
- æ•°æ®ç´¢å¼•å’Œæ£€ç´¢
- çŸ¥è¯†åº“æ„å»º
- å¿«é€ŸåŸå‹å¼€å‘

ç¤ºä¾‹ï¼š
- æŠ€æœ¯æ–‡æ¡£é—®ç­”
- ä¼ä¸šçŸ¥è¯†åº“
- è®ºæ–‡æ£€ç´¢ç³»ç»Ÿ

âœ… åŒæ—¶ä½¿ç”¨ä¸¤è€…ï¼š
# LangChainå¤„ç†Agenté€»è¾‘
# LlamaIndexå¤„ç†æ•°æ®æ£€ç´¢

from llama_index import VectorStoreIndex
from langchain.agents import AgentExecutor

# ç”¨LlamaIndexæ„å»ºç´¢å¼•
index = VectorStoreIndex.from_documents(docs)

# ç”¨LangChainæ„å»ºAgent
agent = AgentExecutor(
    agent=agent,
    tools=[index.as_tool()]
)
```

---

## å­¦ä¹ æ¸…å•

æ£€æŸ¥ä½ æŒæ¡äº†ä»¥ä¸‹æŠ€èƒ½ï¼š

### åŸºç¡€æ¦‚å¿µ âœ…

- [ ] ç†è§£LangChainçš„å…­å¤§æ ¸å¿ƒæ¨¡å—
- [ ] çŸ¥é“LangChainè§£å†³äº†å“ªäº›é—®é¢˜
- [ ] èƒ½å¤Ÿå®‰è£…å’Œé…ç½®LangChainç¯å¢ƒ
- [ ] ç†è§£Model I/Oã€Chainsã€Memoryçš„ä½œç”¨

### Model I/O âœ…

- [ ] ä¼šä½¿ç”¨ChatOpenAIè°ƒç”¨æ¨¡å‹
- [ ] ç†è§£ä¸åŒMessageç±»å‹çš„ä½œç”¨
- [ ] ä¼šåˆ›å»ºChatPromptTemplateæ¨¡æ¿
- [ ] ä¼šä½¿ç”¨PromptTemplateç®¡ç†æç¤ºè¯
- [ ] ä¼šä½¿ç”¨StrOutputParserè§£ææ–‡æœ¬
- [ ] ä¼šä½¿ç”¨JsonOutputParserè§£æJSON
- [ ] ä¼šä½¿ç”¨PydanticOutputParserè¿›è¡Œç±»å‹å®‰å…¨è§£æ

### Chains âœ…

- [ ] ç†è§£Chainçš„æ¦‚å¿µå’Œä½œç”¨
- [ ] ä¼šåˆ›å»ºLLMChainåŸºç¡€é“¾
- [ ] ä¼šä½¿ç”¨SequentialChainä¸²è”å¤šä¸ªæ­¥éª¤
- [ ] ç†è§£å¹¶ä¼šä½¿ç”¨LCELè¯­æ³•ï¼ˆ| æ“ä½œç¬¦ï¼‰
- [ ] ä¼šä½¿ç”¨RunnablePassthroughå¤„ç†æ•°æ®
- [ ] ä¼šåˆ›å»ºRouterChainå®ç°æ¡ä»¶åˆ†æ”¯
- [ ] èƒ½å¤Ÿæ„å»ºå¤æ‚çš„Chainå·¥ä½œæµ

### Memory âœ…

- [ ] ç†è§£Memoryçš„å¿…è¦æ€§
- [ ] ä¼šä½¿ç”¨ConversationBufferMemoryä¿å­˜å®Œæ•´å¯¹è¯
- [ ] ä¼šä½¿ç”¨ConversationBufferWindowMemoryé™åˆ¶é•¿åº¦
- [ ] ä¼šä½¿ç”¨ConversationSummaryMemoryå‹ç¼©å†å²
- [ ] çŸ¥é“ä¸åŒMemoryç±»å‹çš„é€‚ç”¨åœºæ™¯
- [ ] ä¼šåœ¨LCELä¸­ä½¿ç”¨Memory
- [ ] ä¼šä¿å­˜å’ŒåŠ è½½Memory

### å®æˆ˜èƒ½åŠ› âœ…

- [ ] èƒ½å¤Ÿç‹¬ç«‹æ„å»ºæ™ºèƒ½é—®ç­”ç³»ç»Ÿ
- [ ] èƒ½å¤Ÿå®ç°å¤šè½®å¯¹è¯
- [ ] èƒ½å¤Ÿå¤„ç†APIè°ƒç”¨å¤±è´¥
- [ ] çŸ¥é“å¦‚ä½•è°ƒè¯•Chain
- [ ] ç†è§£Tokenæˆæœ¬ä¼˜åŒ–æ–¹æ³•
- [ ] èƒ½å¤Ÿè®¾è®¡åˆç†çš„æç¤ºè¯æ¨¡æ¿

### æœ€ä½³å®è·µ âœ…

- [ ] çŸ¥é“å¦‚ä½•ç»„ç»‡æç¤ºè¯æ¨¡æ¿
- [ ] ç†è§£é”™è¯¯å¤„ç†å’Œé‡è¯•ç­–ç•¥
- [ ] ä¼šä½¿ç”¨ç¼“å­˜ä¼˜åŒ–æˆæœ¬
- [ ] çŸ¥é“å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹
- [ ] ç†è§£LCEL vs ä¼ ç»ŸChainçš„åŒºåˆ«
- [ ] èƒ½å¤Ÿé˜…è¯»å’Œç†è§£LangChainæ–‡æ¡£

---

## æ‹“å±•ç»ƒä¹ 

### ç»ƒä¹ 1ï¼šæ„å»ºå¤šè§’è‰²å¯¹è¯ç³»ç»Ÿ

**è¦æ±‚**ï¼š
- å®ç°3ä¸ªä¸åŒæ€§æ ¼çš„AIè§’è‰²
- ç”¨æˆ·å¯ä»¥é€‰æ‹©ä¸å“ªä¸ªè§’è‰²å¯¹è¯
- æ¯ä¸ªè§’è‰²æœ‰ç‹¬ç«‹çš„Memory
- è§’è‰²ä¹‹é—´å¯ä»¥ç›¸äº’å¼•ç”¨

```python
# æç¤ºï¼š
# 1. ä¸ºæ¯ä¸ªè§’è‰²åˆ›å»ºç‹¬ç«‹çš„Chainå’ŒMemory
# 2. ä½¿ç”¨RouterChainæ ¹æ®ç”¨æˆ·é€‰æ‹©è·¯ç”±
# 3. åœ¨æç¤ºè¯ä¸­å®šä¹‰è§’è‰²ç‰¹ç‚¹
```

### ç»ƒä¹ 2ï¼šå®ç°å¸¦çŸ¥è¯†åº“çš„é—®ç­”ç³»ç»Ÿ

**è¦æ±‚**ï¼š
- ç»´æŠ¤ä¸€ä¸ªçŸ¥è¯†å­—å…¸ï¼ˆå¦‚ï¼šè¯¾ç¨‹ä¿¡æ¯ã€FAQï¼‰
- ç”¨æˆ·æé—®æ—¶å…ˆæŸ¥çŸ¥è¯†åº“
- çŸ¥è¯†åº“æ²¡æœ‰ç­”æ¡ˆå†ç”¨LLMå›ç­”
- è®°å½•æ‰€æœ‰é—®ç­”å†å²

```python
# æç¤ºï¼š
# 1. ä½¿ç”¨RouterChainå®ç°é€»è¾‘åˆ†æ”¯
# 2. ç¬¬ä¸€ä¸ªåˆ†æ”¯ï¼šæŸ¥çŸ¥è¯†åº“
# 3. ç¬¬äºŒä¸ªåˆ†æ”¯ï¼šLLMå›ç­”
# 4. ä½¿ç”¨Memoryä¿å­˜å†å²
```

### ç»ƒä¹ 3ï¼šæ„å»ºä»£ç å®¡æŸ¥åŠ©æ‰‹

**è¦æ±‚**ï¼š
- æ¥æ”¶ä»£ç å’Œé—®é¢˜æè¿°
- é€æ­¥åˆ†æä»£ç é—®é¢˜
- ç»™å‡ºä¿®æ”¹å»ºè®®å’Œç¤ºä¾‹
- æ”¯æŒå¤šè½®å¯¹è¯è¿½é—®

```python
# æç¤ºï¼š
# 1. ä½¿ç”¨SequentialChainåˆ†æ­¥éª¤åˆ†æ
# 2. æ­¥éª¤1ï¼šç†è§£ä»£ç ç»“æ„
# 3. æ­¥éª¤2ï¼šè¯†åˆ«é—®é¢˜
# 4. æ­¥éª¤3ï¼šç”Ÿæˆå»ºè®®
# 5. ä½¿ç”¨Memoryæ”¯æŒè¿½é—®
```

### ç»ƒä¹ 4ï¼šå®ç°å¤šè¯­è¨€ç¿»è¯‘ç³»ç»Ÿ

**è¦æ±‚**ï¼š
- æ”¯æŒä¸­è‹±æ—¥éŸ©å››ç§è¯­è¨€äº’è¯‘
- è‡ªåŠ¨æ£€æµ‹æºè¯­è¨€
- ä¿æŒä¸Šä¸‹æ–‡ä¸€è‡´æ€§
- å¯ä»¥è°ƒæ•´ç¿»è¯‘é£æ ¼ï¼ˆæ­£å¼/å£è¯­ï¼‰

```python
# æç¤ºï¼š
# 1. ä½¿ç”¨RouterChainæ ¹æ®ç›®æ ‡è¯­è¨€é€‰æ‹©
# 2. æç¤ºè¯ä¸­å®šä¹‰è¯­è¨€ç‰¹å¾
# 3. ä½¿ç”¨Memoryä¿æŒç¿»è¯‘ä¸Šä¸‹æ–‡
# 4. æ·»åŠ é£æ ¼å‚æ•°
```

### ç»ƒä¹ 5ï¼šæ„å»ºå­¦ä¹ è¿›åº¦è¿½è¸ªç³»ç»Ÿ

**è¦æ±‚**ï¼š
- è®°å½•ç”¨æˆ·å­¦ä¹ è¿‡çš„ä¸»é¢˜
- æ ¹æ®å†å²è°ƒæ•´å›ç­”æ·±åº¦
- ç”Ÿæˆä¸ªæ€§åŒ–çš„å­¦ä¹ å»ºè®®
- æä¾›ç›¸å…³ç»ƒä¹ é¢˜

```python
# æç¤ºï¼š
# 1. ä½¿ç”¨SummaryMemoryæ€»ç»“å­¦ä¹ å†…å®¹
# 2. åœ¨æç¤ºè¯ä¸­æ³¨å…¥å­¦ä¹ å†å²
# 3. æ ¹æ®ç”¨æˆ·æ°´å¹³è°ƒæ•´æç¤ºè¯
# 4. ä½¿ç”¨Chainç”Ÿæˆç»ƒä¹ é¢˜
```

---

## è¿›é˜¶å­¦ä¹ æ–¹å‘

æ­å–œä½ å®Œæˆ LangChain åŸºç¡€å­¦ä¹ ï¼æ¥ä¸‹æ¥å¯ä»¥ï¼š

### 1. æ·±å…¥ LangChain

**RAGï¼ˆæ£€ç´¢å¢å¼ºç”Ÿæˆï¼‰**ï¼š
- Document Loadersï¼šåŠ è½½å„ç§æ–‡æ¡£
- Text Splittersï¼šæ™ºèƒ½åˆ†å‰²æ–‡æœ¬
- Vector Storesï¼šå‘é‡æ•°æ®åº“
- Retrieversï¼šæ£€ç´¢å™¨

**Agentsï¼ˆæ™ºèƒ½ä»£ç†ï¼‰**ï¼š
- Toolsï¼šè‡ªå®šä¹‰å·¥å…·
- Agent Typesï¼šReActã€Self-Ask
- Agent Executorsï¼šæ‰§è¡Œå™¨
- è®¡åˆ’å’Œæ¨ç†

**LangChain ç”Ÿæ€**ï¼š
- LangServeï¼šéƒ¨ç½²æœåŠ¡
- LangSmithï¼šè°ƒè¯•å’Œç›‘æ§
- Templatesï¼šé¢„åˆ¶æ¨¡æ¿

### 2. å®è·µé¡¹ç›®

```bash
âœ… åŠ¨æ‰‹å®è·µï¼š
1. æ„å»ºä¸ªäººçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ
2. å¼€å‘AIå®¢æœæœºå™¨äºº
3. åˆ›å»ºä»£ç åŠ©æ‰‹Agent
4. å®ç°æ–‡æ¡£åˆ†æå·¥å…·
5. å¼€å‘è‡ªåŠ¨åŒ–å·¥ä½œæµ
```

### 3. å­¦ä¹ èµ„æº

- **LangChain å®˜æ–¹æ–‡æ¡£**ï¼šhttps://python.langchain.com
- **LangChain GitHub**ï¼šhttps://github.com/langchain-ai/langchain
- **LangSmith**ï¼šhttps://smith.langchain.comï¼ˆè°ƒè¯•å¹³å°ï¼‰
- **ç¤ºä¾‹é¡¹ç›®**ï¼šhttps://github.com/langchain-ai/langchain/tree/master/cookbook

### 4. ç¤¾åŒºå’Œèµ„è®¯

- **Discord ç¤¾åŒº**ï¼šåŠ å…¥ LangChain Discord
- **Twitter**ï¼šå…³æ³¨ @langchainai
- **ä¸­æ–‡ç¤¾åŒº**ï¼šå¾®ä¿¡å…¬ä¼—å·ã€çŸ¥ä¹ä¸“æ 
- **å®æˆ˜æ¡ˆä¾‹**ï¼šå­¦ä¹ å¼€æºé¡¹ç›®

### 5. æœ€ä½³å®è·µ

```python
âœ… å¼€å‘å»ºè®®ï¼š
1. ä»ç®€å•å¼€å§‹ï¼Œé€æ­¥å¢åŠ å¤æ‚åº¦
2. é‡è§†æç¤ºè¯è®¾è®¡
3. ä½¿ç”¨ Memory ç®¡ç†å¯¹è¯
4. å–„ç”¨ LCEL ç®€åŒ–ä»£ç 
5. æ³¨æ„ Token æˆæœ¬ä¼˜åŒ–
6. åšå¥½é”™è¯¯å¤„ç†
7. ä½¿ç”¨ LangSmith è°ƒè¯•
8. ç¼–å†™æµ‹è¯•ä¿è¯è´¨é‡
```

**ä¸‹ä¸€ç« é¢„å‘Š**ï¼šPrompt Engineering - æŒæ¡æç¤ºè¯çš„é«˜çº§æŠ€å·§ï¼

---

## 2024-2026æ–°å¢ï¼šLangGraphåŸºç¡€

### ä»€ä¹ˆæ˜¯LangGraphï¼Ÿ

**LangGraph** æ˜¯LangChainå®˜æ–¹åœ¨2024å¹´æ¨å‡ºçš„æ–°åº“ï¼Œä¸“é—¨ç”¨äºæ„å»º**æœ‰çŠ¶æ€çš„ã€å¤šæ­¥éª¤çš„Agentåº”ç”¨**ã€‚

```
LangChain vs LangGraphï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç‰¹æ€§      â”‚   LangChain      â”‚   LangGraph      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ é€‚ç”¨åœºæ™¯    â”‚ çº¿æ€§/ç®€å•å·¥ä½œæµ  â”‚ å¤æ‚çŠ¶æ€æœº      â”‚
â”‚ çŠ¶æ€ç®¡ç†    â”‚ æ‰‹åŠ¨            â”‚ å†…ç½®çŠ¶æ€ç®¡ç†     â”‚
â”‚ å¾ªç¯/åˆ†æ”¯   â”‚ å›°éš¾            â”‚ åŸç”Ÿæ”¯æŒ         â”‚
â”‚ Agentç¼–æ’   â”‚ åŸºç¡€Agent       â”‚ Multi-Agentç³»ç»Ÿ  â”‚
â”‚ å¯è§†åŒ–      â”‚ æ—               â”‚ è‡ªåŠ¨ç”Ÿæˆå›¾       â”‚
â”‚ å­¦ä¹ æ›²çº¿    â”‚ ä½              â”‚ ä¸­ç­‰             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å¿«é€Ÿå¼€å§‹

```python
# å®‰è£… LangGraph
pip install langgraph==0.2.0

# ç®€å•ç¤ºä¾‹ï¼šçŠ¶æ€å›¾
from langgraph.graph import StateGraph, END
from typing import TypedDict

# 1. å®šä¹‰çŠ¶æ€
class GraphState(TypedDict):
    input: str
    intermediate: str
    output: str

# 2. å®šä¹‰èŠ‚ç‚¹å‡½æ•°
def node_a(state: GraphState) -> GraphState:
    """èŠ‚ç‚¹Aï¼šå¤„ç†è¾“å…¥"""
    return {
        **state,
        "intermediate": f"å¤„ç†è¿‡: {state['input']}"
    }

def node_b(state: GraphState) -> GraphState:
    """èŠ‚ç‚¹Bï¼šç”Ÿæˆè¾“å‡º"""
    return {
        **state,
        "output": f"æœ€ç»ˆç»“æœ: {state['intermediate']}"
    }

# 3. æ„å»ºå›¾
workflow = StateGraph(GraphState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("process", node_a)
workflow.add_node("finalize", node_b)

# æ·»åŠ è¾¹
workflow.set_entry_point("process")
workflow.add_edge("process", "finalize")
workflow.add_edge("finalize", END)

# 4. ç¼–è¯‘å›¾
app = workflow.compile()

# 5. è¿è¡Œ
result = app.invoke({"input": "Hello LangGraph"})
print(result)
# {'input': 'Hello LangGraph', 'intermediate': 'å¤„ç†è¿‡: Hello LangGraph', 'output': 'æœ€ç»ˆç»“æœ: å¤„ç†è¿‡: Hello LangGraph'}
```

### LangGraph vs ä¼ ç»ŸChain

```python
# ä¼ ç»ŸChain - çº¿æ€§æµç¨‹
chain = (
    prompt
    | llm
    | parser
)
result = chain.invoke(input)

# LangGraph - å¤æ‚çŠ¶æ€æµç¨‹
workflow = StateGraph(State)
workflow.add_node("step1", step1_func)
workflow.add_node("step2", step2_func)
workflow.add_conditional_edges(
    "step1",
    should_continue,  # æ¡ä»¶å‡½æ•°
    {
        "continue": "step2",
        "end": END
    }
)
app = workflow.compile()
result = app.invoke(initial_state)
```

**ä»€ä¹ˆæ—¶å€™ç”¨LangGraphï¼Ÿ**
- âœ… éœ€è¦å¾ªç¯å’Œæ¡ä»¶åˆ†æ”¯
- âœ… å¤šAgentåä½œ
- âœ… éœ€è¦ç»´æŠ¤å¤æ‚çŠ¶æ€
- âœ… éœ€è¦å¯è§†åŒ–å’Œè°ƒè¯•å·¥ä½œæµ

**æ›´å¤šLangGraphå†…å®¹è§ [Chapter 05: AI Agent](chapter-05)**

---

**é‡åˆ°é—®é¢˜ï¼Ÿ**
- æŸ¥çœ‹ä»£ç ç¤ºä¾‹ï¼š`examples/` ç›®å½•
- å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼šhttps://python.langchain.com
- LangGraphæ–‡æ¡£ï¼šhttps://langchain-ai.github.io/langgraph
- å‘é‚®ä»¶æ±‚åŠ©ï¼šesimonx@163.com

**ä¸‹ä¸€ç« ï¼š[Prompt Engineering â†’](chapter-03)**
