# ç¬¬2ç« ï¼šLangChainæ¡†æ¶å…¥é—¨

## æœ¬ç« å¯¼è¯»

åœ¨ç¬¬1ç« ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•ç›´æ¥è°ƒç”¨OpenAI APIæ„å»ºç®€å•çš„AIåº”ç”¨ã€‚ä½†éšç€åº”ç”¨å˜å¾—å¤æ‚ï¼Œç›´æ¥ä½¿ç”¨APIä¼šé‡åˆ°å¾ˆå¤šæŒ‘æˆ˜ï¼šå¦‚ä½•ç®¡ç†å¯¹è¯å†å²ï¼Ÿå¦‚ä½•ä¸²è”å¤šä¸ªæ­¥éª¤ï¼Ÿå¦‚ä½•é›†æˆå¤–éƒ¨æ•°æ®ï¼Ÿ

**LangChain** æ˜¯ç›®å‰æœ€æµè¡Œçš„LLMåº”ç”¨å¼€å‘æ¡†æ¶ï¼Œå®ƒä¸ºæˆ‘ä»¬æä¾›äº†ä¸€å¥—å®Œæ•´çš„å·¥å…·é“¾ï¼Œå¤§å¤§ç®€åŒ–äº†LLMåº”ç”¨çš„å¼€å‘ã€‚

**å­¦ä¹ ç›®æ ‡**ï¼š
- ç†è§£LangChainçš„æ ¸å¿ƒæ¦‚å¿µå’Œæ¶æ„
- æŒæ¡Model I/Oï¼šæ¨¡å‹ã€æç¤ºè¯ã€è¾“å‡ºè§£æ
- å­¦ä¹ Chainsï¼šä¸²è”å¤šä¸ªæ“ä½œ
- å®ç°Memoryï¼šå¯¹è¯è®°å¿†ç®¡ç†

**é¢„è®¡å­¦ä¹ æ—¶é—´**ï¼š60åˆ†é’Ÿ

---

## 2.1 ä»€ä¹ˆæ˜¯LangChainï¼Ÿ

### 2.1.1 LangChainç®€ä»‹

**LangChain** æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œä¸“é—¨ç”¨äºå¼€å‘åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„åº”ç”¨ã€‚å®ƒæä¾›äº†ä¸€ç³»åˆ—æ¨¡å—åŒ–çš„ç»„ä»¶ï¼Œè®©ä½ å¯ä»¥è½»æ¾æ„å»ºå¤æ‚çš„AIåº”ç”¨ã€‚

**æ ¸å¿ƒä»·å€¼**ï¼š

```
ç›´æ¥ä½¿ç”¨APIçš„é—®é¢˜ï¼š
  âŒ æç¤ºè¯ç®¡ç†æ··ä¹±
  âŒ éš¾ä»¥ä¸²è”å¤šä¸ªæ­¥éª¤
  âŒ å¯¹è¯å†å²éœ€è¦æ‰‹åŠ¨ç»´æŠ¤
  âŒ éš¾ä»¥é›†æˆå¤–éƒ¨æ•°æ®
  âŒ ç¼ºä¹æ ‡å‡†åŒ–ç»„ä»¶

LangChainçš„è§£å†³æ–¹æ¡ˆï¼š
  âœ… æç¤ºè¯æ¨¡æ¿åŒ–ç®¡ç†
  âœ… Chainé“¾å¼è°ƒç”¨
  âœ… Memoryè‡ªåŠ¨ç®¡ç†
  âœ… Data Connectioné›†æˆ
  âœ… ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿ
```

### 2.1.2 LangChainçš„å…­å¤§æ ¸å¿ƒæ¨¡å—

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LangChain æ¶æ„å›¾                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                              â”‚
â”‚  ğŸ“¦ Model I/O      æ¨¡å‹è¾“å…¥è¾“å‡º                â”‚
â”‚    â”œâ”€â”€ Models    LLMåŒ…è£…å’Œç»Ÿä¸€æ¥å£            â”‚
â”‚    â”œâ”€â”€ Prompts   æç¤ºè¯æ¨¡æ¿                   â”‚
â”‚    â””â”€â”€ Parsers   è¾“å‡ºè§£æå™¨                   â”‚
â”‚                                              â”‚
â”‚  ğŸ”— Chains        é“¾å¼è°ƒç”¨                    â”‚
â”‚    â”œâ”€â”€ LLMChain  åŸºç¡€é“¾                      â”‚
â”‚    â”œâ”€â”€ Sequential é¡ºåºé“¾                     â”‚
â”‚    â””â”€â”€ Router    è·¯ç”±é“¾                      â”‚
â”‚                                              â”‚
â”‚  ğŸ§  Memory        è®°å¿†ç®¡ç†                    â”‚
â”‚    â”œâ”€â”€ Buffer    ç¼“å†²è®°å¿†                    â”‚
â”‚    â”œâ”€â”€ Summary   æ‘˜è¦è®°å¿†                    â”‚
â”‚    â””â”€â”€ Knowledge çŸ¥è¯†å›¾è°±è®°å¿†                 â”‚
â”‚                                              â”‚
â”‚  ğŸ“š Retrieval     æ£€ç´¢ç”Ÿæˆ                    â”‚
â”‚    â”œâ”€â”€ Loaders   æ–‡æ¡£åŠ è½½å™¨                   â”‚
â”‚    â”œâ”€â”€ Splitters æ–‡æœ¬åˆ†å‰²å™¨                  â”‚
â”‚    â”œâ”€â”€ Embeddings å‘é‡åŒ–                    â”‚
â”‚    â””â”€â”€ VectorStores å‘é‡æ•°æ®åº“               â”‚
â”‚                                              â”‚
â”‚  ğŸ¤– Agents        æ™ºèƒ½ä»£ç†                    â”‚
â”‚    â”œâ”€â”€ Tools     å·¥å…·å®šä¹‰                    â”‚
â”‚    â”œâ”€â”€ Agent     ä»£ç†ç±»å‹                    â”‚
â”‚    â””â”€â”€ Executor  æ‰§è¡Œå™¨                      â”‚
â”‚                                              â”‚
â”‚  ğŸ”§ Chains        å·¥å…·å’Œå›è°ƒ                  â”‚
â”‚    â””â”€â”€ Callbacks è§‚å¯Ÿå’Œç›‘å¬                   â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.1.3 å®‰è£…LangChain

```bash
# æ ¸å¿ƒåŒ…
pip install langchain

# OpenAIé›†æˆ
pip install langchain-openai

# å…¶ä»–å¸¸ç”¨åŒ…
pip install langchain-community  # ç¤¾åŒºæ‰©å±•
pip install chromadb              # å‘é‡æ•°æ®åº“
pip install faiss-cpu             # å‘é‡æœç´¢

# å®Œæ•´ä¾èµ–
pip install langchain langchain-openai langchain-community chromadb
```

---

## 2.2 Model I/Oï¼šæ¨¡å‹è¾“å…¥è¾“å‡º

Model I/Oæ˜¯LangChainæœ€åŸºç¡€çš„æ¨¡å—ï¼ŒåŒ…å«ä¸‰ä¸ªæ ¸å¿ƒç»„ä»¶ï¼š
- **Models**ï¼šä¸å¤§è¯­è¨€æ¨¡å‹äº¤äº’
- **Prompts**ï¼šç®¡ç†æç¤ºè¯
- **Output Parsers**ï¼šè§£ææ¨¡å‹è¾“å‡º

### 2.2.1 Modelsï¼šæ¨¡å‹æ¥å£

LangChainæä¾›äº†ç»Ÿä¸€çš„æ¨¡å‹æ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAI
from config import Config

# 1. Chatæ¨¡å‹ï¼ˆæ¨èï¼‰
chat = ChatOpenAI(
    model="gpt-3.5-turbo",
    api_key=Config.OPENAI_API_KEY,
    temperature=0.7,
    max_tokens=1000
)

# 2. Completionæ¨¡å‹ï¼ˆæ—§ç‰ˆï¼‰
llm = OpenAI(
    model="gpt-3.5-turbo-instruct",
    api_key=Config.OPENAI_API_KEY
)

# ä½¿ç”¨ç¤ºä¾‹
message = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
response = chat.invoke(message)
print(response.content)  # AIçš„å›å¤
```

**Messageç±»å‹**ï¼š

```python
from langchain_core.messages import (
    HumanMessage,      # äººç±»æ¶ˆæ¯
    AIMessage,         # AIæ¶ˆæ¯
    SystemMessage,     # ç³»ç»Ÿæ¶ˆæ¯
    ToolMessage        # å·¥å…·æ¶ˆæ¯
)

# æ„å»ºæ¶ˆæ¯åˆ—è¡¨
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„Pythonå¯¼å¸ˆ"),
    HumanMessage(content="ä»€ä¹ˆæ˜¯è£…é¥°å™¨ï¼Ÿ")
]

response = chat.invoke(messages)
print(response.content)
```

### 2.2.2 Promptsï¼šæç¤ºè¯æ¨¡æ¿

#### åŸºç¡€æç¤ºè¯æ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate

# æ–¹å¼1ï¼šä½¿ç”¨PromptTemplate
from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate(
    template="è¯·è§£é‡Š{concept}è¿™ä¸ªæ¦‚å¿µï¼Œç”¨{style}çš„é£æ ¼",
    input_variables=["concept", "style"]
)

# å¡«å……æ¨¡æ¿
formatted = prompt.format(
    concept="æœºå™¨å­¦ä¹ ",
    style="é€šä¿—æ˜“æ‡‚"
)
print(formatted)
# è¾“å‡ºï¼šè¯·è§£é‡Šæœºå™¨å­¦ä¹ è¿™ä¸ªæ¦‚å¿µï¼Œç”¨é€šä¿—æ˜“æ‡‚çš„é£æ ¼
```

#### Chatæç¤ºè¯æ¨¡æ¿

```python
from langchain_core.prompts import ChatPromptTemplate

# åˆ›å»ºæ¨¡æ¿
prompt_template = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{role}åŠ©æ‰‹ã€‚"),
    ("human", "{user_input}")
])

# å¡«å……å¹¶è°ƒç”¨
prompt = prompt_template.format_messages(
    role="Pythonç¼–ç¨‹",
    user_input="ä»€ä¹ˆæ˜¯åˆ—è¡¨æ¨å¯¼å¼ï¼Ÿ"
)

response = chat.invoke(prompt)
print(response.content)
```

#### å®ç”¨çš„æç¤ºè¯æ¨¡æ¿

```python
# 1. ä»£ç ç”Ÿæˆæ¨¡æ¿
code_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ª{language}ç¼–ç¨‹ä¸“å®¶ã€‚"),
    ("human", """è¯·å¸®æˆ‘å®ç°ä»¥ä¸‹åŠŸèƒ½ï¼š
    åŠŸèƒ½æè¿°ï¼š{task}
    è¦æ±‚ï¼š
    - ä»£ç è¦æœ‰æ³¨é‡Š
    - åŒ…å«ä½¿ç”¨ç¤ºä¾‹
    - è§£é‡Šæ—¶é—´å¤æ‚åº¦
    """)
])

# 2. æ–‡æ¡£åˆ†ææ¨¡æ¿
doc_prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªæ–‡æ¡£åˆ†æå¸ˆã€‚"),
    ("human", """è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£ï¼š
    {document}

    ä»»åŠ¡ï¼š{task}
    è¾“å‡ºæ ¼å¼ï¼š{format}
    """)
])

# 3. è§’è‰²æ‰®æ¼”æ¨¡æ¿
role_prompt = ChatPromptTemplate.from_messages([
    ("system", """ä½ ç°åœ¨æ˜¯{role}ã€‚
    ä½ çš„ç‰¹ç‚¹æ˜¯ï¼š{characteristics}
    ä½ çš„è¯´è¯é£æ ¼ï¼š{style}"""),
    ("human", "{question}")
])
```

### 2.2.3 Output Parsersï¼šè¾“å‡ºè§£æå™¨

è¾“å‡ºè§£æå™¨è®©LLMçš„è¾“å‡ºæ›´ç»“æ„åŒ–ã€‚

#### åŸºç¡€è§£æå™¨

```python
from langchain_core.output_parsers import StrOutputParser

# åˆ›å»ºè§£æå™¨
output_parser = StrOutputParser()

# ä½¿ç”¨é“¾å¼è°ƒç”¨
chain = chat | output_parser  # | æ˜¯ç®¡é“æ“ä½œç¬¦

response = chain.invoke("ä»€ä¹ˆæ˜¯Pythonï¼Ÿ")
print(response)  # ç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼Œæ— éœ€.content
```

#### ç»“æ„åŒ–è¾“å‡ºè§£æå™¨

```python
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import PromptTemplate

# 1. å®šä¹‰æœŸæœ›çš„è¾“å‡ºæ ¼å¼
format_instructions = """
è¯·ä»¥JSONæ ¼å¼å›å¤ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
    "summary": "ç®€è¦æ€»ç»“",
    "key_points": ["è¦ç‚¹1", "è¦ç‚¹2"],
    "difficulty": "éš¾åº¦ç­‰çº§(1-5)"
}
"""

# 2. åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt = PromptTemplate(
    template="""è¯·åˆ†æä»¥ä¸‹ä¸»é¢˜ï¼š{topic}

    {format_instructions}
    """,
    input_variables=["topic"],
    partial_variables={"format_instructions": format_instructions}
)

# 3. åˆ›å»ºè§£æå™¨å’Œé“¾
parser = JsonOutputParser()
chain = prompt | chat | parser

# 4. è°ƒç”¨
result = chain.invoke({"topic": "Pythonè£…é¥°å™¨"})
print(result)
# è¾“å‡ºï¼š{'summary': '...', 'key_points': [...], 'difficulty': 3}
```

#### Pydanticè§£æå™¨ï¼ˆæ¨èï¼‰

```python
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

# 1. å®šä¹‰æ•°æ®æ¨¡å‹
class CodeAnalysis(BaseModel):
    """ä»£ç åˆ†æç»“æœ"""
    language: str = Field(description="ç¼–ç¨‹è¯­è¨€")
    time_complexity: str = Field(description="æ—¶é—´å¤æ‚åº¦")
    has_issues: bool = Field(description="æ˜¯å¦å­˜åœ¨é—®é¢˜")
    suggestions: list[str] = Field(description="æ”¹è¿›å»ºè®®")

# 2. åˆ›å»ºè§£æå™¨
parser = PydanticOutputParser(pydantic_object=CodeAnalysis)

# 3. è·å–æ ¼å¼è¯´æ˜
format_instructions = parser.get_format_instructions()

# 4. åˆ›å»ºæç¤ºè¯
prompt = PromptTemplate(
    template="""åˆ†æä»¥ä¸‹ä»£ç ï¼š
    {code}

    {format_instructions}
    """,
    input_variables=["code"],
    partial_variables={"format_instructions": format_instructions}
)

# 5. åˆ›å»ºé“¾
chain = prompt | chat | parser

# 6. è°ƒç”¨
result = chain.invoke({"code": "def foo(): return [i**2 for i in range(1000000)]"})
print(result)
# CodeAnalysis(language='Python', time_complexity='O(n)', has_issues=False, suggestions=[...])
```

---

## 2.3 Chainsï¼šé“¾å¼è°ƒç”¨

Chainså…è®¸ä½ å°†å¤šä¸ªç»„ä»¶ä¸²è”èµ·æ¥ï¼Œæ„å»ºå¤æ‚çš„å·¥ä½œæµã€‚

### 2.3.1 LLMChainï¼šæœ€åŸºæœ¬çš„é“¾

```python
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate

# åˆ›å»ºæç¤ºè¯æ¨¡æ¿
prompt = PromptTemplate(
    input_variables=["product"],
    template="ä¸º{product}äº§å“å†™ä¸€ä¸ªå¸å¼•äººçš„å¹¿å‘Šè¯­ï¼Œä¸è¶…è¿‡20å­—ã€‚"
)

# åˆ›å»ºé“¾
chain = LLMChain(
    llm=chat,
    prompt=prompt
)

# è°ƒç”¨é“¾
result = chain.run(product="æ™ºèƒ½æ‰‹è¡¨")
print(result)  # "è…•é—´æ™ºæ…§ï¼Œå¥åº·éšè¡Œï¼"

# ä¹Ÿå¯ä»¥ç”¨invokeæ–¹å¼
result = chain.invoke({"product": "æ™ºèƒ½æ‰‹è¡¨"})
print(result['text'])
```

### 2.3.2 Sequential Chainï¼šé¡ºåºé“¾

æŒ‰é¡ºåºæ‰§è¡Œå¤šä¸ªé“¾ã€‚

```python
from langchain.chains import SequentialChain

# Chain 1: ç”Ÿæˆæ•…äº‹å¤§çº²
outline_prompt = PromptTemplate(
    input_variables=["topic"],
    template="ä¸ºä¸»é¢˜'{topic}'åˆ›ä½œä¸€ä¸ª3ç« çš„æ•…äº‹å¤§çº²"
)
outline_chain = LLMChain(
    llm=chat,
    prompt=outline_prompt,
    output_key="outline"
)

# Chain 2: æ ¹æ®å¤§çº²å†™ç¬¬ä¸€ç« 
chapter_prompt = PromptTemplate(
    input_variables=["outline"],
    template="æ ¹æ®ä»¥ä¸‹å¤§çº²å†™ç¬¬ä¸€ç« ï¼ˆ500å­—ï¼‰ï¼š\n{outline}"
)
chapter_chain = LLMChain(
    llm=chat,
    prompt=chapter_prompt,
    output_key="chapter"
)

# Chain 3: ä¸ºç« èŠ‚èµ·æ ‡é¢˜
title_prompt = PromptTemplate(
    input_variables=["chapter"],
    template="ä¸ºä»¥ä¸‹ç« èŠ‚èµ·ä¸€ä¸ªå¸å¼•äººçš„æ ‡é¢˜ï¼š\n{chapter}"
)
title_chain = LLMChain(
    llm=chat,
    prompt=title_prompt,
    output_key="title"
)

# ç»„åˆæˆé¡ºåºé“¾
overall_chain = SequentialChain(
    chains=[outline_chain, chapter_chain, title_chain],
    input_variables=["topic"],
    output_variables=["outline", "chapter", "title"]
)

# æ‰§è¡Œ
result = overall_chain.invoke({"topic": "æ—¶é—´æ—…è¡Œ"})
print(f"å¤§çº²ï¼š\n{result['outline']}\n")
print(f"ç¬¬ä¸€ç« ï¼š\n{result['chapter']}\n")
print(f"æ ‡é¢˜ï¼š{result['title']}")
```

### 2.3.3 ä½¿ç”¨LCELï¼ˆLangChain Expression Languageï¼‰

LCELæ˜¯LangChainæ¨èçš„æ–°è¯­æ³•ï¼Œæ›´ç®€æ´ä¼˜é›…ã€‚

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ |
prompt = ChatPromptTemplate.from_messages([
    ("human", "è®²ä¸€ä¸ªå…³äº{topic}çš„æ•…äº‹")
])

# åˆ›å»ºé“¾ï¼šprompt | model | parser
chain = prompt | chat | StrOutputParser()

# è°ƒç”¨
result = chain.invoke({"topic": "å‹‡æ•¢çš„å°å…”å­"})
print(result)
```

#### å¤æ‚çš„LCELé“¾

```python
# é“¾å¼è°ƒç”¨å¤šä¸ªç»„ä»¶
from langchain_core.runnables import RunnablePassthrough

# 1. å‡†å¤‡æ•°æ®
def prepare_data(inputs):
    topic = inputs["topic"]
    return {
        "topic": topic,
        "style": inputs.get("style", "å¹½é»˜"),
        "length": inputs.get("length", "short")
    }

# 2. åˆ›å»ºæç¤ºè¯
prompt = ChatPromptTemplate.from_template(
    "ç”¨{style}çš„é£æ ¼å†™ä¸€ä¸ª{length}çš„æ•…äº‹ï¼Œä¸»é¢˜æ˜¯{topic}"
)

# 3. æ„å»ºé“¾
chain = (
    RunnablePassthrough.assign(
        style=lambda x: x.get("style", "å¹½é»˜"),
        length=lambda x: x.get("length", "short")
    )
    | prompt
    | chat
    | StrOutputParser()
)

# è°ƒç”¨
result = chain.invoke({
    "topic": "AIå­¦ä¹ ",
    "style": "åŠ±å¿—",
    "length": "long"
})
```

### 2.3.4 Router Chainï¼šè·¯ç”±é“¾

æ ¹æ®è¾“å…¥å†…å®¹åŠ¨æ€é€‰æ‹©ä¸åŒçš„å¤„ç†é“¾ã€‚

```python
from langchain.chains import RouterChain
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

# å®šä¹‰å¤šä¸ªå¤„ç†é“¾
# Chain 1: ä»£ç ç›¸å…³
code_prompt = PromptTemplate(
    template="ä½ æ˜¯ä¸€ä¸ªç¼–ç¨‹ä¸“å®¶ã€‚{input}",
    input_variables=["input"]
)
code_chain = LLMChain(llm=chat, prompt=code_prompt)

# Chain 2: æ–‡æœ¬ç›¸å…³
text_prompt = PromptTemplate(
    template="ä½ æ˜¯ä¸€ä¸ªå†™ä½œåŠ©æ‰‹ã€‚{input}",
    input_variables=["input"]
)
text_chain = LLMChain(llm=chat, prompt=text_prompt)

# Chain 3: é€šç”¨
general_prompt = PromptTemplate(
    template="ä½ æ˜¯AIåŠ©æ‰‹ã€‚{input}",
    input_variables=["input"]
)
general_chain = LLMChain(llm=chat, prompt=general_prompt)

# è·¯ç”±é€»è¾‘
from langchain.chains.router import MultiPromptChain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.chains.router.multi_prompt_prompt import MULTI_PROMPT_ROUTER_TEMPLATE

# å®šä¹‰è·¯ç”±çš„ç›®æ ‡å’Œæè¿°
destinations = [
    "code: ç¼–ç¨‹å’Œä»£ç ç›¸å…³é—®é¢˜",
    "text: å†™ä½œå’Œæ–‡æœ¬ç›¸å…³é—®é¢˜",
    "general: å…¶ä»–é€šç”¨é—®é¢˜"
]

# åˆ›å»ºè·¯ç”±æç¤ºè¯
router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(
    destinations=destinations
)
router_prompt = PromptTemplate(
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser()
)

# åˆ›å»ºè·¯ç”±é“¾
router_chain = LLMRouterChain.from_llm(
    chat,
    router_prompt
)

# ç»„åˆ
chain = MultiPromptChain(
    router_chain=router_chain,
    destination_chains={
        "code": code_chain,
        "text": text_chain,
        "general": general_chain
    },
    default_chain=general_chain
)

# ä½¿ç”¨
result = chain.run("å¦‚ä½•åœ¨Pythonä¸­å®ç°å¿«é€Ÿæ’åºï¼Ÿ")
# ä¼šè‡ªåŠ¨è·¯ç”±åˆ°codeé“¾
```

---

## 2.4 Memoryï¼šè®°å¿†ç®¡ç†

Memoryç»„ä»¶è®©AIèƒ½å¤Ÿè®°ä½å¯¹è¯å†å²ï¼Œå®ç°å¤šè½®å¯¹è¯ã€‚

### 2.4.1 ConversationBufferMemoryï¼šç¼“å†²è®°å¿†

```python
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

# åˆ›å»ºè®°å¿†
memory = ConversationBufferMemory()

# åˆ›å»ºå¯¹è¯é“¾
conversation = ConversationChain(
    llm=chat,
    memory=memory,
    verbose=True  # æ‰“å°è¯¦ç»†ä¿¡æ¯
)

# ç¬¬ä¸€æ¬¡å¯¹è¯
response1 = conversation.predict(input="æˆ‘å«å°æ˜")
print(response1)
# AI: ä½ å¥½å°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ã€‚

# ç¬¬äºŒæ¬¡å¯¹è¯ï¼ˆAIè®°ä½äº†åå­—ï¼‰
response2 = conversation.predict(input="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")
print(response2)
# AI: ä½ å«å°æ˜ã€‚

# æŸ¥çœ‹è®°å¿†å†å²
print(memory.load_memory_variables({}))
# {'history': 'Human: æˆ‘å«å°æ˜\nAI: ä½ å¥½å°æ˜ï¼...'}
```

### 2.4.2 ConversationBufferWindowMemoryï¼šçª—å£è®°å¿†

åªä¿å­˜æœ€è¿‘Nè½®å¯¹è¯ï¼ŒèŠ‚çœTokenã€‚

```python
from langchain.memory import ConversationBufferWindowMemory

# åªä¿å­˜æœ€è¿‘3è½®å¯¹è¯
memory = ConversationBufferWindowMemory(k=3)

conversation = ConversationChain(
    llm=chat,
    memory=memory
)

# è¿›è¡Œå¤šè½®å¯¹è¯
conversation.predict(input="æˆ‘å–œæ¬¢Python")
conversation.predict(input="æˆ‘ä¹Ÿå–œæ¬¢JavaScript")
conversation.predict(input="æˆ‘æœ€å–œæ¬¢Rust")
conversation.predict(input="æˆ‘å–œæ¬¢ä»€ä¹ˆè¯­è¨€ï¼Ÿ")
# AIè®°å¾—æœ€è¿‘3è½®çš„å¯¹è¯ï¼Œä½†ä¸è®°å¾—æ›´æ—©çš„å†…å®¹
```

### 2.4.3 ConversationSummaryMemoryï¼šæ‘˜è¦è®°å¿†

å¯¹é•¿å¯¹è¯è¿›è¡Œæ‘˜è¦ï¼ŒèŠ‚çœTokenã€‚

```python
from langchain.memory import ConversationSummaryMemory

# åˆ›å»ºæ‘˜è¦è®°å¿†
memory = ConversationSummaryMemory(llm=chat)

conversation = ConversationChain(
    llm=chat,
    memory=memory,
    verbose=True
)

# è¿›è¡Œå¤šè½®å¯¹è¯
conversation.predict(input="æˆ‘æ˜¯å°æ˜ï¼Œæ˜¯ä¸€åç¨‹åºå‘˜")
conversation.predict(input="æˆ‘ä¸»è¦ç”¨Pythonå¼€å‘")
conversation.predict(input="æˆ‘åœ¨æ­å·å·¥ä½œ")

# æŸ¥çœ‹æ‘˜è¦
print(memory.load_memory_variables({}))
# ç±»ä¼¼ï¼š'summary': 'The human introduces themselves as Xiao Ming...'
```

### 2.4.4 åœ¨LCELä¸­ä½¿ç”¨Memory

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.messages import HumanMessage, AIMessage

# åˆ›å»ºåŒ…å«å†å²æ¶ˆæ¯çš„æç¤ºè¯
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚"),
    MessagesPlaceholder(variable_name="history"),  # å†å²æ¶ˆæ¯å ä½ç¬¦
    ("human", "{input}")
])

# åˆ›å»ºé“¾
chain = prompt | chat

# åˆå§‹åŒ–å†å²
history = []

# ç¬¬ä¸€è½®
inputs = {"input": "ä½ å¥½", "history": history}
response = chain.invoke(inputs)
print(response.content)

# ä¿å­˜åˆ°å†å²
history.append(HumanMessage(content="ä½ å¥½"))
history.append(AIMessage(content=response.content))

# ç¬¬äºŒè½®
inputs = {"input": "æˆ‘å«ä»€ä¹ˆï¼Ÿ", "history": history}
response = chain.invoke(inputs)
# AIå¯ä»¥å¼•ç”¨ä¹‹å‰çš„å¯¹è¯
```

### 2.4.5 å®ç”¨çš„è®°å¿†ç®¡ç†æŠ€å·§

```python
# 1. ä¿å­˜å’ŒåŠ è½½è®°å¿†
import json

# ä¿å­˜
memory_dict = memory.save_context(
    {"input": "ç”¨æˆ·è¾“å…¥"},
    {"output": "AIå›å¤"}
)
with open("memory.json", "w") as f:
    json.dump(memory.load_memory_variables({}), f)

# åŠ è½½
with open("memory.json") as f:
    saved_memory = json.load(f)
    memory = ConversationBufferMemory()
    memory.chat_memory.add_user_message(saved_memory['history'])

# 2. æ¸…ç©ºè®°å¿†
memory.clear()

# 3. è·å–Tokenä½¿ç”¨é‡
from langchain.callbacks import get_openai_callback

with get_openai_callback() as cb:
    response = conversation.predict(input="ä½ å¥½")
    print(f"Total Tokens: {cb.total_tokens}")
    print(f"Total Cost: ${cb.total_cost}")
```

---

## 2.5 ç»¼åˆå®æˆ˜ï¼šæ™ºèƒ½é—®ç­”ç³»ç»Ÿ

è®©æˆ‘ä»¬å°†æ‰€å­¦çŸ¥è¯†æ•´åˆèµ·æ¥ï¼Œæ„å»ºä¸€ä¸ªæ™ºèƒ½é—®ç­”ç³»ç»Ÿã€‚

```python
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
from config import Config

class SmartQA:
    """æ™ºèƒ½é—®ç­”ç³»ç»Ÿ"""

    def __init__(self):
        # åˆå§‹åŒ–æ¨¡å‹
        self.llm = ChatOpenAI(
            model=Config.MODEL_NAME,
            api_key=Config.OPENAI_API_KEY,
            temperature=0.7
        )

        # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œåå«å°å¾ã€‚

ä½ çš„ç‰¹ç‚¹ï¼š
- å›ç­”å‡†ç¡®ä¸“ä¸š
- ä½¿ç”¨emojiå¢åŠ äº²å’ŒåŠ›
- é‡è¦å†…å®¹ç”¨åˆ—è¡¨å‘ˆç°
- ä¸ç¡®å®šçš„é—®é¢˜ä¼šè¯šå®å‘ŠçŸ¥"""),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])

        # åˆ›å»ºè®°å¿†
        self.memory = ConversationBufferMemory(
            return_messages=True,
            memory_key="history"
        )

        # åˆ›å»ºé“¾
        self.chain = self.prompt | self.llm | StrOutputParser()

    def ask(self, question: str) -> str:
        """æé—®"""
        # è·å–å†å²
        history = self.memory.chat_memory.messages

        # è°ƒç”¨é“¾
        response = self.chain.invoke({
            "input": question,
            "history": history
        })

        # ä¿å­˜åˆ°è®°å¿†
        self.memory.chat_memory.add_user_message(question)
        self.memory.chat_memory.add_ai_message(response)

        return response

    def chat(self):
        """äº¤äº’å¼èŠå¤©"""
        print("ğŸ¤– æ™ºèƒ½é—®ç­”ç³»ç»Ÿï¼ˆè¾“å…¥'quit'é€€å‡ºï¼‰\n")

        while True:
            question = input("\nä½ ï¼š").strip()

            if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                print("å°å¾ï¼šå†è§ï¼ğŸ‘‹")
                break

            if not question:
                continue

            try:
                answer = self.ask(question)
                print(f"å°å¾ï¼š{answer}")
            except Exception as e:
                print(f"âŒ é”™è¯¯ï¼š{e}")

# ä½¿ç”¨
if __name__ == "__main__":
    qa = SmartQA()
    qa.chat()
```

---

## 2.6 æœ€ä½³å®è·µ

### 2.6.1 æç¤ºè¯æ¨¡æ¿ç®¡ç†

```python
# åˆ›å»ºtemplates.py
PROMPTS = {
    "code_assistant": ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯{language}ç¼–ç¨‹ä¸“å®¶"),
        ("human", "{question}")
    ]),

    "writer": ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸“ä¸šå†™æ‰‹ï¼Œé£æ ¼{style}"),
        ("human", "ä¸»é¢˜ï¼š{topic}ï¼Œå­—æ•°ï¼š{words}")
    ]),

    "analyst": ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯æ•°æ®åˆ†æå¸ˆ"),
        ("human", "åˆ†æä»¥ä¸‹æ•°æ®ï¼š\n{data}")
    ])
}

# ä½¿ç”¨
from templates import PROMPTS

chain = PROMPTS["code_assistant"] | chat | StrOutputParser()
```

### 2.6.2 é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
from tenacity import retry, stop_after_attempt, wait_exponential
from langchain_core.exceptions import LangChainException

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def safe_chain_invoke(chain, inputs):
    """å¸¦é‡è¯•çš„é“¾è°ƒç”¨"""
    try:
        return chain.invoke(inputs)
    except LangChainException as e:
        print(f"âš ï¸ è°ƒç”¨å¤±è´¥: {e}")
        raise
    except Exception as e:
        print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
        raise
```

### 2.6.3 æˆæœ¬ä¼˜åŒ–

```python
# 1. ä½¿ç”¨æ›´ä¾¿å®œçš„æ¨¡å‹
fast_llm = ChatOpenAI(model="gpt-3.5-turbo")
smart_llm = ChatOpenAI(model="gpt-4")

# ç®€å•ä»»åŠ¡ç”¨å¿«é€Ÿæ¨¡å‹
simple_chain = fast_llm

# å¤æ‚ä»»åŠ¡ç”¨æ™ºèƒ½æ¨¡å‹
complex_chain = smart_llm

# 2. ç¼“å­˜ç»“æœ
from langchain.cache import InMemoryCache
from langchain.globals import set_llm_cache

set_llm_cache(InMemoryCache())

# ç¬¬ä¸€æ¬¡è°ƒç”¨ä¼šè¯·æ±‚API
chain.invoke("Pythonæ˜¯ä»€ä¹ˆï¼Ÿ")

# ç¬¬äºŒæ¬¡ç›´æ¥ä»ç¼“å­˜è¯»å–
chain.invoke("Pythonæ˜¯ä»€ä¹ˆï¼Ÿ")

# 3. æµå¼è¾“å‡ºèŠ‚çœé¦–å­—èŠ‚æ—¶é—´
for chunk in chain.stream("è®²ä¸ªæ•…äº‹"):
    print(chunk.content, end="", flush=True)
```

---

## 2.7 æœ¬ç« å°ç»“

### 2.7.1 æ ¸å¿ƒæ¦‚å¿µå›é¡¾

âœ… **LangChainå…­å¤§æ¨¡å—**ï¼š
- Model I/Oã€Chainsã€Memory
- Retrievalã€Agentsã€Callbacks

âœ… **Model I/O**ï¼š
- Modelsï¼šç»Ÿä¸€çš„æ¨¡å‹æ¥å£
- Promptsï¼šæç¤ºè¯æ¨¡æ¿
- Output Parsersï¼šç»“æ„åŒ–è¾“å‡º

âœ… **Chains**ï¼š
- LLMChainï¼šåŸºç¡€é“¾
- Sequential Chainï¼šé¡ºåºé“¾
- Router Chainï¼šè·¯ç”±é“¾
- LCELï¼šæ–°çš„é“¾å¼è¯­æ³•

âœ… **Memory**ï¼š
- BufferMemoryï¼šå…¨éƒ¨è®°å¿†
- WindowMemoryï¼šçª—å£è®°å¿†
- SummaryMemoryï¼šæ‘˜è¦è®°å¿†

### 2.7.2 ä½ å·²ç»å­¦ä¼š

- âœ… ä½¿ç”¨LangChainç»Ÿä¸€æ¥å£è°ƒç”¨LLM
- âœ… åˆ›å»ºå’Œç®¡ç†æç¤ºè¯æ¨¡æ¿
- âœ… è§£æå’Œç»“æ„åŒ–è¾“å‡º
- âœ… æ„å»ºå¤æ‚çš„é“¾å¼å·¥ä½œæµ
- âœ… å®ç°å¯¹è¯è®°å¿†ç®¡ç†
- âœ… æ„å»ºå®Œæ•´çš„é—®ç­”ç³»ç»Ÿ

### 2.7.3 ä¸‹ä¸€æ­¥

æ­å–œå®Œæˆç¬¬2ç« ï¼ä½ å·²ç»æŒæ¡äº†LangChainçš„æ ¸å¿ƒç”¨æ³•ã€‚

**åœ¨ä¸‹ä¸€ç« ï¼Œæˆ‘ä»¬å°†å­¦ä¹ **ï¼š
- Prompt Engineeringçš„æ ¸å¿ƒåŸåˆ™
- é«˜çº§æç¤ºè¯æŠ€å·§å’Œæ¨¡å¼
- Few-shot Learning
- æç¤ºè¯ä¼˜åŒ–æ–¹æ³•

**å‡†å¤‡å¥½äº†å—ï¼Ÿç»§ç»­ï¼** ğŸš€

---

## 2.8 ç»ƒä¹ é¢˜

### 2.8.1 åŸºç¡€ç»ƒä¹ 

**ç»ƒä¹ 1**ï¼šä½¿ç”¨LCELåˆ›å»ºä¸€ä¸ªé“¾ï¼Œå®ç°ï¼š
- æ¥æ”¶ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
- ç¿»è¯‘æˆè‹±æ–‡
- æ€»ç»“è‹±æ–‡å†…å®¹
- è¿”å›ä¸­æ–‡æ‘˜è¦

**ç»ƒä¹ 2**ï¼šåˆ›å»ºä¸€ä¸ªå¸¦è®°å¿†çš„"ä»£ç è§£é‡Šå™¨"ï¼š
- å¯ä»¥è¿ç»­å¯¹è¯
- è®°ä½ç”¨æˆ·æåˆ°çš„ä»£ç ç‰‡æ®µ
- æä¾›é€æ­¥è§£é‡Š

**ç»ƒä¹ 3**ï¼šå®ç°Router Chainï¼Œæ ¹æ®é—®é¢˜ç±»å‹è‡ªåŠ¨é€‰æ‹©ï¼š
- ä»£ç é—®é¢˜ â†’ ä½¿ç”¨æŠ€æœ¯ä¸“å®¶è§’è‰²
- åˆ›æ„é—®é¢˜ â†’ ä½¿ç”¨åˆ›æ„å†™æ‰‹è§’è‰²
- å…¶ä»– â†’ ä½¿ç”¨é€šç”¨åŠ©æ‰‹è§’è‰²

### 2.8.2 è¿›é˜¶ç»ƒä¹ 

**æŒ‘æˆ˜1**ï¼šæ„å»º"å­¦ä¹ åŠ©æ‰‹"ç³»ç»Ÿ
åŠŸèƒ½ï¼š
- è¿½è¸ªå­¦ä¹ è¿›åº¦
- æ ¹æ®ç”¨æˆ·æ°´å¹³è°ƒæ•´å›ç­”æ·±åº¦
- ç”Ÿæˆç»ƒä¹ é¢˜
- æä¾›å­¦ä¹ å»ºè®®

**æŒ‘æˆ˜2**ï¼šå®ç°"å¤šè½®å¯¹è¯æ–‡æ¡ˆç”Ÿæˆå™¨"
åœºæ™¯ï¼š
- ç¬¬ä¸€è½®ï¼šäº†è§£äº§å“ç‰¹ç‚¹
- ç¬¬äºŒè½®ï¼šç¡®å®šç›®æ ‡å—ä¼—
- ç¬¬ä¸‰è½®ï¼šé€‰æ‹©æ–‡æ¡ˆé£æ ¼
- ç¬¬å››è½®ï¼šç”Ÿæˆå¤šä¸ªç‰ˆæœ¬

---

**é‡åˆ°é—®é¢˜ï¼Ÿ**
- æŸ¥çœ‹ä»£ç ç¤ºä¾‹ï¼š`examples/` ç›®å½•
- å‚è€ƒå®˜æ–¹æ–‡æ¡£ï¼šhttps://python.langchain.com
- å‘é‚®ä»¶æ±‚åŠ©ï¼šesimonx@163.com

**ä¸‹ä¸€ç« ï¼š[Prompt Engineering â†’](chapter-03)**
